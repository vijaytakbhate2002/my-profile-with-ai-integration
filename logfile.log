2024-11-25 21:25:27 - root - INFO - Creating a new FAISS document store...
2024-11-25 21:25:27 - haystack.modeling.utils - INFO - Using devices: CPU - Number of GPUs: 0
2024-11-25 21:25:28 - haystack.modeling.model.language_model - WARNING - Using a model of type 'mpnet' which might be incompatible with DPR encoders. Only Bert-based encoders are supported. They need input_ids, token_type_ids, attention_mask as input tensors.
2024-11-25 21:25:30 - haystack.modeling.model.language_model - INFO - Auto-detected model language: english
2024-11-25 21:25:31 - haystack.modeling.model.language_model - WARNING - Using a model of type 'mpnet' which might be incompatible with DPR encoders. Only Bert-based encoders are supported. They need input_ids, token_type_ids, attention_mask as input tensors.
2024-11-25 21:25:33 - haystack.modeling.model.language_model - INFO - Auto-detected model language: english
2024-11-25 21:25:33 - root - ERROR - Error reading PDF file: EOF marker not found
2024-11-25 21:25:33 - root - INFO - Documents successfully loaded: []
2024-11-25 21:25:33 - root - INFO - loaded document = []
2024-11-25 21:25:33 - root - WARNING - No documents were loaded from the PDF.
2024-11-25 21:25:33 - haystack.modeling.utils - INFO - Using devices: CPU - Number of GPUs: 0
2024-11-25 21:25:33 - haystack.modeling.utils - INFO - Using devices: CPU - Number of GPUs: 0
2024-11-25 21:25:34 - haystack.modeling.model.language_model - INFO -  * LOADING MODEL: 'deepset/roberta-base-squad2' (Roberta)
2024-11-25 21:25:34 - haystack.modeling.model.language_model - INFO - Auto-detected model language: english
2024-11-25 21:25:34 - haystack.modeling.model.language_model - INFO - Loaded 'deepset/roberta-base-squad2' (Roberta model) from model hub.
2024-11-25 21:25:35 - haystack.modeling.utils - INFO - Using devices: CPU - Number of GPUs: 0
2024-11-25 21:25:35 - haystack.modeling.data_handler.processor - ERROR - There were 1 errors during preprocessing at positions: {0}
2024-11-25 21:28:38 - root - INFO - Creating a new FAISS document store...
2024-11-25 21:28:38 - haystack.modeling.utils - INFO - Using devices: CPU - Number of GPUs: 0
2024-11-25 21:28:40 - haystack.modeling.model.language_model - WARNING - Using a model of type 'mpnet' which might be incompatible with DPR encoders. Only Bert-based encoders are supported. They need input_ids, token_type_ids, attention_mask as input tensors.
2024-11-25 21:28:42 - haystack.modeling.model.language_model - INFO - Auto-detected model language: english
2024-11-25 21:28:42 - haystack.modeling.model.language_model - WARNING - Using a model of type 'mpnet' which might be incompatible with DPR encoders. Only Bert-based encoders are supported. They need input_ids, token_type_ids, attention_mask as input tensors.
2024-11-25 21:28:44 - haystack.modeling.model.language_model - INFO - Auto-detected model language: english
2024-11-25 21:28:44 - root - INFO - Reading text document
2024-11-25 21:28:44 - root - ERROR - Error reading PDF file: EOF marker not found
2024-11-25 21:28:44 - root - INFO - Documents successfully loaded: []
2024-11-25 21:28:44 - root - INFO - loaded document = []
2024-11-25 21:28:44 - root - WARNING - No documents were loaded from the PDF.
2024-11-25 21:28:44 - haystack.modeling.utils - INFO - Using devices: CPU - Number of GPUs: 0
2024-11-25 21:28:44 - haystack.modeling.utils - INFO - Using devices: CPU - Number of GPUs: 0
2024-11-25 21:28:45 - haystack.modeling.model.language_model - INFO -  * LOADING MODEL: 'deepset/roberta-base-squad2' (Roberta)
2024-11-25 21:28:45 - haystack.modeling.model.language_model - INFO - Auto-detected model language: english
2024-11-25 21:28:45 - haystack.modeling.model.language_model - INFO - Loaded 'deepset/roberta-base-squad2' (Roberta model) from model hub.
2024-11-25 21:28:46 - haystack.modeling.utils - INFO - Using devices: CPU - Number of GPUs: 0
2024-11-25 21:28:46 - haystack.modeling.data_handler.processor - ERROR - There were 1 errors during preprocessing at positions: {0}
2024-11-25 21:30:28 - root - INFO - Creating a new FAISS document store...
2024-11-25 21:30:28 - haystack.modeling.utils - INFO - Using devices: CPU - Number of GPUs: 0
2024-11-25 21:30:29 - haystack.modeling.model.language_model - WARNING - Using a model of type 'mpnet' which might be incompatible with DPR encoders. Only Bert-based encoders are supported. They need input_ids, token_type_ids, attention_mask as input tensors.
2024-11-25 21:30:31 - haystack.modeling.model.language_model - INFO - Auto-detected model language: english
2024-11-25 21:30:32 - haystack.modeling.model.language_model - WARNING - Using a model of type 'mpnet' which might be incompatible with DPR encoders. Only Bert-based encoders are supported. They need input_ids, token_type_ids, attention_mask as input tensors.
2024-11-25 21:30:33 - haystack.modeling.model.language_model - INFO - Auto-detected model language: english
2024-11-25 21:30:33 - root - INFO - Reading text document
2024-11-25 21:30:33 - root - INFO - loaded document = [{'content': '\nPersonal Information\n\nName: Vijay Takbhate\nEmail: vijaytakbhate20@gmail.com Phone: 8767363681\nGitHub: https://github.com/vijaytakbhate2002\nLinkedIn: https://www.linkedin.com/in/vijay-takbhate-b9231a236/ Kaggle: https://www.kaggle.com/vijay20213\n\n'}, {'content': '\n\nExperience - Fox Solutions Pvt. Ltd.\nRole: Automation Engineer Duration: Jun 2024 - Oct 2024 Location: Maharashtra\nKey Contributions:\n-Completed 2 months of internship plus 4 months of full-time work.\n-Worked with PLC and SCADA systems, focusing on automating processes and optimizing operational efficiency.\n-Collaborated with cross-functional teams to implement automation solutions for industrial applications.\n\n'}, {'content': '\n\nExperience - Cei Design Consultancy Pvt. Ltd.\nRole: Python Developer Intern Duration: Aug 2024 - Sept 2024 Location: Remote, Maharashtra\nKey Contributions:\n-Specialized in data processing using Python and Excel.\n-Utilized OpenCV for image processing tasks.\n\n'}, {'content': '\n\nExperience - Ujucode\nRole: Subject Matter Expert Intern Duration: Aug 2023 - Oct 2023 Location: Remote, Maharashtra\nKey Contributions:\n-Contributed as a Python developer for a ChatBot project.\n-Handled backend development tasks and researched Python modules.\n\n'}, {'content': '\n\nProject - Twitter Post Sentiment Prediction\nDetails:\n-Engineered an ETL pipeline using PySpark and SQL.\n-Conducted sentiment analysis using NLP (TF-IDF) and optimized hyperparameters.\n-Monitored model performance through MLFlow on Databricks.\n-Leveraged Google Cloud Storage and MySQL for data management.\n-Deployed the model using Docker and hosted it on Render.\n\n'}, {'content': '\n\nProject - Text-Text Chat-Bot\nDetails:\n-Designed an advanced Chat-Bot using the NVIDIA API and prompt engineering.\n-Features include paraphrasing, grammar correction, AI detection, plagiarism checking, and content summarization.\n-Targeted at content creators, researchers, and businesses.\n-Technologies Used: HTML, CSS, Python Flask, Cloud Database, and Render.\n\n'}, {'content': '\n\nProject - Hand Gesture Recognition\nDetails:\n-Used Google-s MediaPipe framework for detecting hand landmarks and gestures.\n-Created and labeled a custom dataset of hand gestures for training.\n-Developed a Streamlit application to improve accessibility and flexibility.\n\n'}, {'content': '\n\nTechnical Skills\nLanguages: MySQL, Python, HTML, CSS\nTechnologies: Streamlit, Flask, VS Code, GitHub, MLflow, Docker, PySpark, Databricks, Google Cloud Platform\n\n'}, {'content': '\n\nCertification\nMLOps Bootcamp: Mastering AI Operations for Success (Jun 2024)\n-Learned about the MLOps lifecycle and modular programming.\n-Acquired skills in Git, Python, Flask, and MLflow.\n\n'}, {'content': '\n\nEducation Details:\nBachelor of Technology in Electronics and Telecommunication (May 2024) Institution: SVERI-s College of Engineering, Pandharpur, Maharashtra Score: 81.71\nDiploma in Electronics and Telecommunication (May 2021) Institution: SVERI-s College of Engineering, Pandharpur, Maharashtra Score: 91.73\n\n'}, {'content': "\n\nBlogging\n\n\nSupervised, Unsupervised, and Beyond: ML Techniques Simplified\nNovember 25, 2024\nThere are several techniques for ML training. Among these, I will cover the following:\n\nSupervised and Unsupervised Learning\nSemi-Supervised Learning\nOffline and Online Learning\nInstance-Based and Model-Based Learning\n\nSupervised and Unsupervised Learning\nSupervised learning is like spoon-feeding our ML model in its initial stages, allowing it to learn and improve over time. Here, I’m referring to the training process.\n\nIn supervised learning, there are input columns and output columns, also called target columns. For example, in spam detection—a classification problem—the input is the email, and the target is whether the email is spam or not. That’s it!\nThis process resembles a student-teacher scenario where the teacher is a human, and the student is the model. The dataset serves as the knowledge used to train the student (model)\nSee content credentials\n\nhumand and model\nIn Unsupervised learning, we are not aware of the data labels. Instead, we separate the input data by analyzing similarities and grouping them into different clusters.\nOnce the clusters are formed, we can assign custom labels to each one. This technique is widely used to identify product relationships in online shopping and to recommend new products based on a user’s purchase history.\n\nUnsupervised Learning Clusters\nHere three clusters with three different image categories are formed\n\nSemi-Supervised Learning\nSemi-Supervised Learning is a combination of supervised and unsupervised learning, where some data is labeled and some data is unlabeled. A good example of this is Google Photos, which automatically separates new photos into their respective groups based on whether they contain a particular person in each image.\n\nThere are several techniques under semi-supervised learning; here, we will focus on the following:\nSelf Learning\nConsistency Regularization\nGenerative Models\nGraph-Based Learning\n\nLet's discuss them one by one:\n\nSelf Learning\nSelf Learning trains a model with labeled data and generates pseudo-labels for the unlabeled data. \n\nSelf learning\nThen, the model is trained on the entire dataset, which includes both the generated pseudo-labels and the labeled data.\n\nConsistency Regularization\nThis technique uses data augmentation to generate similar data, and then the model is enforced to predict the same outcome for both the augmented and original data. It helps create a model that can find similarities in both labeled and unlabeled data, predicting the same output for unlabeled data as it would for labeled data.\n\nData augmentation includes techniques like image flipping, blurring, rotation, etc. After augmenting the data, the model is trained to predict the same class for both the augmented and original images.\n\nGenerative Models\nGenerative models create synthetic data points and learn the underlying structure of the training data. These models can generate new datasets using encoders. \n\nExamples include Generative Adversarial Networks (GANs) and Variational Autoencoders (VAEs).\n\nGraph-Based Learning\nThis technique creates nodes for both labeled and unlabeled data. Similar nodes are then connected to each other.\nFor example, when dealing with images, each image starts as a separate node. If similar images are found based on their features, they are grouped or connected by an edge.\nRefer below graph to understand graph-based learning\nGraph based learning\nHere label 1 is came from labeled image of group 1, simillarly for label 2 and label 3. Finally we came up with three labels that means labeled data has three unique labels.\nBatch (Offline) and Online Learning\nBatch Learning\nBatch learning, also known as offline learning, is a technique where the developer needs to stop the deployed ML model, retrain it on new data, and then redeploy it. \nThis technique is useful when frequent updates are not required. For example, product recommendation systems can often be retrained weekly or even over several days.\nBatch learning can be costly when training the model on the entire dataset every time. However, using incremental learning models can help avoid unnecessary retraining, making the process more efficient.\nHere are some examples on incremental learning models, as of now we are focusing on general concepts so don't go much deeper into these types.\nStochastic Gradient Descent (SGD)\nNaive Bayes (Online Version)\nIncremental Support Vector Machines (ISVM)\nOnline Random Forests\nIncremental Decision Trees\nYou can refer this article to learn more about incremental learning\nHere’s your text with grammatical corrections and improved clarity:\nOnline Learning\nOnline learning is used when an ML model needs to be updated continuously with new inputs, such as in stock price prediction. In this scenario, the model must remain aware of the most recent data.\n\nBatch and online learning\nOnly models capable of incremental learning are used in online learning. Each new instance is fed back into the model to update its internal weights based on the latest input.\n\nInstance-Based and Model-Based Learning\nInstance-Based Learning\nInstance-based learning involves comparing a new input with stored data. If similarity is found, the model returns the label of the corresponding input. This approach uses pattern matching techniques. \nHere model works as a search engin not exactly same but it will find simmilarity from stored data.\nSome common models used in instance-based learning include:\nK-Nearest Neighbors (K-NN) with labels\nLocally Weighted Regression, etc.\nModel-Based Learning\nIn model-based learning, we train the model using a training dataset. During the training process, the model creates its own complex patterns (mathematical equations) to make predictions for future inputs. \nA simple example is the equation y=mx+cy = mx + c, which represents a trained model. Some common models used in model-based learning include:\nLinear Regression\nLogistic Regression\nDecision Tree, etc.\n\nSummary\nMachine Learning techniques can be broadly categorized into supervised, unsupervised, and semi-supervised learning. \nSupervised learning uses labeled data, like spam detection, while unsupervised learning works with unlabeled data, grouping similar points into clusters. \nSemi-supervised learning combines both, employing techniques like self-learning and consistency regularization to leverage partially labeled datasets.\nOther approaches include batch learning, where models are retrained periodically, and online learning, which updates models continuously with new data, suitable for dynamic tasks like stock price prediction. \nInstance-based learning relies on pattern matching (e.g., K-Nearest Neighbors), while model-based learning creates mathematical models (e.g., Linear Regression) to make predictions.\nThis is all about Machine Learning techniques. If you learned something, let me know in the comments. Your suggestions will help me improve my blogs.\nThanks for reading!\n\n"}, {'content': '\nBlogging\n\nMastering the End-to-End Machine Learning Lifecycle: From Data to Deployment\n\nNovember 24, 2024\n This article takes you through the complete lifecycle of a Machine Learning project. From ETL to deployment, I’ll share every detail of how I brought this project to life.\nData Engineering\nData Engineering serves as the starting point in the Machine Learning project lifecycle, bringing all distributed data together in one place.\nFor this project, I utilized an ETL pipeline—a core concept in Data Engineering. It enabled me to extract raw data, transform it into a meaningful format, and load it into a suitable location for further processing.\nLet\'s deep dive into ETL pipeline\nETL stands for Extract, Transform, and Load. These pipelines can be executed periodically (e.g., daily or hourly) to fetch real-time data, enhancing the predictive power of our Machine Learning model for real-world applications.\nThis process can be automated using tools like Apache Airflow, Kubeflow Pipelines, AWS Step Functions, and more, streamlining the workflow for consistent and efficient data updates.\n\nETL Pipeline:\nETL Pipeline\nExtract Operation\nThe extract operation is responsible for fetching data from various sources such as websites, databases, and APIs, as illustrated in the flowchart above.\n\nTransform Operation\nThe transform operation focuses on data cleaning, feature selection, and manipulation. In my project, I used this step to extract only the required features, ensuring the data is ready for the next phase.\n\nLoad Operation\nThe load operation transfers the transformed data to its destination, where we can choose the appropriate storage format. Typically, the processed data is stored in a database for further analysis and model training.\n\nFor my project, I used PySpark to build the ETL pipeline, as it enables efficient processing of large datasets.\nWhy not Pandas?\nWhile Pandas is excellent for small to medium-sized datasets, it stores DataFrames in RAM, which can lead to out-of-memory exceptions when handling large datasets.\nIn contrast, PySpark creates a session and processes data in chunks, storing it in ROM, making it ideal for handling large-scale data.\nFor the actual implementation of the pipeline using PySpark, please refer to the accompanying jupyter notebook.\nEDA (Data Analysis)\nData understanding is a critical stage before building any Machine Learning model. It allows us to analyze the data, plan the data processing steps, and gain insights into its structure and quality.\nIn this project, I examined the balance of data in my training and testing datasets and found it to be well-distributed across all four categories.\n\nBalanceness Checking on train and test data\nThroughout the entire process, I focused on two columns:\nTweets\nSentiments\nThe Tweets column contains the raw Twitter text data, while the Sentiments column serves as the target variable for prediction.\nNatural Language Processing\nI applied several key NLP techniques to preprocess the data and prepare it for Machine Learning model building. Below are the main steps I executed:\n\nText Processing\nRegex: I applied regular expressions to clean the text by removing URLs, hashtags, HTML tags, and keeping only alphanumeric characters. This helped eliminate unnecessary noise from the data.\nNLTK: Using the Natural Language Toolkit (NLTK), I performed word tokenization, stemming and lemmatization.\nWord tokenization is just splitting sentence into words, so we can processing each word from sentence individually.\nStemming helps us to truncate prefix or suffix of text to reduce count on unique words from corpus (paragraph).\nLemmatization is the proecss of converting any word into it\'s base word, for eg. Played will convert to play.\n\nWordCloud \nWord cloud concept help you to understand importance of words from given data. I splitted my data into four sections.\ndata for negative sentiments\ndata for positive sentiments\ndata for neutral sentiments\ndata for irrelevant sentiments\n\nHere is the representation of most frequent words for each category.\nCategorywise Word Cloud Presentation\nHere you can clearly see that there is no much difference in negative and positive sentiments data.\nthis is representation of bad data, here we can filterout our data for further processing, it might reduce data but you can do data augmentation techniques here to increase your data.\naugmenting of data in NLP with TF-IDF will not bad idea because TF-IDF and any other porcessing technique that I used is not able to detect sentence grammer or it doesn,t require sophisticated text. \nYou can think our input text will work as bag of word for model. you can understand by refering below vectorization technique.\nVectorization with TF-IDF \nI used Term Frequency-Inverse Document Frequency (TF-IDF) for text vectorization. This method transforms text into a numerical format, considering the importance of each word across documents, which prevents frequent words from dominating the model.\n\nTF-IDF\nTF-IDF stands for Term Frequency-Inverse Document Frequency. \nTerm Frequency (TF): This measures how frequently a term (t) appears in a specific document (d). It\'s calculated by dividing the number of occurrences of the term in the document by the total number of terms in that document.\nInverse Document Frequency (IDF): This measures how important a term is across the entire collection of documents. It\'s calculated by taking the logarithm of the ratio of the total number of documents (N) to the number of documents containing the term (df(t)). A higher IDF value indicates a rarer term, making it more significant.\nHere is the resulant data we got from TF-IDF\nAfter all preprocessing of text it\'s time to save our data for mlflow experiments. To see actual implementation of end to end process of NLP you can refer jupyter notebook.\nmlflow experiments\nMLflow experiments allow you to conduct multiple experiments with your trained model, helping you track and compare results over time. For running MLflow experiments, I prefer using Databricks as it offers an integrated experiment section, making the process much more streamlined and efficient.\nIn Databricks, you can easily connect your experiment by passing the experiment ID into the MLflow code. This integration simplifies the entire workflow, enabling better experiment tracking and easier comparison of model performance.\nFor my experiments, I worked with several models, including Logistic Regression, Multinomial Naive Bayes, and Decision Tree Classifier. By applying different combinations of parameters, I was able to experiment with and compare the performance of each model. Here\'s a preview of the Logistic Regression model’s F1 score, which highlights the model\'s ability to balance precision and recall:\nLogistic Regression F1 Score: 0.58\nThis approach allowed me to track the effectiveness of each model and make adjustments as needed for improving performance.\n\nF1 score with different parameters (Logistic Regression)\nI recommend running the notebook below in your own account to see the results. You\'ll definitely start appreciating the power of MLflow.\nFor more information, please refer to the accompanying Databricks notebook.\n\nHyperparameter tuning\nHyperparameter tuning is crucial for identifying the best parameter combination for your model. This technique involves an iterative process where, for every parameter combination, the model is trained and tested on a dataset. It is often described as a "trial and error" method.\nHowever, this approach can be computationally expensive, especially when working with complex or heavy machine learning models. For large-scale problems, hyperparameter tuning can be made more efficient through sampling or batch methods. In these methods, you don\'t use the entire dataset for training the model; instead, you choose random or stratified data points from the dataset to train the model. Although this may slightly reduce accuracy, it is more feasible when working with large datasets.\nFor my project, I used Grid Search CV to find the best hyperparameter combination for the model. Below are some common techniques for hyperparameter tuning, especially for large datasets:\nGrid Search This technique exhaustively searches through a specified set of hyperparameter values, trying all possible combinations. While effective, it can be computationally expensive for larger datasets due to the exhaustive nature of the search.\nRandom Search Randomly samples from the hyperparameter space, offering a faster alternative to grid search. This method explores a wider range of hyperparameters with fewer evaluations, making it more efficient for larger datasets.\nBayesian Optimization This method uses probabilistic models to predict the performance of different hyperparameters. It selects the next set of hyperparameters to evaluate based on previous results, making it more efficient and suitable for large datasets.\nGenetic Algorithms Inspired by natural selection, these algorithms iteratively evolve a population of hyperparameter sets to improve model performance. This method works well with complex search spaces.\nHyperband Hyperband combines random search with early stopping to dynamically allocate resources across multiple configurations, identifying promising hyperparameters quickly without excessive computational costs.\nBayesian Optimization with Gaussian Processes This technique models the hyperparameter search space using Gaussian processes, focusing on regions that are likely to yield better results, which is particularly useful for large datasets where computational resources are limited.\n\nTo optimize hyperparameter tuning for larger datasets, these techniques can be combined with parallel computing and distributed processing frameworks such as Dask, Spark, or multi-GPU setups. This enables more efficient hyperparameter search and reduces the overall computational overhead.\nI choosed Logistc Regression model and trained my Model with best parameters.\nSource Distribution for Model Packaging\nPackaging your machine learning model is a best practice, especially if you don’t plan to update it frequently. Imagine thousands of lines of code that can now be utilized with just a single line—this is the power of model packaging.\n\n1. Project Folder Setup\nTo ensure better organization, I created a main folder called sentiment_prediction and moved all machine learning pipeline files and dependencies into this folder. This helped in maintaining a clean structure and simplified the management of the entire project.\nBefore moving forward I recommend you to visit this pdf it will practically show you step by step process for building python package.\nPDF : step by step guid for python package building\n\n2. Manifest.in\nThe Manifest.in file plays a crucial role in controlling which files and folders should be included or excluded during the packaging process. It helps to specify the structure of the package for distribution.\nKey commands used in the Manifest.in file include:\ninclude <file/folder>: Include specific files or folders.\nexclude <file/folder>: Exclude specific files or folders.\nrecursive-include <path>: Include all files from a directory recursively.\nrecursive-exclude <path>: Exclude all files from a directory recursively.\n3. Setup.py\nThe setup.py file contains the project\'s metadata and is essential for creating the package. It defines key information about the project, such as:\nProject name, version, description, and author details.\nDependencies required for the package (install_requires), making it easy to install all necessary libraries.\n\n4. Building the Package\nTo build the package, I used the following command:\n\npython setup.py sdist bdist_wheel\nThis command generates two folders:\n\nbuild/: Contains the entire project package as defined in the Manifest.in.\ndist/: Contains the distributable files: .whl (wheel file) .gz (compressed source archive)\n\n5. Global Access via GitHub\nNow you can access your package gloablly, by refering your repository. I provided my package below go and check out.\n\nRepository: GitHub Repo Link\nYou can install the package directly from GitHub using the following command:\n\npip install git+https://github.com/vijaytakbhate2002/sentiment_prediction_python_package.git \nTo ensure it worked globally, I tested it again:\n\nfrom sentiment_prediction import predict\nprint(predict.predictor("Great progress shared today!")) \noutput:[\'Negative\']\nFlask Application \nBuilding application will help us to give our NLP model experience to people, so I built one flask application.\nWeb Application UI\n\nHere is demonstration of project: project demo\nI left a blank section for user suggestion and feedback, these feedbacks are getting stored in database for future model analysis or any business work.\nDatabase configuration\nFor storing collected user data we need to configure a database. It will help us to improve model as per user need.\nI used Google Cloud MySQL instance for integrating my application with database, GCP is paid but you can use free credit of GCP for first 3 months, for doing almost all Cloud Work.\nYou need to create your GCP account, then create one instance under SQL and by configuring your local system with instance you are good to go.\nDocker containerization\nIf you are not awared about docker and it\'s basic concepts you can refer my previous article which explains you all about docker.\nThis guide will help you build a solid foundation in Docker, enabling you to confidently use it for your projects.\nDocker guide: Comprehensive Docker guide for deploying Flas app\nDeploy\nDeployment of web app will help us to engange people and provide them real actual experience of our services.\nAfter deployment you need to collect user data and store it for future analysis, this data contain user feedback and suggestions.\nAfter deployment it\'s not end of the process we need to collect user feedback and again follow same steps fine tune Model as per user need.\n\nSummary of Blog\nThis blog covers the lifecycle of a Machine Learning project, from ETL to deployment. It details building an ETL pipeline using PySpark for efficient data handling, EDA, and NLP preprocessing techniques like tokenization, TF-IDF, and WordCloud visualization. \nIt highlights ML experiments with MLflow on Databricks and hyperparameter tuning using Grid Search.\nThe model was packaged into a Python package and deployed as a Flask application with a database backend (Google Cloud MySQL) and Dockerized for scalability. \nThe app collects user feedback for continuous improvement. It emphasizes end-to-end integration, including cloud and containerization, to deliver a robust ML solution.\nHappy Learning!\n\n'}, {'content': '\nLanguages I Speak\nEnglish, Marathi, Hindi\n\n'}, {'content': '\n\nSoft skills:\nCritical Thinking, Intellectual Rigor, Problem Solving, Understanding Business Needs'}]
2024-11-25 21:30:33 - root - INFO - Writing documents to the document store...
2024-11-25 21:30:36 - haystack.document_stores.faiss - INFO - Updating embeddings for 14 docs...
2024-11-25 21:30:43 - root - INFO - Documents and embeddings updated.
2024-11-25 21:30:43 - haystack.modeling.utils - INFO - Using devices: CPU - Number of GPUs: 0
2024-11-25 21:30:43 - haystack.modeling.utils - INFO - Using devices: CPU - Number of GPUs: 0
2024-11-25 21:30:43 - haystack.modeling.model.language_model - INFO -  * LOADING MODEL: 'deepset/roberta-base-squad2' (Roberta)
2024-11-25 21:30:44 - haystack.modeling.model.language_model - INFO - Auto-detected model language: english
2024-11-25 21:30:44 - haystack.modeling.model.language_model - INFO - Loaded 'deepset/roberta-base-squad2' (Roberta model) from model hub.
2024-11-25 21:30:45 - haystack.modeling.utils - INFO - Using devices: CPU - Number of GPUs: 0
2024-11-25 21:30:45 - haystack.modeling.data_handler.processor - ERROR - There were 1 errors during preprocessing at positions: {0}
2024-11-25 21:36:51 - root - INFO - Loading existing FAISS document store...
2024-11-25 21:36:51 - haystack.modeling.utils - INFO - Using devices: CPU - Number of GPUs: 0
2024-11-25 21:36:52 - haystack.modeling.model.language_model - WARNING - Using a model of type 'mpnet' which might be incompatible with DPR encoders. Only Bert-based encoders are supported. They need input_ids, token_type_ids, attention_mask as input tensors.
2024-11-25 21:36:54 - haystack.modeling.model.language_model - INFO - Auto-detected model language: english
2024-11-25 21:36:55 - haystack.modeling.model.language_model - WARNING - Using a model of type 'mpnet' which might be incompatible with DPR encoders. Only Bert-based encoders are supported. They need input_ids, token_type_ids, attention_mask as input tensors.
2024-11-25 21:36:56 - haystack.modeling.model.language_model - INFO - Auto-detected model language: english
2024-11-25 21:36:56 - root - INFO - Reading text document
2024-11-25 21:36:56 - root - INFO - loaded document = [{'content': '\nPersonal Information\n\nName: Vijay Takbhate\nEmail: vijaytakbhate20@gmail.com Phone: 8767363681\nGitHub: https://github.com/vijaytakbhate2002\nLinkedIn: https://www.linkedin.com/in/vijay-takbhate-b9231a236/ Kaggle: https://www.kaggle.com/vijay20213\n\n'}, {'content': '\n\nExperience - Fox Solutions Pvt. Ltd.\nRole: Automation Engineer Duration: Jun 2024 - Oct 2024 Location: Maharashtra\nKey Contributions:\n-Completed 2 months of internship plus 4 months of full-time work.\n-Worked with PLC and SCADA systems, focusing on automating processes and optimizing operational efficiency.\n-Collaborated with cross-functional teams to implement automation solutions for industrial applications.\n\n'}, {'content': '\n\nExperience - Cei Design Consultancy Pvt. Ltd.\nRole: Python Developer Intern Duration: Aug 2024 - Sept 2024 Location: Remote, Maharashtra\nKey Contributions:\n-Specialized in data processing using Python and Excel.\n-Utilized OpenCV for image processing tasks.\n\n'}, {'content': '\n\nExperience - Ujucode\nRole: Subject Matter Expert Intern Duration: Aug 2023 - Oct 2023 Location: Remote, Maharashtra\nKey Contributions:\n-Contributed as a Python developer for a ChatBot project.\n-Handled backend development tasks and researched Python modules.\n\n'}, {'content': '\n\nProject - Twitter Post Sentiment Prediction\nDetails:\n-Engineered an ETL pipeline using PySpark and SQL.\n-Conducted sentiment analysis using NLP (TF-IDF) and optimized hyperparameters.\n-Monitored model performance through MLFlow on Databricks.\n-Leveraged Google Cloud Storage and MySQL for data management.\n-Deployed the model using Docker and hosted it on Render.\n\n'}, {'content': '\n\nProject - Text-Text Chat-Bot\nDetails:\n-Designed an advanced Chat-Bot using the NVIDIA API and prompt engineering.\n-Features include paraphrasing, grammar correction, AI detection, plagiarism checking, and content summarization.\n-Targeted at content creators, researchers, and businesses.\n-Technologies Used: HTML, CSS, Python Flask, Cloud Database, and Render.\n\n'}, {'content': '\n\nProject - Hand Gesture Recognition\nDetails:\n-Used Google-s MediaPipe framework for detecting hand landmarks and gestures.\n-Created and labeled a custom dataset of hand gestures for training.\n-Developed a Streamlit application to improve accessibility and flexibility.\n\n'}, {'content': '\n\nTechnical Skills\nLanguages: MySQL, Python, HTML, CSS\nTechnologies: Streamlit, Flask, VS Code, GitHub, MLflow, Docker, PySpark, Databricks, Google Cloud Platform\n\n'}, {'content': '\n\nCertification\nMLOps Bootcamp: Mastering AI Operations for Success (Jun 2024)\n-Learned about the MLOps lifecycle and modular programming.\n-Acquired skills in Git, Python, Flask, and MLflow.\n\n'}, {'content': '\n\nEducation Details:\nBachelor of Technology in Electronics and Telecommunication (May 2024) Institution: SVERI-s College of Engineering, Pandharpur, Maharashtra Score: 81.71\nDiploma in Electronics and Telecommunication (May 2021) Institution: SVERI-s College of Engineering, Pandharpur, Maharashtra Score: 91.73\n\n'}, {'content': "\n\nBlogging\n\n\nSupervised, Unsupervised, and Beyond: ML Techniques Simplified\nNovember 25, 2024\nThere are several techniques for ML training. Among these, I will cover the following:\n\nSupervised and Unsupervised Learning\nSemi-Supervised Learning\nOffline and Online Learning\nInstance-Based and Model-Based Learning\n\nSupervised and Unsupervised Learning\nSupervised learning is like spoon-feeding our ML model in its initial stages, allowing it to learn and improve over time. Here, I’m referring to the training process.\n\nIn supervised learning, there are input columns and output columns, also called target columns. For example, in spam detection—a classification problem—the input is the email, and the target is whether the email is spam or not. That’s it!\nThis process resembles a student-teacher scenario where the teacher is a human, and the student is the model. The dataset serves as the knowledge used to train the student (model)\nSee content credentials\n\nhumand and model\nIn Unsupervised learning, we are not aware of the data labels. Instead, we separate the input data by analyzing similarities and grouping them into different clusters.\nOnce the clusters are formed, we can assign custom labels to each one. This technique is widely used to identify product relationships in online shopping and to recommend new products based on a user’s purchase history.\n\nUnsupervised Learning Clusters\nHere three clusters with three different image categories are formed\n\nSemi-Supervised Learning\nSemi-Supervised Learning is a combination of supervised and unsupervised learning, where some data is labeled and some data is unlabeled. A good example of this is Google Photos, which automatically separates new photos into their respective groups based on whether they contain a particular person in each image.\n\nThere are several techniques under semi-supervised learning; here, we will focus on the following:\nSelf Learning\nConsistency Regularization\nGenerative Models\nGraph-Based Learning\n\nLet's discuss them one by one:\n\nSelf Learning\nSelf Learning trains a model with labeled data and generates pseudo-labels for the unlabeled data. \n\nSelf learning\nThen, the model is trained on the entire dataset, which includes both the generated pseudo-labels and the labeled data.\n\nConsistency Regularization\nThis technique uses data augmentation to generate similar data, and then the model is enforced to predict the same outcome for both the augmented and original data. It helps create a model that can find similarities in both labeled and unlabeled data, predicting the same output for unlabeled data as it would for labeled data.\n\nData augmentation includes techniques like image flipping, blurring, rotation, etc. After augmenting the data, the model is trained to predict the same class for both the augmented and original images.\n\nGenerative Models\nGenerative models create synthetic data points and learn the underlying structure of the training data. These models can generate new datasets using encoders. \n\nExamples include Generative Adversarial Networks (GANs) and Variational Autoencoders (VAEs).\n\nGraph-Based Learning\nThis technique creates nodes for both labeled and unlabeled data. Similar nodes are then connected to each other.\nFor example, when dealing with images, each image starts as a separate node. If similar images are found based on their features, they are grouped or connected by an edge.\nRefer below graph to understand graph-based learning\nGraph based learning\nHere label 1 is came from labeled image of group 1, simillarly for label 2 and label 3. Finally we came up with three labels that means labeled data has three unique labels.\nBatch (Offline) and Online Learning\nBatch Learning\nBatch learning, also known as offline learning, is a technique where the developer needs to stop the deployed ML model, retrain it on new data, and then redeploy it. \nThis technique is useful when frequent updates are not required. For example, product recommendation systems can often be retrained weekly or even over several days.\nBatch learning can be costly when training the model on the entire dataset every time. However, using incremental learning models can help avoid unnecessary retraining, making the process more efficient.\nHere are some examples on incremental learning models, as of now we are focusing on general concepts so don't go much deeper into these types.\nStochastic Gradient Descent (SGD)\nNaive Bayes (Online Version)\nIncremental Support Vector Machines (ISVM)\nOnline Random Forests\nIncremental Decision Trees\nYou can refer this article to learn more about incremental learning\nHere’s your text with grammatical corrections and improved clarity:\nOnline Learning\nOnline learning is used when an ML model needs to be updated continuously with new inputs, such as in stock price prediction. In this scenario, the model must remain aware of the most recent data.\n\nBatch and online learning\nOnly models capable of incremental learning are used in online learning. Each new instance is fed back into the model to update its internal weights based on the latest input.\n\nInstance-Based and Model-Based Learning\nInstance-Based Learning\nInstance-based learning involves comparing a new input with stored data. If similarity is found, the model returns the label of the corresponding input. This approach uses pattern matching techniques. \nHere model works as a search engin not exactly same but it will find simmilarity from stored data.\nSome common models used in instance-based learning include:\nK-Nearest Neighbors (K-NN) with labels\nLocally Weighted Regression, etc.\nModel-Based Learning\nIn model-based learning, we train the model using a training dataset. During the training process, the model creates its own complex patterns (mathematical equations) to make predictions for future inputs. \nA simple example is the equation y=mx+cy = mx + c, which represents a trained model. Some common models used in model-based learning include:\nLinear Regression\nLogistic Regression\nDecision Tree, etc.\n\nSummary\nMachine Learning techniques can be broadly categorized into supervised, unsupervised, and semi-supervised learning. \nSupervised learning uses labeled data, like spam detection, while unsupervised learning works with unlabeled data, grouping similar points into clusters. \nSemi-supervised learning combines both, employing techniques like self-learning and consistency regularization to leverage partially labeled datasets.\nOther approaches include batch learning, where models are retrained periodically, and online learning, which updates models continuously with new data, suitable for dynamic tasks like stock price prediction. \nInstance-based learning relies on pattern matching (e.g., K-Nearest Neighbors), while model-based learning creates mathematical models (e.g., Linear Regression) to make predictions.\nThis is all about Machine Learning techniques. If you learned something, let me know in the comments. Your suggestions will help me improve my blogs.\nThanks for reading!\n\n"}, {'content': '\nBlogging\n\nMastering the End-to-End Machine Learning Lifecycle: From Data to Deployment\n\nNovember 24, 2024\n This article takes you through the complete lifecycle of a Machine Learning project. From ETL to deployment, I’ll share every detail of how I brought this project to life.\nData Engineering\nData Engineering serves as the starting point in the Machine Learning project lifecycle, bringing all distributed data together in one place.\nFor this project, I utilized an ETL pipeline—a core concept in Data Engineering. It enabled me to extract raw data, transform it into a meaningful format, and load it into a suitable location for further processing.\nLet\'s deep dive into ETL pipeline\nETL stands for Extract, Transform, and Load. These pipelines can be executed periodically (e.g., daily or hourly) to fetch real-time data, enhancing the predictive power of our Machine Learning model for real-world applications.\nThis process can be automated using tools like Apache Airflow, Kubeflow Pipelines, AWS Step Functions, and more, streamlining the workflow for consistent and efficient data updates.\n\nETL Pipeline:\nETL Pipeline\nExtract Operation\nThe extract operation is responsible for fetching data from various sources such as websites, databases, and APIs, as illustrated in the flowchart above.\n\nTransform Operation\nThe transform operation focuses on data cleaning, feature selection, and manipulation. In my project, I used this step to extract only the required features, ensuring the data is ready for the next phase.\n\nLoad Operation\nThe load operation transfers the transformed data to its destination, where we can choose the appropriate storage format. Typically, the processed data is stored in a database for further analysis and model training.\n\nFor my project, I used PySpark to build the ETL pipeline, as it enables efficient processing of large datasets.\nWhy not Pandas?\nWhile Pandas is excellent for small to medium-sized datasets, it stores DataFrames in RAM, which can lead to out-of-memory exceptions when handling large datasets.\nIn contrast, PySpark creates a session and processes data in chunks, storing it in ROM, making it ideal for handling large-scale data.\nFor the actual implementation of the pipeline using PySpark, please refer to the accompanying jupyter notebook.\nEDA (Data Analysis)\nData understanding is a critical stage before building any Machine Learning model. It allows us to analyze the data, plan the data processing steps, and gain insights into its structure and quality.\nIn this project, I examined the balance of data in my training and testing datasets and found it to be well-distributed across all four categories.\n\nBalanceness Checking on train and test data\nThroughout the entire process, I focused on two columns:\nTweets\nSentiments\nThe Tweets column contains the raw Twitter text data, while the Sentiments column serves as the target variable for prediction.\nNatural Language Processing\nI applied several key NLP techniques to preprocess the data and prepare it for Machine Learning model building. Below are the main steps I executed:\n\nText Processing\nRegex: I applied regular expressions to clean the text by removing URLs, hashtags, HTML tags, and keeping only alphanumeric characters. This helped eliminate unnecessary noise from the data.\nNLTK: Using the Natural Language Toolkit (NLTK), I performed word tokenization, stemming and lemmatization.\nWord tokenization is just splitting sentence into words, so we can processing each word from sentence individually.\nStemming helps us to truncate prefix or suffix of text to reduce count on unique words from corpus (paragraph).\nLemmatization is the proecss of converting any word into it\'s base word, for eg. Played will convert to play.\n\nWordCloud \nWord cloud concept help you to understand importance of words from given data. I splitted my data into four sections.\ndata for negative sentiments\ndata for positive sentiments\ndata for neutral sentiments\ndata for irrelevant sentiments\n\nHere is the representation of most frequent words for each category.\nCategorywise Word Cloud Presentation\nHere you can clearly see that there is no much difference in negative and positive sentiments data.\nthis is representation of bad data, here we can filterout our data for further processing, it might reduce data but you can do data augmentation techniques here to increase your data.\naugmenting of data in NLP with TF-IDF will not bad idea because TF-IDF and any other porcessing technique that I used is not able to detect sentence grammer or it doesn,t require sophisticated text. \nYou can think our input text will work as bag of word for model. you can understand by refering below vectorization technique.\nVectorization with TF-IDF \nI used Term Frequency-Inverse Document Frequency (TF-IDF) for text vectorization. This method transforms text into a numerical format, considering the importance of each word across documents, which prevents frequent words from dominating the model.\n\nTF-IDF\nTF-IDF stands for Term Frequency-Inverse Document Frequency. \nTerm Frequency (TF): This measures how frequently a term (t) appears in a specific document (d). It\'s calculated by dividing the number of occurrences of the term in the document by the total number of terms in that document.\nInverse Document Frequency (IDF): This measures how important a term is across the entire collection of documents. It\'s calculated by taking the logarithm of the ratio of the total number of documents (N) to the number of documents containing the term (df(t)). A higher IDF value indicates a rarer term, making it more significant.\nHere is the resulant data we got from TF-IDF\nAfter all preprocessing of text it\'s time to save our data for mlflow experiments. To see actual implementation of end to end process of NLP you can refer jupyter notebook.\nmlflow experiments\nMLflow experiments allow you to conduct multiple experiments with your trained model, helping you track and compare results over time. For running MLflow experiments, I prefer using Databricks as it offers an integrated experiment section, making the process much more streamlined and efficient.\nIn Databricks, you can easily connect your experiment by passing the experiment ID into the MLflow code. This integration simplifies the entire workflow, enabling better experiment tracking and easier comparison of model performance.\nFor my experiments, I worked with several models, including Logistic Regression, Multinomial Naive Bayes, and Decision Tree Classifier. By applying different combinations of parameters, I was able to experiment with and compare the performance of each model. Here\'s a preview of the Logistic Regression model’s F1 score, which highlights the model\'s ability to balance precision and recall:\nLogistic Regression F1 Score: 0.58\nThis approach allowed me to track the effectiveness of each model and make adjustments as needed for improving performance.\n\nF1 score with different parameters (Logistic Regression)\nI recommend running the notebook below in your own account to see the results. You\'ll definitely start appreciating the power of MLflow.\nFor more information, please refer to the accompanying Databricks notebook.\n\nHyperparameter tuning\nHyperparameter tuning is crucial for identifying the best parameter combination for your model. This technique involves an iterative process where, for every parameter combination, the model is trained and tested on a dataset. It is often described as a "trial and error" method.\nHowever, this approach can be computationally expensive, especially when working with complex or heavy machine learning models. For large-scale problems, hyperparameter tuning can be made more efficient through sampling or batch methods. In these methods, you don\'t use the entire dataset for training the model; instead, you choose random or stratified data points from the dataset to train the model. Although this may slightly reduce accuracy, it is more feasible when working with large datasets.\nFor my project, I used Grid Search CV to find the best hyperparameter combination for the model. Below are some common techniques for hyperparameter tuning, especially for large datasets:\nGrid Search This technique exhaustively searches through a specified set of hyperparameter values, trying all possible combinations. While effective, it can be computationally expensive for larger datasets due to the exhaustive nature of the search.\nRandom Search Randomly samples from the hyperparameter space, offering a faster alternative to grid search. This method explores a wider range of hyperparameters with fewer evaluations, making it more efficient for larger datasets.\nBayesian Optimization This method uses probabilistic models to predict the performance of different hyperparameters. It selects the next set of hyperparameters to evaluate based on previous results, making it more efficient and suitable for large datasets.\nGenetic Algorithms Inspired by natural selection, these algorithms iteratively evolve a population of hyperparameter sets to improve model performance. This method works well with complex search spaces.\nHyperband Hyperband combines random search with early stopping to dynamically allocate resources across multiple configurations, identifying promising hyperparameters quickly without excessive computational costs.\nBayesian Optimization with Gaussian Processes This technique models the hyperparameter search space using Gaussian processes, focusing on regions that are likely to yield better results, which is particularly useful for large datasets where computational resources are limited.\n\nTo optimize hyperparameter tuning for larger datasets, these techniques can be combined with parallel computing and distributed processing frameworks such as Dask, Spark, or multi-GPU setups. This enables more efficient hyperparameter search and reduces the overall computational overhead.\nI choosed Logistc Regression model and trained my Model with best parameters.\nSource Distribution for Model Packaging\nPackaging your machine learning model is a best practice, especially if you don’t plan to update it frequently. Imagine thousands of lines of code that can now be utilized with just a single line—this is the power of model packaging.\n\n1. Project Folder Setup\nTo ensure better organization, I created a main folder called sentiment_prediction and moved all machine learning pipeline files and dependencies into this folder. This helped in maintaining a clean structure and simplified the management of the entire project.\nBefore moving forward I recommend you to visit this pdf it will practically show you step by step process for building python package.\nPDF : step by step guid for python package building\n\n2. Manifest.in\nThe Manifest.in file plays a crucial role in controlling which files and folders should be included or excluded during the packaging process. It helps to specify the structure of the package for distribution.\nKey commands used in the Manifest.in file include:\ninclude <file/folder>: Include specific files or folders.\nexclude <file/folder>: Exclude specific files or folders.\nrecursive-include <path>: Include all files from a directory recursively.\nrecursive-exclude <path>: Exclude all files from a directory recursively.\n3. Setup.py\nThe setup.py file contains the project\'s metadata and is essential for creating the package. It defines key information about the project, such as:\nProject name, version, description, and author details.\nDependencies required for the package (install_requires), making it easy to install all necessary libraries.\n\n4. Building the Package\nTo build the package, I used the following command:\n\npython setup.py sdist bdist_wheel\nThis command generates two folders:\n\nbuild/: Contains the entire project package as defined in the Manifest.in.\ndist/: Contains the distributable files: .whl (wheel file) .gz (compressed source archive)\n\n5. Global Access via GitHub\nNow you can access your package gloablly, by refering your repository. I provided my package below go and check out.\n\nRepository: GitHub Repo Link\nYou can install the package directly from GitHub using the following command:\n\npip install git+https://github.com/vijaytakbhate2002/sentiment_prediction_python_package.git \nTo ensure it worked globally, I tested it again:\n\nfrom sentiment_prediction import predict\nprint(predict.predictor("Great progress shared today!")) \noutput:[\'Negative\']\nFlask Application \nBuilding application will help us to give our NLP model experience to people, so I built one flask application.\nWeb Application UI\n\nHere is demonstration of project: project demo\nI left a blank section for user suggestion and feedback, these feedbacks are getting stored in database for future model analysis or any business work.\nDatabase configuration\nFor storing collected user data we need to configure a database. It will help us to improve model as per user need.\nI used Google Cloud MySQL instance for integrating my application with database, GCP is paid but you can use free credit of GCP for first 3 months, for doing almost all Cloud Work.\nYou need to create your GCP account, then create one instance under SQL and by configuring your local system with instance you are good to go.\nDocker containerization\nIf you are not awared about docker and it\'s basic concepts you can refer my previous article which explains you all about docker.\nThis guide will help you build a solid foundation in Docker, enabling you to confidently use it for your projects.\nDocker guide: Comprehensive Docker guide for deploying Flas app\nDeploy\nDeployment of web app will help us to engange people and provide them real actual experience of our services.\nAfter deployment you need to collect user data and store it for future analysis, this data contain user feedback and suggestions.\nAfter deployment it\'s not end of the process we need to collect user feedback and again follow same steps fine tune Model as per user need.\n\nSummary of Blog\nThis blog covers the lifecycle of a Machine Learning project, from ETL to deployment. It details building an ETL pipeline using PySpark for efficient data handling, EDA, and NLP preprocessing techniques like tokenization, TF-IDF, and WordCloud visualization. \nIt highlights ML experiments with MLflow on Databricks and hyperparameter tuning using Grid Search.\nThe model was packaged into a Python package and deployed as a Flask application with a database backend (Google Cloud MySQL) and Dockerized for scalability. \nThe app collects user feedback for continuous improvement. It emphasizes end-to-end integration, including cloud and containerization, to deliver a robust ML solution.\nHappy Learning!\n\n'}, {'content': '\nLanguages I Speak\nEnglish, Marathi, Hindi\n\n'}, {'content': '\n\nSoft skills:\nCritical Thinking, Intellectual Rigor, Problem Solving, Understanding Business Needs'}]
2024-11-25 21:36:56 - root - INFO - Document store already contains 14 documents. Skipping write.
2024-11-25 21:36:56 - haystack.modeling.utils - INFO - Using devices: CPU - Number of GPUs: 0
2024-11-25 21:36:56 - haystack.modeling.utils - INFO - Using devices: CPU - Number of GPUs: 0
2024-11-25 21:36:57 - haystack.modeling.model.language_model - INFO -  * LOADING MODEL: 'deepset/roberta-base-squad2' (Roberta)
2024-11-25 21:36:58 - haystack.modeling.model.language_model - INFO - Auto-detected model language: english
2024-11-25 21:36:58 - haystack.modeling.model.language_model - INFO - Loaded 'deepset/roberta-base-squad2' (Roberta model) from model hub.
2024-11-25 21:36:59 - haystack.modeling.utils - INFO - Using devices: CPU - Number of GPUs: 0
2024-11-25 21:36:59 - haystack.modeling.data_handler.processor - ERROR - There were 1 errors during preprocessing at positions: {0}
2024-11-25 21:38:41 - root - INFO - Loading existing FAISS document store...
2024-11-25 21:38:41 - haystack.modeling.utils - INFO - Using devices: CPU - Number of GPUs: 0
2024-11-25 21:38:43 - haystack.modeling.model.language_model - WARNING - Using a model of type 'mpnet' which might be incompatible with DPR encoders. Only Bert-based encoders are supported. They need input_ids, token_type_ids, attention_mask as input tensors.
2024-11-25 21:38:45 - haystack.modeling.model.language_model - INFO - Auto-detected model language: english
2024-11-25 21:38:46 - haystack.modeling.model.language_model - WARNING - Using a model of type 'mpnet' which might be incompatible with DPR encoders. Only Bert-based encoders are supported. They need input_ids, token_type_ids, attention_mask as input tensors.
2024-11-25 21:38:47 - haystack.modeling.model.language_model - INFO - Auto-detected model language: english
2024-11-25 21:38:47 - root - INFO - Reading text document
2024-11-25 21:38:47 - root - INFO - loaded document = [{'content': '\nPersonal Information\n\nName: Vijay Takbhate\nEmail: vijaytakbhate20@gmail.com Phone: 8767363681\nGitHub: https://github.com/vijaytakbhate2002\nLinkedIn: https://www.linkedin.com/in/vijay-takbhate-b9231a236/ Kaggle: https://www.kaggle.com/vijay20213\n\n'}, {'content': '\n\nExperience - Fox Solutions Pvt. Ltd.\nRole: Automation Engineer Duration: Jun 2024 - Oct 2024 Location: Maharashtra\nKey Contributions:\n-Completed 2 months of internship plus 4 months of full-time work.\n-Worked with PLC and SCADA systems, focusing on automating processes and optimizing operational efficiency.\n-Collaborated with cross-functional teams to implement automation solutions for industrial applications.\n\n'}, {'content': '\n\nExperience - Cei Design Consultancy Pvt. Ltd.\nRole: Python Developer Intern Duration: Aug 2024 - Sept 2024 Location: Remote, Maharashtra\nKey Contributions:\n-Specialized in data processing using Python and Excel.\n-Utilized OpenCV for image processing tasks.\n\n'}, {'content': '\n\nExperience - Ujucode\nRole: Subject Matter Expert Intern Duration: Aug 2023 - Oct 2023 Location: Remote, Maharashtra\nKey Contributions:\n-Contributed as a Python developer for a ChatBot project.\n-Handled backend development tasks and researched Python modules.\n\n'}, {'content': '\n\nProject - Twitter Post Sentiment Prediction\nDetails:\n-Engineered an ETL pipeline using PySpark and SQL.\n-Conducted sentiment analysis using NLP (TF-IDF) and optimized hyperparameters.\n-Monitored model performance through MLFlow on Databricks.\n-Leveraged Google Cloud Storage and MySQL for data management.\n-Deployed the model using Docker and hosted it on Render.\n\n'}, {'content': '\n\nProject - Text-Text Chat-Bot\nDetails:\n-Designed an advanced Chat-Bot using the NVIDIA API and prompt engineering.\n-Features include paraphrasing, grammar correction, AI detection, plagiarism checking, and content summarization.\n-Targeted at content creators, researchers, and businesses.\n-Technologies Used: HTML, CSS, Python Flask, Cloud Database, and Render.\n\n'}, {'content': '\n\nProject - Hand Gesture Recognition\nDetails:\n-Used Google-s MediaPipe framework for detecting hand landmarks and gestures.\n-Created and labeled a custom dataset of hand gestures for training.\n-Developed a Streamlit application to improve accessibility and flexibility.\n\n'}, {'content': '\n\nTechnical Skills\nLanguages: MySQL, Python, HTML, CSS\nTechnologies: Streamlit, Flask, VS Code, GitHub, MLflow, Docker, PySpark, Databricks, Google Cloud Platform\n\n'}, {'content': '\n\nCertification\nMLOps Bootcamp: Mastering AI Operations for Success (Jun 2024)\n-Learned about the MLOps lifecycle and modular programming.\n-Acquired skills in Git, Python, Flask, and MLflow.\n\n'}, {'content': '\n\nEducation Details:\nBachelor of Technology in Electronics and Telecommunication (May 2024) Institution: SVERI-s College of Engineering, Pandharpur, Maharashtra Score: 81.71\nDiploma in Electronics and Telecommunication (May 2021) Institution: SVERI-s College of Engineering, Pandharpur, Maharashtra Score: 91.73\n\n'}, {'content': "\n\nBlogging\n\n\nSupervised, Unsupervised, and Beyond: ML Techniques Simplified\nNovember 25, 2024\nThere are several techniques for ML training. Among these, I will cover the following:\n\nSupervised and Unsupervised Learning\nSemi-Supervised Learning\nOffline and Online Learning\nInstance-Based and Model-Based Learning\n\nSupervised and Unsupervised Learning\nSupervised learning is like spoon-feeding our ML model in its initial stages, allowing it to learn and improve over time. Here, I’m referring to the training process.\n\nIn supervised learning, there are input columns and output columns, also called target columns. For example, in spam detection—a classification problem—the input is the email, and the target is whether the email is spam or not. That’s it!\nThis process resembles a student-teacher scenario where the teacher is a human, and the student is the model. The dataset serves as the knowledge used to train the student (model)\nSee content credentials\n\nhumand and model\nIn Unsupervised learning, we are not aware of the data labels. Instead, we separate the input data by analyzing similarities and grouping them into different clusters.\nOnce the clusters are formed, we can assign custom labels to each one. This technique is widely used to identify product relationships in online shopping and to recommend new products based on a user’s purchase history.\n\nUnsupervised Learning Clusters\nHere three clusters with three different image categories are formed\n\nSemi-Supervised Learning\nSemi-Supervised Learning is a combination of supervised and unsupervised learning, where some data is labeled and some data is unlabeled. A good example of this is Google Photos, which automatically separates new photos into their respective groups based on whether they contain a particular person in each image.\n\nThere are several techniques under semi-supervised learning; here, we will focus on the following:\nSelf Learning\nConsistency Regularization\nGenerative Models\nGraph-Based Learning\n\nLet's discuss them one by one:\n\nSelf Learning\nSelf Learning trains a model with labeled data and generates pseudo-labels for the unlabeled data. \n\nSelf learning\nThen, the model is trained on the entire dataset, which includes both the generated pseudo-labels and the labeled data.\n\nConsistency Regularization\nThis technique uses data augmentation to generate similar data, and then the model is enforced to predict the same outcome for both the augmented and original data. It helps create a model that can find similarities in both labeled and unlabeled data, predicting the same output for unlabeled data as it would for labeled data.\n\nData augmentation includes techniques like image flipping, blurring, rotation, etc. After augmenting the data, the model is trained to predict the same class for both the augmented and original images.\n\nGenerative Models\nGenerative models create synthetic data points and learn the underlying structure of the training data. These models can generate new datasets using encoders. \n\nExamples include Generative Adversarial Networks (GANs) and Variational Autoencoders (VAEs).\n\nGraph-Based Learning\nThis technique creates nodes for both labeled and unlabeled data. Similar nodes are then connected to each other.\nFor example, when dealing with images, each image starts as a separate node. If similar images are found based on their features, they are grouped or connected by an edge.\nRefer below graph to understand graph-based learning\nGraph based learning\nHere label 1 is came from labeled image of group 1, simillarly for label 2 and label 3. Finally we came up with three labels that means labeled data has three unique labels.\nBatch (Offline) and Online Learning\nBatch Learning\nBatch learning, also known as offline learning, is a technique where the developer needs to stop the deployed ML model, retrain it on new data, and then redeploy it. \nThis technique is useful when frequent updates are not required. For example, product recommendation systems can often be retrained weekly or even over several days.\nBatch learning can be costly when training the model on the entire dataset every time. However, using incremental learning models can help avoid unnecessary retraining, making the process more efficient.\nHere are some examples on incremental learning models, as of now we are focusing on general concepts so don't go much deeper into these types.\nStochastic Gradient Descent (SGD)\nNaive Bayes (Online Version)\nIncremental Support Vector Machines (ISVM)\nOnline Random Forests\nIncremental Decision Trees\nYou can refer this article to learn more about incremental learning\nHere’s your text with grammatical corrections and improved clarity:\nOnline Learning\nOnline learning is used when an ML model needs to be updated continuously with new inputs, such as in stock price prediction. In this scenario, the model must remain aware of the most recent data.\n\nBatch and online learning\nOnly models capable of incremental learning are used in online learning. Each new instance is fed back into the model to update its internal weights based on the latest input.\n\nInstance-Based and Model-Based Learning\nInstance-Based Learning\nInstance-based learning involves comparing a new input with stored data. If similarity is found, the model returns the label of the corresponding input. This approach uses pattern matching techniques. \nHere model works as a search engin not exactly same but it will find simmilarity from stored data.\nSome common models used in instance-based learning include:\nK-Nearest Neighbors (K-NN) with labels\nLocally Weighted Regression, etc.\nModel-Based Learning\nIn model-based learning, we train the model using a training dataset. During the training process, the model creates its own complex patterns (mathematical equations) to make predictions for future inputs. \nA simple example is the equation y=mx+cy = mx + c, which represents a trained model. Some common models used in model-based learning include:\nLinear Regression\nLogistic Regression\nDecision Tree, etc.\n\nSummary\nMachine Learning techniques can be broadly categorized into supervised, unsupervised, and semi-supervised learning. \nSupervised learning uses labeled data, like spam detection, while unsupervised learning works with unlabeled data, grouping similar points into clusters. \nSemi-supervised learning combines both, employing techniques like self-learning and consistency regularization to leverage partially labeled datasets.\nOther approaches include batch learning, where models are retrained periodically, and online learning, which updates models continuously with new data, suitable for dynamic tasks like stock price prediction. \nInstance-based learning relies on pattern matching (e.g., K-Nearest Neighbors), while model-based learning creates mathematical models (e.g., Linear Regression) to make predictions.\nThis is all about Machine Learning techniques. If you learned something, let me know in the comments. Your suggestions will help me improve my blogs.\nThanks for reading!\n\n"}, {'content': '\nBlogging\n\nMastering the End-to-End Machine Learning Lifecycle: From Data to Deployment\n\nNovember 24, 2024\n This article takes you through the complete lifecycle of a Machine Learning project. From ETL to deployment, I’ll share every detail of how I brought this project to life.\nData Engineering\nData Engineering serves as the starting point in the Machine Learning project lifecycle, bringing all distributed data together in one place.\nFor this project, I utilized an ETL pipeline—a core concept in Data Engineering. It enabled me to extract raw data, transform it into a meaningful format, and load it into a suitable location for further processing.\nLet\'s deep dive into ETL pipeline\nETL stands for Extract, Transform, and Load. These pipelines can be executed periodically (e.g., daily or hourly) to fetch real-time data, enhancing the predictive power of our Machine Learning model for real-world applications.\nThis process can be automated using tools like Apache Airflow, Kubeflow Pipelines, AWS Step Functions, and more, streamlining the workflow for consistent and efficient data updates.\n\nETL Pipeline:\nETL Pipeline\nExtract Operation\nThe extract operation is responsible for fetching data from various sources such as websites, databases, and APIs, as illustrated in the flowchart above.\n\nTransform Operation\nThe transform operation focuses on data cleaning, feature selection, and manipulation. In my project, I used this step to extract only the required features, ensuring the data is ready for the next phase.\n\nLoad Operation\nThe load operation transfers the transformed data to its destination, where we can choose the appropriate storage format. Typically, the processed data is stored in a database for further analysis and model training.\n\nFor my project, I used PySpark to build the ETL pipeline, as it enables efficient processing of large datasets.\nWhy not Pandas?\nWhile Pandas is excellent for small to medium-sized datasets, it stores DataFrames in RAM, which can lead to out-of-memory exceptions when handling large datasets.\nIn contrast, PySpark creates a session and processes data in chunks, storing it in ROM, making it ideal for handling large-scale data.\nFor the actual implementation of the pipeline using PySpark, please refer to the accompanying jupyter notebook.\nEDA (Data Analysis)\nData understanding is a critical stage before building any Machine Learning model. It allows us to analyze the data, plan the data processing steps, and gain insights into its structure and quality.\nIn this project, I examined the balance of data in my training and testing datasets and found it to be well-distributed across all four categories.\n\nBalanceness Checking on train and test data\nThroughout the entire process, I focused on two columns:\nTweets\nSentiments\nThe Tweets column contains the raw Twitter text data, while the Sentiments column serves as the target variable for prediction.\nNatural Language Processing\nI applied several key NLP techniques to preprocess the data and prepare it for Machine Learning model building. Below are the main steps I executed:\n\nText Processing\nRegex: I applied regular expressions to clean the text by removing URLs, hashtags, HTML tags, and keeping only alphanumeric characters. This helped eliminate unnecessary noise from the data.\nNLTK: Using the Natural Language Toolkit (NLTK), I performed word tokenization, stemming and lemmatization.\nWord tokenization is just splitting sentence into words, so we can processing each word from sentence individually.\nStemming helps us to truncate prefix or suffix of text to reduce count on unique words from corpus (paragraph).\nLemmatization is the proecss of converting any word into it\'s base word, for eg. Played will convert to play.\n\nWordCloud \nWord cloud concept help you to understand importance of words from given data. I splitted my data into four sections.\ndata for negative sentiments\ndata for positive sentiments\ndata for neutral sentiments\ndata for irrelevant sentiments\n\nHere is the representation of most frequent words for each category.\nCategorywise Word Cloud Presentation\nHere you can clearly see that there is no much difference in negative and positive sentiments data.\nthis is representation of bad data, here we can filterout our data for further processing, it might reduce data but you can do data augmentation techniques here to increase your data.\naugmenting of data in NLP with TF-IDF will not bad idea because TF-IDF and any other porcessing technique that I used is not able to detect sentence grammer or it doesn,t require sophisticated text. \nYou can think our input text will work as bag of word for model. you can understand by refering below vectorization technique.\nVectorization with TF-IDF \nI used Term Frequency-Inverse Document Frequency (TF-IDF) for text vectorization. This method transforms text into a numerical format, considering the importance of each word across documents, which prevents frequent words from dominating the model.\n\nTF-IDF\nTF-IDF stands for Term Frequency-Inverse Document Frequency. \nTerm Frequency (TF): This measures how frequently a term (t) appears in a specific document (d). It\'s calculated by dividing the number of occurrences of the term in the document by the total number of terms in that document.\nInverse Document Frequency (IDF): This measures how important a term is across the entire collection of documents. It\'s calculated by taking the logarithm of the ratio of the total number of documents (N) to the number of documents containing the term (df(t)). A higher IDF value indicates a rarer term, making it more significant.\nHere is the resulant data we got from TF-IDF\nAfter all preprocessing of text it\'s time to save our data for mlflow experiments. To see actual implementation of end to end process of NLP you can refer jupyter notebook.\nmlflow experiments\nMLflow experiments allow you to conduct multiple experiments with your trained model, helping you track and compare results over time. For running MLflow experiments, I prefer using Databricks as it offers an integrated experiment section, making the process much more streamlined and efficient.\nIn Databricks, you can easily connect your experiment by passing the experiment ID into the MLflow code. This integration simplifies the entire workflow, enabling better experiment tracking and easier comparison of model performance.\nFor my experiments, I worked with several models, including Logistic Regression, Multinomial Naive Bayes, and Decision Tree Classifier. By applying different combinations of parameters, I was able to experiment with and compare the performance of each model. Here\'s a preview of the Logistic Regression model’s F1 score, which highlights the model\'s ability to balance precision and recall:\nLogistic Regression F1 Score: 0.58\nThis approach allowed me to track the effectiveness of each model and make adjustments as needed for improving performance.\n\nF1 score with different parameters (Logistic Regression)\nI recommend running the notebook below in your own account to see the results. You\'ll definitely start appreciating the power of MLflow.\nFor more information, please refer to the accompanying Databricks notebook.\n\nHyperparameter tuning\nHyperparameter tuning is crucial for identifying the best parameter combination for your model. This technique involves an iterative process where, for every parameter combination, the model is trained and tested on a dataset. It is often described as a "trial and error" method.\nHowever, this approach can be computationally expensive, especially when working with complex or heavy machine learning models. For large-scale problems, hyperparameter tuning can be made more efficient through sampling or batch methods. In these methods, you don\'t use the entire dataset for training the model; instead, you choose random or stratified data points from the dataset to train the model. Although this may slightly reduce accuracy, it is more feasible when working with large datasets.\nFor my project, I used Grid Search CV to find the best hyperparameter combination for the model. Below are some common techniques for hyperparameter tuning, especially for large datasets:\nGrid Search This technique exhaustively searches through a specified set of hyperparameter values, trying all possible combinations. While effective, it can be computationally expensive for larger datasets due to the exhaustive nature of the search.\nRandom Search Randomly samples from the hyperparameter space, offering a faster alternative to grid search. This method explores a wider range of hyperparameters with fewer evaluations, making it more efficient for larger datasets.\nBayesian Optimization This method uses probabilistic models to predict the performance of different hyperparameters. It selects the next set of hyperparameters to evaluate based on previous results, making it more efficient and suitable for large datasets.\nGenetic Algorithms Inspired by natural selection, these algorithms iteratively evolve a population of hyperparameter sets to improve model performance. This method works well with complex search spaces.\nHyperband Hyperband combines random search with early stopping to dynamically allocate resources across multiple configurations, identifying promising hyperparameters quickly without excessive computational costs.\nBayesian Optimization with Gaussian Processes This technique models the hyperparameter search space using Gaussian processes, focusing on regions that are likely to yield better results, which is particularly useful for large datasets where computational resources are limited.\n\nTo optimize hyperparameter tuning for larger datasets, these techniques can be combined with parallel computing and distributed processing frameworks such as Dask, Spark, or multi-GPU setups. This enables more efficient hyperparameter search and reduces the overall computational overhead.\nI choosed Logistc Regression model and trained my Model with best parameters.\nSource Distribution for Model Packaging\nPackaging your machine learning model is a best practice, especially if you don’t plan to update it frequently. Imagine thousands of lines of code that can now be utilized with just a single line—this is the power of model packaging.\n\n1. Project Folder Setup\nTo ensure better organization, I created a main folder called sentiment_prediction and moved all machine learning pipeline files and dependencies into this folder. This helped in maintaining a clean structure and simplified the management of the entire project.\nBefore moving forward I recommend you to visit this pdf it will practically show you step by step process for building python package.\nPDF : step by step guid for python package building\n\n2. Manifest.in\nThe Manifest.in file plays a crucial role in controlling which files and folders should be included or excluded during the packaging process. It helps to specify the structure of the package for distribution.\nKey commands used in the Manifest.in file include:\ninclude <file/folder>: Include specific files or folders.\nexclude <file/folder>: Exclude specific files or folders.\nrecursive-include <path>: Include all files from a directory recursively.\nrecursive-exclude <path>: Exclude all files from a directory recursively.\n3. Setup.py\nThe setup.py file contains the project\'s metadata and is essential for creating the package. It defines key information about the project, such as:\nProject name, version, description, and author details.\nDependencies required for the package (install_requires), making it easy to install all necessary libraries.\n\n4. Building the Package\nTo build the package, I used the following command:\n\npython setup.py sdist bdist_wheel\nThis command generates two folders:\n\nbuild/: Contains the entire project package as defined in the Manifest.in.\ndist/: Contains the distributable files: .whl (wheel file) .gz (compressed source archive)\n\n5. Global Access via GitHub\nNow you can access your package gloablly, by refering your repository. I provided my package below go and check out.\n\nRepository: GitHub Repo Link\nYou can install the package directly from GitHub using the following command:\n\npip install git+https://github.com/vijaytakbhate2002/sentiment_prediction_python_package.git \nTo ensure it worked globally, I tested it again:\n\nfrom sentiment_prediction import predict\nprint(predict.predictor("Great progress shared today!")) \noutput:[\'Negative\']\nFlask Application \nBuilding application will help us to give our NLP model experience to people, so I built one flask application.\nWeb Application UI\n\nHere is demonstration of project: project demo\nI left a blank section for user suggestion and feedback, these feedbacks are getting stored in database for future model analysis or any business work.\nDatabase configuration\nFor storing collected user data we need to configure a database. It will help us to improve model as per user need.\nI used Google Cloud MySQL instance for integrating my application with database, GCP is paid but you can use free credit of GCP for first 3 months, for doing almost all Cloud Work.\nYou need to create your GCP account, then create one instance under SQL and by configuring your local system with instance you are good to go.\nDocker containerization\nIf you are not awared about docker and it\'s basic concepts you can refer my previous article which explains you all about docker.\nThis guide will help you build a solid foundation in Docker, enabling you to confidently use it for your projects.\nDocker guide: Comprehensive Docker guide for deploying Flas app\nDeploy\nDeployment of web app will help us to engange people and provide them real actual experience of our services.\nAfter deployment you need to collect user data and store it for future analysis, this data contain user feedback and suggestions.\nAfter deployment it\'s not end of the process we need to collect user feedback and again follow same steps fine tune Model as per user need.\n\nSummary of Blog\nThis blog covers the lifecycle of a Machine Learning project, from ETL to deployment. It details building an ETL pipeline using PySpark for efficient data handling, EDA, and NLP preprocessing techniques like tokenization, TF-IDF, and WordCloud visualization. \nIt highlights ML experiments with MLflow on Databricks and hyperparameter tuning using Grid Search.\nThe model was packaged into a Python package and deployed as a Flask application with a database backend (Google Cloud MySQL) and Dockerized for scalability. \nThe app collects user feedback for continuous improvement. It emphasizes end-to-end integration, including cloud and containerization, to deliver a robust ML solution.\nHappy Learning!\n\n'}, {'content': '\nLanguages I Speak\nEnglish, Marathi, Hindi\n\n'}, {'content': '\n\nSoft skills:\nCritical Thinking, Intellectual Rigor, Problem Solving, Understanding Business Needs'}]
2024-11-25 21:38:47 - root - INFO - Document store already contains 14 documents. Skipping write.
2024-11-25 21:38:47 - haystack.modeling.utils - INFO - Using devices: CPU - Number of GPUs: 0
2024-11-25 21:38:47 - haystack.modeling.utils - INFO - Using devices: CPU - Number of GPUs: 0
2024-11-25 21:38:48 - haystack.modeling.model.language_model - INFO -  * LOADING MODEL: 'deepset/roberta-base-squad2' (Roberta)
2024-11-25 21:38:49 - haystack.modeling.model.language_model - INFO - Auto-detected model language: english
2024-11-25 21:38:49 - haystack.modeling.model.language_model - INFO - Loaded 'deepset/roberta-base-squad2' (Roberta model) from model hub.
2024-11-25 21:38:51 - haystack.modeling.utils - INFO - Using devices: CPU - Number of GPUs: 0
2024-11-25 21:38:51 - haystack.modeling.data_handler.processor - ERROR - There were 1 errors during preprocessing at positions: {0}
2024-11-25 21:39:43 - root - INFO - Loading existing FAISS document store...
2024-11-25 21:39:43 - haystack.modeling.utils - INFO - Using devices: CPU - Number of GPUs: 0
2024-11-25 21:39:44 - haystack.modeling.model.language_model - WARNING - Using a model of type 'mpnet' which might be incompatible with DPR encoders. Only Bert-based encoders are supported. They need input_ids, token_type_ids, attention_mask as input tensors.
2024-11-25 21:39:46 - haystack.modeling.model.language_model - INFO - Auto-detected model language: english
2024-11-25 21:39:47 - haystack.modeling.model.language_model - WARNING - Using a model of type 'mpnet' which might be incompatible with DPR encoders. Only Bert-based encoders are supported. They need input_ids, token_type_ids, attention_mask as input tensors.
2024-11-25 21:39:49 - haystack.modeling.model.language_model - INFO - Auto-detected model language: english
2024-11-25 21:39:49 - root - INFO - Reading text document
2024-11-25 21:39:49 - root - INFO - loaded document = [{'content': '\nPersonal Information\n\nName: Vijay Takbhate\nEmail: vijaytakbhate20@gmail.com Phone: 8767363681\nGitHub: https://github.com/vijaytakbhate2002\nLinkedIn: https://www.linkedin.com/in/vijay-takbhate-b9231a236/ Kaggle: https://www.kaggle.com/vijay20213\n\n'}, {'content': '\n\nExperience - Fox Solutions Pvt. Ltd.\nRole: Automation Engineer Duration: Jun 2024 - Oct 2024 Location: Maharashtra\nKey Contributions:\n-Completed 2 months of internship plus 4 months of full-time work.\n-Worked with PLC and SCADA systems, focusing on automating processes and optimizing operational efficiency.\n-Collaborated with cross-functional teams to implement automation solutions for industrial applications.\n\n'}, {'content': '\n\nExperience - Cei Design Consultancy Pvt. Ltd.\nRole: Python Developer Intern Duration: Aug 2024 - Sept 2024 Location: Remote, Maharashtra\nKey Contributions:\n-Specialized in data processing using Python and Excel.\n-Utilized OpenCV for image processing tasks.\n\n'}, {'content': '\n\nExperience - Ujucode\nRole: Subject Matter Expert Intern Duration: Aug 2023 - Oct 2023 Location: Remote, Maharashtra\nKey Contributions:\n-Contributed as a Python developer for a ChatBot project.\n-Handled backend development tasks and researched Python modules.\n\n'}, {'content': '\n\nProject - Twitter Post Sentiment Prediction\nDetails:\n-Engineered an ETL pipeline using PySpark and SQL.\n-Conducted sentiment analysis using NLP (TF-IDF) and optimized hyperparameters.\n-Monitored model performance through MLFlow on Databricks.\n-Leveraged Google Cloud Storage and MySQL for data management.\n-Deployed the model using Docker and hosted it on Render.\n\n'}, {'content': '\n\nProject - Text-Text Chat-Bot\nDetails:\n-Designed an advanced Chat-Bot using the NVIDIA API and prompt engineering.\n-Features include paraphrasing, grammar correction, AI detection, plagiarism checking, and content summarization.\n-Targeted at content creators, researchers, and businesses.\n-Technologies Used: HTML, CSS, Python Flask, Cloud Database, and Render.\n\n'}, {'content': '\n\nProject - Hand Gesture Recognition\nDetails:\n-Used Google-s MediaPipe framework for detecting hand landmarks and gestures.\n-Created and labeled a custom dataset of hand gestures for training.\n-Developed a Streamlit application to improve accessibility and flexibility.\n\n'}, {'content': '\n\nTechnical Skills\nLanguages: MySQL, Python, HTML, CSS\nTechnologies: Streamlit, Flask, VS Code, GitHub, MLflow, Docker, PySpark, Databricks, Google Cloud Platform\n\n'}, {'content': '\n\nCertification\nMLOps Bootcamp: Mastering AI Operations for Success (Jun 2024)\n-Learned about the MLOps lifecycle and modular programming.\n-Acquired skills in Git, Python, Flask, and MLflow.\n\n'}, {'content': '\n\nEducation Details:\nBachelor of Technology in Electronics and Telecommunication (May 2024) Institution: SVERI-s College of Engineering, Pandharpur, Maharashtra Score: 81.71\nDiploma in Electronics and Telecommunication (May 2021) Institution: SVERI-s College of Engineering, Pandharpur, Maharashtra Score: 91.73\n\n'}, {'content': "\n\nBlogging\n\n\nSupervised, Unsupervised, and Beyond: ML Techniques Simplified\nNovember 25, 2024\nThere are several techniques for ML training. Among these, I will cover the following:\n\nSupervised and Unsupervised Learning\nSemi-Supervised Learning\nOffline and Online Learning\nInstance-Based and Model-Based Learning\n\nSupervised and Unsupervised Learning\nSupervised learning is like spoon-feeding our ML model in its initial stages, allowing it to learn and improve over time. Here, I’m referring to the training process.\n\nIn supervised learning, there are input columns and output columns, also called target columns. For example, in spam detection—a classification problem—the input is the email, and the target is whether the email is spam or not. That’s it!\nThis process resembles a student-teacher scenario where the teacher is a human, and the student is the model. The dataset serves as the knowledge used to train the student (model)\nSee content credentials\n\nhumand and model\nIn Unsupervised learning, we are not aware of the data labels. Instead, we separate the input data by analyzing similarities and grouping them into different clusters.\nOnce the clusters are formed, we can assign custom labels to each one. This technique is widely used to identify product relationships in online shopping and to recommend new products based on a user’s purchase history.\n\nUnsupervised Learning Clusters\nHere three clusters with three different image categories are formed\n\nSemi-Supervised Learning\nSemi-Supervised Learning is a combination of supervised and unsupervised learning, where some data is labeled and some data is unlabeled. A good example of this is Google Photos, which automatically separates new photos into their respective groups based on whether they contain a particular person in each image.\n\nThere are several techniques under semi-supervised learning; here, we will focus on the following:\nSelf Learning\nConsistency Regularization\nGenerative Models\nGraph-Based Learning\n\nLet's discuss them one by one:\n\nSelf Learning\nSelf Learning trains a model with labeled data and generates pseudo-labels for the unlabeled data. \n\nSelf learning\nThen, the model is trained on the entire dataset, which includes both the generated pseudo-labels and the labeled data.\n\nConsistency Regularization\nThis technique uses data augmentation to generate similar data, and then the model is enforced to predict the same outcome for both the augmented and original data. It helps create a model that can find similarities in both labeled and unlabeled data, predicting the same output for unlabeled data as it would for labeled data.\n\nData augmentation includes techniques like image flipping, blurring, rotation, etc. After augmenting the data, the model is trained to predict the same class for both the augmented and original images.\n\nGenerative Models\nGenerative models create synthetic data points and learn the underlying structure of the training data. These models can generate new datasets using encoders. \n\nExamples include Generative Adversarial Networks (GANs) and Variational Autoencoders (VAEs).\n\nGraph-Based Learning\nThis technique creates nodes for both labeled and unlabeled data. Similar nodes are then connected to each other.\nFor example, when dealing with images, each image starts as a separate node. If similar images are found based on their features, they are grouped or connected by an edge.\nRefer below graph to understand graph-based learning\nGraph based learning\nHere label 1 is came from labeled image of group 1, simillarly for label 2 and label 3. Finally we came up with three labels that means labeled data has three unique labels.\nBatch (Offline) and Online Learning\nBatch Learning\nBatch learning, also known as offline learning, is a technique where the developer needs to stop the deployed ML model, retrain it on new data, and then redeploy it. \nThis technique is useful when frequent updates are not required. For example, product recommendation systems can often be retrained weekly or even over several days.\nBatch learning can be costly when training the model on the entire dataset every time. However, using incremental learning models can help avoid unnecessary retraining, making the process more efficient.\nHere are some examples on incremental learning models, as of now we are focusing on general concepts so don't go much deeper into these types.\nStochastic Gradient Descent (SGD)\nNaive Bayes (Online Version)\nIncremental Support Vector Machines (ISVM)\nOnline Random Forests\nIncremental Decision Trees\nYou can refer this article to learn more about incremental learning\nHere’s your text with grammatical corrections and improved clarity:\nOnline Learning\nOnline learning is used when an ML model needs to be updated continuously with new inputs, such as in stock price prediction. In this scenario, the model must remain aware of the most recent data.\n\nBatch and online learning\nOnly models capable of incremental learning are used in online learning. Each new instance is fed back into the model to update its internal weights based on the latest input.\n\nInstance-Based and Model-Based Learning\nInstance-Based Learning\nInstance-based learning involves comparing a new input with stored data. If similarity is found, the model returns the label of the corresponding input. This approach uses pattern matching techniques. \nHere model works as a search engin not exactly same but it will find simmilarity from stored data.\nSome common models used in instance-based learning include:\nK-Nearest Neighbors (K-NN) with labels\nLocally Weighted Regression, etc.\nModel-Based Learning\nIn model-based learning, we train the model using a training dataset. During the training process, the model creates its own complex patterns (mathematical equations) to make predictions for future inputs. \nA simple example is the equation y=mx+cy = mx + c, which represents a trained model. Some common models used in model-based learning include:\nLinear Regression\nLogistic Regression\nDecision Tree, etc.\n\nSummary\nMachine Learning techniques can be broadly categorized into supervised, unsupervised, and semi-supervised learning. \nSupervised learning uses labeled data, like spam detection, while unsupervised learning works with unlabeled data, grouping similar points into clusters. \nSemi-supervised learning combines both, employing techniques like self-learning and consistency regularization to leverage partially labeled datasets.\nOther approaches include batch learning, where models are retrained periodically, and online learning, which updates models continuously with new data, suitable for dynamic tasks like stock price prediction. \nInstance-based learning relies on pattern matching (e.g., K-Nearest Neighbors), while model-based learning creates mathematical models (e.g., Linear Regression) to make predictions.\nThis is all about Machine Learning techniques. If you learned something, let me know in the comments. Your suggestions will help me improve my blogs.\nThanks for reading!\n\n"}, {'content': '\nBlogging\n\nMastering the End-to-End Machine Learning Lifecycle: From Data to Deployment\n\nNovember 24, 2024\n This article takes you through the complete lifecycle of a Machine Learning project. From ETL to deployment, I’ll share every detail of how I brought this project to life.\nData Engineering\nData Engineering serves as the starting point in the Machine Learning project lifecycle, bringing all distributed data together in one place.\nFor this project, I utilized an ETL pipeline—a core concept in Data Engineering. It enabled me to extract raw data, transform it into a meaningful format, and load it into a suitable location for further processing.\nLet\'s deep dive into ETL pipeline\nETL stands for Extract, Transform, and Load. These pipelines can be executed periodically (e.g., daily or hourly) to fetch real-time data, enhancing the predictive power of our Machine Learning model for real-world applications.\nThis process can be automated using tools like Apache Airflow, Kubeflow Pipelines, AWS Step Functions, and more, streamlining the workflow for consistent and efficient data updates.\n\nETL Pipeline:\nETL Pipeline\nExtract Operation\nThe extract operation is responsible for fetching data from various sources such as websites, databases, and APIs, as illustrated in the flowchart above.\n\nTransform Operation\nThe transform operation focuses on data cleaning, feature selection, and manipulation. In my project, I used this step to extract only the required features, ensuring the data is ready for the next phase.\n\nLoad Operation\nThe load operation transfers the transformed data to its destination, where we can choose the appropriate storage format. Typically, the processed data is stored in a database for further analysis and model training.\n\nFor my project, I used PySpark to build the ETL pipeline, as it enables efficient processing of large datasets.\nWhy not Pandas?\nWhile Pandas is excellent for small to medium-sized datasets, it stores DataFrames in RAM, which can lead to out-of-memory exceptions when handling large datasets.\nIn contrast, PySpark creates a session and processes data in chunks, storing it in ROM, making it ideal for handling large-scale data.\nFor the actual implementation of the pipeline using PySpark, please refer to the accompanying jupyter notebook.\nEDA (Data Analysis)\nData understanding is a critical stage before building any Machine Learning model. It allows us to analyze the data, plan the data processing steps, and gain insights into its structure and quality.\nIn this project, I examined the balance of data in my training and testing datasets and found it to be well-distributed across all four categories.\n\nBalanceness Checking on train and test data\nThroughout the entire process, I focused on two columns:\nTweets\nSentiments\nThe Tweets column contains the raw Twitter text data, while the Sentiments column serves as the target variable for prediction.\nNatural Language Processing\nI applied several key NLP techniques to preprocess the data and prepare it for Machine Learning model building. Below are the main steps I executed:\n\nText Processing\nRegex: I applied regular expressions to clean the text by removing URLs, hashtags, HTML tags, and keeping only alphanumeric characters. This helped eliminate unnecessary noise from the data.\nNLTK: Using the Natural Language Toolkit (NLTK), I performed word tokenization, stemming and lemmatization.\nWord tokenization is just splitting sentence into words, so we can processing each word from sentence individually.\nStemming helps us to truncate prefix or suffix of text to reduce count on unique words from corpus (paragraph).\nLemmatization is the proecss of converting any word into it\'s base word, for eg. Played will convert to play.\n\nWordCloud \nWord cloud concept help you to understand importance of words from given data. I splitted my data into four sections.\ndata for negative sentiments\ndata for positive sentiments\ndata for neutral sentiments\ndata for irrelevant sentiments\n\nHere is the representation of most frequent words for each category.\nCategorywise Word Cloud Presentation\nHere you can clearly see that there is no much difference in negative and positive sentiments data.\nthis is representation of bad data, here we can filterout our data for further processing, it might reduce data but you can do data augmentation techniques here to increase your data.\naugmenting of data in NLP with TF-IDF will not bad idea because TF-IDF and any other porcessing technique that I used is not able to detect sentence grammer or it doesn,t require sophisticated text. \nYou can think our input text will work as bag of word for model. you can understand by refering below vectorization technique.\nVectorization with TF-IDF \nI used Term Frequency-Inverse Document Frequency (TF-IDF) for text vectorization. This method transforms text into a numerical format, considering the importance of each word across documents, which prevents frequent words from dominating the model.\n\nTF-IDF\nTF-IDF stands for Term Frequency-Inverse Document Frequency. \nTerm Frequency (TF): This measures how frequently a term (t) appears in a specific document (d). It\'s calculated by dividing the number of occurrences of the term in the document by the total number of terms in that document.\nInverse Document Frequency (IDF): This measures how important a term is across the entire collection of documents. It\'s calculated by taking the logarithm of the ratio of the total number of documents (N) to the number of documents containing the term (df(t)). A higher IDF value indicates a rarer term, making it more significant.\nHere is the resulant data we got from TF-IDF\nAfter all preprocessing of text it\'s time to save our data for mlflow experiments. To see actual implementation of end to end process of NLP you can refer jupyter notebook.\nmlflow experiments\nMLflow experiments allow you to conduct multiple experiments with your trained model, helping you track and compare results over time. For running MLflow experiments, I prefer using Databricks as it offers an integrated experiment section, making the process much more streamlined and efficient.\nIn Databricks, you can easily connect your experiment by passing the experiment ID into the MLflow code. This integration simplifies the entire workflow, enabling better experiment tracking and easier comparison of model performance.\nFor my experiments, I worked with several models, including Logistic Regression, Multinomial Naive Bayes, and Decision Tree Classifier. By applying different combinations of parameters, I was able to experiment with and compare the performance of each model. Here\'s a preview of the Logistic Regression model’s F1 score, which highlights the model\'s ability to balance precision and recall:\nLogistic Regression F1 Score: 0.58\nThis approach allowed me to track the effectiveness of each model and make adjustments as needed for improving performance.\n\nF1 score with different parameters (Logistic Regression)\nI recommend running the notebook below in your own account to see the results. You\'ll definitely start appreciating the power of MLflow.\nFor more information, please refer to the accompanying Databricks notebook.\n\nHyperparameter tuning\nHyperparameter tuning is crucial for identifying the best parameter combination for your model. This technique involves an iterative process where, for every parameter combination, the model is trained and tested on a dataset. It is often described as a "trial and error" method.\nHowever, this approach can be computationally expensive, especially when working with complex or heavy machine learning models. For large-scale problems, hyperparameter tuning can be made more efficient through sampling or batch methods. In these methods, you don\'t use the entire dataset for training the model; instead, you choose random or stratified data points from the dataset to train the model. Although this may slightly reduce accuracy, it is more feasible when working with large datasets.\nFor my project, I used Grid Search CV to find the best hyperparameter combination for the model. Below are some common techniques for hyperparameter tuning, especially for large datasets:\nGrid Search This technique exhaustively searches through a specified set of hyperparameter values, trying all possible combinations. While effective, it can be computationally expensive for larger datasets due to the exhaustive nature of the search.\nRandom Search Randomly samples from the hyperparameter space, offering a faster alternative to grid search. This method explores a wider range of hyperparameters with fewer evaluations, making it more efficient for larger datasets.\nBayesian Optimization This method uses probabilistic models to predict the performance of different hyperparameters. It selects the next set of hyperparameters to evaluate based on previous results, making it more efficient and suitable for large datasets.\nGenetic Algorithms Inspired by natural selection, these algorithms iteratively evolve a population of hyperparameter sets to improve model performance. This method works well with complex search spaces.\nHyperband Hyperband combines random search with early stopping to dynamically allocate resources across multiple configurations, identifying promising hyperparameters quickly without excessive computational costs.\nBayesian Optimization with Gaussian Processes This technique models the hyperparameter search space using Gaussian processes, focusing on regions that are likely to yield better results, which is particularly useful for large datasets where computational resources are limited.\n\nTo optimize hyperparameter tuning for larger datasets, these techniques can be combined with parallel computing and distributed processing frameworks such as Dask, Spark, or multi-GPU setups. This enables more efficient hyperparameter search and reduces the overall computational overhead.\nI choosed Logistc Regression model and trained my Model with best parameters.\nSource Distribution for Model Packaging\nPackaging your machine learning model is a best practice, especially if you don’t plan to update it frequently. Imagine thousands of lines of code that can now be utilized with just a single line—this is the power of model packaging.\n\n1. Project Folder Setup\nTo ensure better organization, I created a main folder called sentiment_prediction and moved all machine learning pipeline files and dependencies into this folder. This helped in maintaining a clean structure and simplified the management of the entire project.\nBefore moving forward I recommend you to visit this pdf it will practically show you step by step process for building python package.\nPDF : step by step guid for python package building\n\n2. Manifest.in\nThe Manifest.in file plays a crucial role in controlling which files and folders should be included or excluded during the packaging process. It helps to specify the structure of the package for distribution.\nKey commands used in the Manifest.in file include:\ninclude <file/folder>: Include specific files or folders.\nexclude <file/folder>: Exclude specific files or folders.\nrecursive-include <path>: Include all files from a directory recursively.\nrecursive-exclude <path>: Exclude all files from a directory recursively.\n3. Setup.py\nThe setup.py file contains the project\'s metadata and is essential for creating the package. It defines key information about the project, such as:\nProject name, version, description, and author details.\nDependencies required for the package (install_requires), making it easy to install all necessary libraries.\n\n4. Building the Package\nTo build the package, I used the following command:\n\npython setup.py sdist bdist_wheel\nThis command generates two folders:\n\nbuild/: Contains the entire project package as defined in the Manifest.in.\ndist/: Contains the distributable files: .whl (wheel file) .gz (compressed source archive)\n\n5. Global Access via GitHub\nNow you can access your package gloablly, by refering your repository. I provided my package below go and check out.\n\nRepository: GitHub Repo Link\nYou can install the package directly from GitHub using the following command:\n\npip install git+https://github.com/vijaytakbhate2002/sentiment_prediction_python_package.git \nTo ensure it worked globally, I tested it again:\n\nfrom sentiment_prediction import predict\nprint(predict.predictor("Great progress shared today!")) \noutput:[\'Negative\']\nFlask Application \nBuilding application will help us to give our NLP model experience to people, so I built one flask application.\nWeb Application UI\n\nHere is demonstration of project: project demo\nI left a blank section for user suggestion and feedback, these feedbacks are getting stored in database for future model analysis or any business work.\nDatabase configuration\nFor storing collected user data we need to configure a database. It will help us to improve model as per user need.\nI used Google Cloud MySQL instance for integrating my application with database, GCP is paid but you can use free credit of GCP for first 3 months, for doing almost all Cloud Work.\nYou need to create your GCP account, then create one instance under SQL and by configuring your local system with instance you are good to go.\nDocker containerization\nIf you are not awared about docker and it\'s basic concepts you can refer my previous article which explains you all about docker.\nThis guide will help you build a solid foundation in Docker, enabling you to confidently use it for your projects.\nDocker guide: Comprehensive Docker guide for deploying Flas app\nDeploy\nDeployment of web app will help us to engange people and provide them real actual experience of our services.\nAfter deployment you need to collect user data and store it for future analysis, this data contain user feedback and suggestions.\nAfter deployment it\'s not end of the process we need to collect user feedback and again follow same steps fine tune Model as per user need.\n\nSummary of Blog\nThis blog covers the lifecycle of a Machine Learning project, from ETL to deployment. It details building an ETL pipeline using PySpark for efficient data handling, EDA, and NLP preprocessing techniques like tokenization, TF-IDF, and WordCloud visualization. \nIt highlights ML experiments with MLflow on Databricks and hyperparameter tuning using Grid Search.\nThe model was packaged into a Python package and deployed as a Flask application with a database backend (Google Cloud MySQL) and Dockerized for scalability. \nThe app collects user feedback for continuous improvement. It emphasizes end-to-end integration, including cloud and containerization, to deliver a robust ML solution.\nHappy Learning!\n\n'}, {'content': '\nLanguages I Speak\nEnglish, Marathi, Hindi\n\n'}, {'content': '\n\nSoft skills:\nCritical Thinking, Intellectual Rigor, Problem Solving, Understanding Business Needs'}]
2024-11-25 21:39:49 - root - INFO - Document store already contains 14 documents. Skipping write.
2024-11-25 21:39:49 - haystack.modeling.utils - INFO - Using devices: CPU - Number of GPUs: 0
2024-11-25 21:39:49 - haystack.modeling.utils - INFO - Using devices: CPU - Number of GPUs: 0
2024-11-25 21:39:49 - haystack.modeling.model.language_model - INFO -  * LOADING MODEL: 'deepset/roberta-base-squad2' (Roberta)
2024-11-25 21:39:50 - haystack.modeling.model.language_model - INFO - Auto-detected model language: english
2024-11-25 21:39:50 - haystack.modeling.model.language_model - INFO - Loaded 'deepset/roberta-base-squad2' (Roberta model) from model hub.
2024-11-25 21:39:51 - haystack.modeling.utils - INFO - Using devices: CPU - Number of GPUs: 0
2024-11-25 21:39:51 - haystack.modeling.data_handler.processor - ERROR - There were 1 errors during preprocessing at positions: {0}
2024-11-25 21:42:03 - root - INFO - Loading existing FAISS document store...
2024-11-25 21:42:03 - haystack.modeling.utils - INFO - Using devices: CPU - Number of GPUs: 0
2024-11-25 21:42:04 - haystack.modeling.model.language_model - WARNING - Using a model of type 'mpnet' which might be incompatible with DPR encoders. Only Bert-based encoders are supported. They need input_ids, token_type_ids, attention_mask as input tensors.
2024-11-25 21:42:07 - haystack.modeling.model.language_model - INFO - Auto-detected model language: english
2024-11-25 21:42:08 - haystack.modeling.model.language_model - WARNING - Using a model of type 'mpnet' which might be incompatible with DPR encoders. Only Bert-based encoders are supported. They need input_ids, token_type_ids, attention_mask as input tensors.
2024-11-25 21:42:09 - haystack.modeling.model.language_model - INFO - Auto-detected model language: english
2024-11-25 21:42:09 - root - INFO - Reading text document
2024-11-25 21:42:09 - root - INFO - loaded document = [{'content': '\nPersonal Information\n\nName: Vijay Takbhate\nEmail: vijaytakbhate20@gmail.com Phone: 8767363681\nGitHub: https://github.com/vijaytakbhate2002\nLinkedIn: https://www.linkedin.com/in/vijay-takbhate-b9231a236/ Kaggle: https://www.kaggle.com/vijay20213\n\n'}, {'content': '\n\nExperience - Fox Solutions Pvt. Ltd.\nRole: Automation Engineer Duration: Jun 2024 - Oct 2024 Location: Maharashtra\nKey Contributions:\n-Completed 2 months of internship plus 4 months of full-time work.\n-Worked with PLC and SCADA systems, focusing on automating processes and optimizing operational efficiency.\n-Collaborated with cross-functional teams to implement automation solutions for industrial applications.\n\n'}, {'content': '\n\nExperience - Cei Design Consultancy Pvt. Ltd.\nRole: Python Developer Intern Duration: Aug 2024 - Sept 2024 Location: Remote, Maharashtra\nKey Contributions:\n-Specialized in data processing using Python and Excel.\n-Utilized OpenCV for image processing tasks.\n\n'}, {'content': '\n\nExperience - Ujucode\nRole: Subject Matter Expert Intern Duration: Aug 2023 - Oct 2023 Location: Remote, Maharashtra\nKey Contributions:\n-Contributed as a Python developer for a ChatBot project.\n-Handled backend development tasks and researched Python modules.\n\n'}, {'content': '\n\nProject - Twitter Post Sentiment Prediction\nDetails:\n-Engineered an ETL pipeline using PySpark and SQL.\n-Conducted sentiment analysis using NLP (TF-IDF) and optimized hyperparameters.\n-Monitored model performance through MLFlow on Databricks.\n-Leveraged Google Cloud Storage and MySQL for data management.\n-Deployed the model using Docker and hosted it on Render.\n\n'}, {'content': '\n\nProject - Text-Text Chat-Bot\nDetails:\n-Designed an advanced Chat-Bot using the NVIDIA API and prompt engineering.\n-Features include paraphrasing, grammar correction, AI detection, plagiarism checking, and content summarization.\n-Targeted at content creators, researchers, and businesses.\n-Technologies Used: HTML, CSS, Python Flask, Cloud Database, and Render.\n\n'}, {'content': '\n\nProject - Hand Gesture Recognition\nDetails:\n-Used Google-s MediaPipe framework for detecting hand landmarks and gestures.\n-Created and labeled a custom dataset of hand gestures for training.\n-Developed a Streamlit application to improve accessibility and flexibility.\n\n'}, {'content': '\n\nTechnical Skills\nLanguages: MySQL, Python, HTML, CSS\nTechnologies: Streamlit, Flask, VS Code, GitHub, MLflow, Docker, PySpark, Databricks, Google Cloud Platform\n\n'}, {'content': '\n\nCertification\nMLOps Bootcamp: Mastering AI Operations for Success (Jun 2024)\n-Learned about the MLOps lifecycle and modular programming.\n-Acquired skills in Git, Python, Flask, and MLflow.\n\n'}, {'content': '\n\nEducation Details:\nBachelor of Technology in Electronics and Telecommunication (May 2024) Institution: SVERI-s College of Engineering, Pandharpur, Maharashtra Score: 81.71\nDiploma in Electronics and Telecommunication (May 2021) Institution: SVERI-s College of Engineering, Pandharpur, Maharashtra Score: 91.73\n\n'}, {'content': "\n\nBlogging\n\n\nSupervised, Unsupervised, and Beyond: ML Techniques Simplified\nNovember 25, 2024\nThere are several techniques for ML training. Among these, I will cover the following:\n\nSupervised and Unsupervised Learning\nSemi-Supervised Learning\nOffline and Online Learning\nInstance-Based and Model-Based Learning\n\nSupervised and Unsupervised Learning\nSupervised learning is like spoon-feeding our ML model in its initial stages, allowing it to learn and improve over time. Here, I’m referring to the training process.\n\nIn supervised learning, there are input columns and output columns, also called target columns. For example, in spam detection—a classification problem—the input is the email, and the target is whether the email is spam or not. That’s it!\nThis process resembles a student-teacher scenario where the teacher is a human, and the student is the model. The dataset serves as the knowledge used to train the student (model)\nSee content credentials\n\nhumand and model\nIn Unsupervised learning, we are not aware of the data labels. Instead, we separate the input data by analyzing similarities and grouping them into different clusters.\nOnce the clusters are formed, we can assign custom labels to each one. This technique is widely used to identify product relationships in online shopping and to recommend new products based on a user’s purchase history.\n\nUnsupervised Learning Clusters\nHere three clusters with three different image categories are formed\n\nSemi-Supervised Learning\nSemi-Supervised Learning is a combination of supervised and unsupervised learning, where some data is labeled and some data is unlabeled. A good example of this is Google Photos, which automatically separates new photos into their respective groups based on whether they contain a particular person in each image.\n\nThere are several techniques under semi-supervised learning; here, we will focus on the following:\nSelf Learning\nConsistency Regularization\nGenerative Models\nGraph-Based Learning\n\nLet's discuss them one by one:\n\nSelf Learning\nSelf Learning trains a model with labeled data and generates pseudo-labels for the unlabeled data. \n\nSelf learning\nThen, the model is trained on the entire dataset, which includes both the generated pseudo-labels and the labeled data.\n\nConsistency Regularization\nThis technique uses data augmentation to generate similar data, and then the model is enforced to predict the same outcome for both the augmented and original data. It helps create a model that can find similarities in both labeled and unlabeled data, predicting the same output for unlabeled data as it would for labeled data.\n\nData augmentation includes techniques like image flipping, blurring, rotation, etc. After augmenting the data, the model is trained to predict the same class for both the augmented and original images.\n\nGenerative Models\nGenerative models create synthetic data points and learn the underlying structure of the training data. These models can generate new datasets using encoders. \n\nExamples include Generative Adversarial Networks (GANs) and Variational Autoencoders (VAEs).\n\nGraph-Based Learning\nThis technique creates nodes for both labeled and unlabeled data. Similar nodes are then connected to each other.\nFor example, when dealing with images, each image starts as a separate node. If similar images are found based on their features, they are grouped or connected by an edge.\nRefer below graph to understand graph-based learning\nGraph based learning\nHere label 1 is came from labeled image of group 1, simillarly for label 2 and label 3. Finally we came up with three labels that means labeled data has three unique labels.\nBatch (Offline) and Online Learning\nBatch Learning\nBatch learning, also known as offline learning, is a technique where the developer needs to stop the deployed ML model, retrain it on new data, and then redeploy it. \nThis technique is useful when frequent updates are not required. For example, product recommendation systems can often be retrained weekly or even over several days.\nBatch learning can be costly when training the model on the entire dataset every time. However, using incremental learning models can help avoid unnecessary retraining, making the process more efficient.\nHere are some examples on incremental learning models, as of now we are focusing on general concepts so don't go much deeper into these types.\nStochastic Gradient Descent (SGD)\nNaive Bayes (Online Version)\nIncremental Support Vector Machines (ISVM)\nOnline Random Forests\nIncremental Decision Trees\nYou can refer this article to learn more about incremental learning\nHere’s your text with grammatical corrections and improved clarity:\nOnline Learning\nOnline learning is used when an ML model needs to be updated continuously with new inputs, such as in stock price prediction. In this scenario, the model must remain aware of the most recent data.\n\nBatch and online learning\nOnly models capable of incremental learning are used in online learning. Each new instance is fed back into the model to update its internal weights based on the latest input.\n\nInstance-Based and Model-Based Learning\nInstance-Based Learning\nInstance-based learning involves comparing a new input with stored data. If similarity is found, the model returns the label of the corresponding input. This approach uses pattern matching techniques. \nHere model works as a search engin not exactly same but it will find simmilarity from stored data.\nSome common models used in instance-based learning include:\nK-Nearest Neighbors (K-NN) with labels\nLocally Weighted Regression, etc.\nModel-Based Learning\nIn model-based learning, we train the model using a training dataset. During the training process, the model creates its own complex patterns (mathematical equations) to make predictions for future inputs. \nA simple example is the equation y=mx+cy = mx + c, which represents a trained model. Some common models used in model-based learning include:\nLinear Regression\nLogistic Regression\nDecision Tree, etc.\n\nSummary\nMachine Learning techniques can be broadly categorized into supervised, unsupervised, and semi-supervised learning. \nSupervised learning uses labeled data, like spam detection, while unsupervised learning works with unlabeled data, grouping similar points into clusters. \nSemi-supervised learning combines both, employing techniques like self-learning and consistency regularization to leverage partially labeled datasets.\nOther approaches include batch learning, where models are retrained periodically, and online learning, which updates models continuously with new data, suitable for dynamic tasks like stock price prediction. \nInstance-based learning relies on pattern matching (e.g., K-Nearest Neighbors), while model-based learning creates mathematical models (e.g., Linear Regression) to make predictions.\nThis is all about Machine Learning techniques. If you learned something, let me know in the comments. Your suggestions will help me improve my blogs.\nThanks for reading!\n\n"}, {'content': '\nBlogging\n\nMastering the End-to-End Machine Learning Lifecycle: From Data to Deployment\n\nNovember 24, 2024\n This article takes you through the complete lifecycle of a Machine Learning project. From ETL to deployment, I’ll share every detail of how I brought this project to life.\nData Engineering\nData Engineering serves as the starting point in the Machine Learning project lifecycle, bringing all distributed data together in one place.\nFor this project, I utilized an ETL pipeline—a core concept in Data Engineering. It enabled me to extract raw data, transform it into a meaningful format, and load it into a suitable location for further processing.\nLet\'s deep dive into ETL pipeline\nETL stands for Extract, Transform, and Load. These pipelines can be executed periodically (e.g., daily or hourly) to fetch real-time data, enhancing the predictive power of our Machine Learning model for real-world applications.\nThis process can be automated using tools like Apache Airflow, Kubeflow Pipelines, AWS Step Functions, and more, streamlining the workflow for consistent and efficient data updates.\n\nETL Pipeline:\nETL Pipeline\nExtract Operation\nThe extract operation is responsible for fetching data from various sources such as websites, databases, and APIs, as illustrated in the flowchart above.\n\nTransform Operation\nThe transform operation focuses on data cleaning, feature selection, and manipulation. In my project, I used this step to extract only the required features, ensuring the data is ready for the next phase.\n\nLoad Operation\nThe load operation transfers the transformed data to its destination, where we can choose the appropriate storage format. Typically, the processed data is stored in a database for further analysis and model training.\n\nFor my project, I used PySpark to build the ETL pipeline, as it enables efficient processing of large datasets.\nWhy not Pandas?\nWhile Pandas is excellent for small to medium-sized datasets, it stores DataFrames in RAM, which can lead to out-of-memory exceptions when handling large datasets.\nIn contrast, PySpark creates a session and processes data in chunks, storing it in ROM, making it ideal for handling large-scale data.\nFor the actual implementation of the pipeline using PySpark, please refer to the accompanying jupyter notebook.\nEDA (Data Analysis)\nData understanding is a critical stage before building any Machine Learning model. It allows us to analyze the data, plan the data processing steps, and gain insights into its structure and quality.\nIn this project, I examined the balance of data in my training and testing datasets and found it to be well-distributed across all four categories.\n\nBalanceness Checking on train and test data\nThroughout the entire process, I focused on two columns:\nTweets\nSentiments\nThe Tweets column contains the raw Twitter text data, while the Sentiments column serves as the target variable for prediction.\nNatural Language Processing\nI applied several key NLP techniques to preprocess the data and prepare it for Machine Learning model building. Below are the main steps I executed:\n\nText Processing\nRegex: I applied regular expressions to clean the text by removing URLs, hashtags, HTML tags, and keeping only alphanumeric characters. This helped eliminate unnecessary noise from the data.\nNLTK: Using the Natural Language Toolkit (NLTK), I performed word tokenization, stemming and lemmatization.\nWord tokenization is just splitting sentence into words, so we can processing each word from sentence individually.\nStemming helps us to truncate prefix or suffix of text to reduce count on unique words from corpus (paragraph).\nLemmatization is the proecss of converting any word into it\'s base word, for eg. Played will convert to play.\n\nWordCloud \nWord cloud concept help you to understand importance of words from given data. I splitted my data into four sections.\ndata for negative sentiments\ndata for positive sentiments\ndata for neutral sentiments\ndata for irrelevant sentiments\n\nHere is the representation of most frequent words for each category.\nCategorywise Word Cloud Presentation\nHere you can clearly see that there is no much difference in negative and positive sentiments data.\nthis is representation of bad data, here we can filterout our data for further processing, it might reduce data but you can do data augmentation techniques here to increase your data.\naugmenting of data in NLP with TF-IDF will not bad idea because TF-IDF and any other porcessing technique that I used is not able to detect sentence grammer or it doesn,t require sophisticated text. \nYou can think our input text will work as bag of word for model. you can understand by refering below vectorization technique.\nVectorization with TF-IDF \nI used Term Frequency-Inverse Document Frequency (TF-IDF) for text vectorization. This method transforms text into a numerical format, considering the importance of each word across documents, which prevents frequent words from dominating the model.\n\nTF-IDF\nTF-IDF stands for Term Frequency-Inverse Document Frequency. \nTerm Frequency (TF): This measures how frequently a term (t) appears in a specific document (d). It\'s calculated by dividing the number of occurrences of the term in the document by the total number of terms in that document.\nInverse Document Frequency (IDF): This measures how important a term is across the entire collection of documents. It\'s calculated by taking the logarithm of the ratio of the total number of documents (N) to the number of documents containing the term (df(t)). A higher IDF value indicates a rarer term, making it more significant.\nHere is the resulant data we got from TF-IDF\nAfter all preprocessing of text it\'s time to save our data for mlflow experiments. To see actual implementation of end to end process of NLP you can refer jupyter notebook.\nmlflow experiments\nMLflow experiments allow you to conduct multiple experiments with your trained model, helping you track and compare results over time. For running MLflow experiments, I prefer using Databricks as it offers an integrated experiment section, making the process much more streamlined and efficient.\nIn Databricks, you can easily connect your experiment by passing the experiment ID into the MLflow code. This integration simplifies the entire workflow, enabling better experiment tracking and easier comparison of model performance.\nFor my experiments, I worked with several models, including Logistic Regression, Multinomial Naive Bayes, and Decision Tree Classifier. By applying different combinations of parameters, I was able to experiment with and compare the performance of each model. Here\'s a preview of the Logistic Regression model’s F1 score, which highlights the model\'s ability to balance precision and recall:\nLogistic Regression F1 Score: 0.58\nThis approach allowed me to track the effectiveness of each model and make adjustments as needed for improving performance.\n\nF1 score with different parameters (Logistic Regression)\nI recommend running the notebook below in your own account to see the results. You\'ll definitely start appreciating the power of MLflow.\nFor more information, please refer to the accompanying Databricks notebook.\n\nHyperparameter tuning\nHyperparameter tuning is crucial for identifying the best parameter combination for your model. This technique involves an iterative process where, for every parameter combination, the model is trained and tested on a dataset. It is often described as a "trial and error" method.\nHowever, this approach can be computationally expensive, especially when working with complex or heavy machine learning models. For large-scale problems, hyperparameter tuning can be made more efficient through sampling or batch methods. In these methods, you don\'t use the entire dataset for training the model; instead, you choose random or stratified data points from the dataset to train the model. Although this may slightly reduce accuracy, it is more feasible when working with large datasets.\nFor my project, I used Grid Search CV to find the best hyperparameter combination for the model. Below are some common techniques for hyperparameter tuning, especially for large datasets:\nGrid Search This technique exhaustively searches through a specified set of hyperparameter values, trying all possible combinations. While effective, it can be computationally expensive for larger datasets due to the exhaustive nature of the search.\nRandom Search Randomly samples from the hyperparameter space, offering a faster alternative to grid search. This method explores a wider range of hyperparameters with fewer evaluations, making it more efficient for larger datasets.\nBayesian Optimization This method uses probabilistic models to predict the performance of different hyperparameters. It selects the next set of hyperparameters to evaluate based on previous results, making it more efficient and suitable for large datasets.\nGenetic Algorithms Inspired by natural selection, these algorithms iteratively evolve a population of hyperparameter sets to improve model performance. This method works well with complex search spaces.\nHyperband Hyperband combines random search with early stopping to dynamically allocate resources across multiple configurations, identifying promising hyperparameters quickly without excessive computational costs.\nBayesian Optimization with Gaussian Processes This technique models the hyperparameter search space using Gaussian processes, focusing on regions that are likely to yield better results, which is particularly useful for large datasets where computational resources are limited.\n\nTo optimize hyperparameter tuning for larger datasets, these techniques can be combined with parallel computing and distributed processing frameworks such as Dask, Spark, or multi-GPU setups. This enables more efficient hyperparameter search and reduces the overall computational overhead.\nI choosed Logistc Regression model and trained my Model with best parameters.\nSource Distribution for Model Packaging\nPackaging your machine learning model is a best practice, especially if you don’t plan to update it frequently. Imagine thousands of lines of code that can now be utilized with just a single line—this is the power of model packaging.\n\n1. Project Folder Setup\nTo ensure better organization, I created a main folder called sentiment_prediction and moved all machine learning pipeline files and dependencies into this folder. This helped in maintaining a clean structure and simplified the management of the entire project.\nBefore moving forward I recommend you to visit this pdf it will practically show you step by step process for building python package.\nPDF : step by step guid for python package building\n\n2. Manifest.in\nThe Manifest.in file plays a crucial role in controlling which files and folders should be included or excluded during the packaging process. It helps to specify the structure of the package for distribution.\nKey commands used in the Manifest.in file include:\ninclude <file/folder>: Include specific files or folders.\nexclude <file/folder>: Exclude specific files or folders.\nrecursive-include <path>: Include all files from a directory recursively.\nrecursive-exclude <path>: Exclude all files from a directory recursively.\n3. Setup.py\nThe setup.py file contains the project\'s metadata and is essential for creating the package. It defines key information about the project, such as:\nProject name, version, description, and author details.\nDependencies required for the package (install_requires), making it easy to install all necessary libraries.\n\n4. Building the Package\nTo build the package, I used the following command:\n\npython setup.py sdist bdist_wheel\nThis command generates two folders:\n\nbuild/: Contains the entire project package as defined in the Manifest.in.\ndist/: Contains the distributable files: .whl (wheel file) .gz (compressed source archive)\n\n5. Global Access via GitHub\nNow you can access your package gloablly, by refering your repository. I provided my package below go and check out.\n\nRepository: GitHub Repo Link\nYou can install the package directly from GitHub using the following command:\n\npip install git+https://github.com/vijaytakbhate2002/sentiment_prediction_python_package.git \nTo ensure it worked globally, I tested it again:\n\nfrom sentiment_prediction import predict\nprint(predict.predictor("Great progress shared today!")) \noutput:[\'Negative\']\nFlask Application \nBuilding application will help us to give our NLP model experience to people, so I built one flask application.\nWeb Application UI\n\nHere is demonstration of project: project demo\nI left a blank section for user suggestion and feedback, these feedbacks are getting stored in database for future model analysis or any business work.\nDatabase configuration\nFor storing collected user data we need to configure a database. It will help us to improve model as per user need.\nI used Google Cloud MySQL instance for integrating my application with database, GCP is paid but you can use free credit of GCP for first 3 months, for doing almost all Cloud Work.\nYou need to create your GCP account, then create one instance under SQL and by configuring your local system with instance you are good to go.\nDocker containerization\nIf you are not awared about docker and it\'s basic concepts you can refer my previous article which explains you all about docker.\nThis guide will help you build a solid foundation in Docker, enabling you to confidently use it for your projects.\nDocker guide: Comprehensive Docker guide for deploying Flas app\nDeploy\nDeployment of web app will help us to engange people and provide them real actual experience of our services.\nAfter deployment you need to collect user data and store it for future analysis, this data contain user feedback and suggestions.\nAfter deployment it\'s not end of the process we need to collect user feedback and again follow same steps fine tune Model as per user need.\n\nSummary of Blog\nThis blog covers the lifecycle of a Machine Learning project, from ETL to deployment. It details building an ETL pipeline using PySpark for efficient data handling, EDA, and NLP preprocessing techniques like tokenization, TF-IDF, and WordCloud visualization. \nIt highlights ML experiments with MLflow on Databricks and hyperparameter tuning using Grid Search.\nThe model was packaged into a Python package and deployed as a Flask application with a database backend (Google Cloud MySQL) and Dockerized for scalability. \nThe app collects user feedback for continuous improvement. It emphasizes end-to-end integration, including cloud and containerization, to deliver a robust ML solution.\nHappy Learning!\n\n'}, {'content': '\nLanguages I Speak\nEnglish, Marathi, Hindi\n\n'}, {'content': '\n\nSoft skills:\nCritical Thinking, Intellectual Rigor, Problem Solving, Understanding Business Needs'}]
2024-11-25 21:42:09 - root - INFO - Document store already contains 14 documents. Skipping write.
2024-11-25 21:42:09 - haystack.modeling.utils - INFO - Using devices: CPU - Number of GPUs: 0
2024-11-25 21:42:09 - haystack.modeling.utils - INFO - Using devices: CPU - Number of GPUs: 0
2024-11-25 21:42:09 - haystack.modeling.model.language_model - INFO -  * LOADING MODEL: 'deepset/roberta-base-squad2' (Roberta)
2024-11-25 21:42:11 - haystack.modeling.model.language_model - INFO - Auto-detected model language: english
2024-11-25 21:42:11 - haystack.modeling.model.language_model - INFO - Loaded 'deepset/roberta-base-squad2' (Roberta model) from model hub.
2024-11-25 21:42:12 - haystack.modeling.utils - INFO - Using devices: CPU - Number of GPUs: 0
2024-11-25 21:42:12 - haystack.modeling.data_handler.processor - ERROR - There were 1 errors during preprocessing at positions: {0}
2024-11-25 21:43:18 - root - INFO - Creating a new FAISS document store...
2024-11-25 21:43:19 - haystack.modeling.utils - INFO - Using devices: CPU - Number of GPUs: 0
2024-11-25 21:43:20 - haystack.modeling.model.language_model - WARNING - Using a model of type 'mpnet' which might be incompatible with DPR encoders. Only Bert-based encoders are supported. They need input_ids, token_type_ids, attention_mask as input tensors.
2024-11-25 21:43:22 - haystack.modeling.model.language_model - INFO - Auto-detected model language: english
2024-11-25 21:43:23 - haystack.modeling.model.language_model - WARNING - Using a model of type 'mpnet' which might be incompatible with DPR encoders. Only Bert-based encoders are supported. They need input_ids, token_type_ids, attention_mask as input tensors.
2024-11-25 21:43:25 - haystack.modeling.model.language_model - INFO - Auto-detected model language: english
2024-11-25 21:43:25 - root - INFO - Reading text document
2024-11-25 21:43:25 - root - INFO - loaded document = [{'content': '\nPersonal Information\n\nName: Vijay Takbhate\nEmail: vijaytakbhate20@gmail.com Phone: 8767363681\nGitHub: https://github.com/vijaytakbhate2002\nLinkedIn: https://www.linkedin.com/in/vijay-takbhate-b9231a236/ Kaggle: https://www.kaggle.com/vijay20213\n\n'}, {'content': '\n\nExperience - Fox Solutions Pvt. Ltd.\nRole: Automation Engineer Duration: Jun 2024 - Oct 2024 Location: Maharashtra\nKey Contributions:\n-Completed 2 months of internship plus 4 months of full-time work.\n-Worked with PLC and SCADA systems, focusing on automating processes and optimizing operational efficiency.\n-Collaborated with cross-functional teams to implement automation solutions for industrial applications.\n\n'}, {'content': '\n\nExperience - Cei Design Consultancy Pvt. Ltd.\nRole: Python Developer Intern Duration: Aug 2024 - Sept 2024 Location: Remote, Maharashtra\nKey Contributions:\n-Specialized in data processing using Python and Excel.\n-Utilized OpenCV for image processing tasks.\n\n'}, {'content': '\n\nExperience - Ujucode\nRole: Subject Matter Expert Intern Duration: Aug 2023 - Oct 2023 Location: Remote, Maharashtra\nKey Contributions:\n-Contributed as a Python developer for a ChatBot project.\n-Handled backend development tasks and researched Python modules.\n\n'}, {'content': '\n\nProject - Twitter Post Sentiment Prediction\nDetails:\n-Engineered an ETL pipeline using PySpark and SQL.\n-Conducted sentiment analysis using NLP (TF-IDF) and optimized hyperparameters.\n-Monitored model performance through MLFlow on Databricks.\n-Leveraged Google Cloud Storage and MySQL for data management.\n-Deployed the model using Docker and hosted it on Render.\n\n'}, {'content': '\n\nProject - Text-Text Chat-Bot\nDetails:\n-Designed an advanced Chat-Bot using the NVIDIA API and prompt engineering.\n-Features include paraphrasing, grammar correction, AI detection, plagiarism checking, and content summarization.\n-Targeted at content creators, researchers, and businesses.\n-Technologies Used: HTML, CSS, Python Flask, Cloud Database, and Render.\n\n'}, {'content': '\n\nProject - Hand Gesture Recognition\nDetails:\n-Used Google-s MediaPipe framework for detecting hand landmarks and gestures.\n-Created and labeled a custom dataset of hand gestures for training.\n-Developed a Streamlit application to improve accessibility and flexibility.\n\n'}, {'content': '\n\nTechnical Skills\nLanguages: MySQL, Python, HTML, CSS\nTechnologies: Streamlit, Flask, VS Code, GitHub, MLflow, Docker, PySpark, Databricks, Google Cloud Platform\n\n'}, {'content': '\n\nCertification\nMLOps Bootcamp: Mastering AI Operations for Success (Jun 2024)\n-Learned about the MLOps lifecycle and modular programming.\n-Acquired skills in Git, Python, Flask, and MLflow.\n\n'}, {'content': '\n\nEducation Details:\nBachelor of Technology in Electronics and Telecommunication (May 2024) Institution: SVERI-s College of Engineering, Pandharpur, Maharashtra Score: 81.71\nDiploma in Electronics and Telecommunication (May 2021) Institution: SVERI-s College of Engineering, Pandharpur, Maharashtra Score: 91.73\n\n'}, {'content': "\n\nBlogging\n\n\nSupervised, Unsupervised, and Beyond: ML Techniques Simplified\nNovember 25, 2024\nThere are several techniques for ML training. Among these, I will cover the following:\n\nSupervised and Unsupervised Learning\nSemi-Supervised Learning\nOffline and Online Learning\nInstance-Based and Model-Based Learning\n\nSupervised and Unsupervised Learning\nSupervised learning is like spoon-feeding our ML model in its initial stages, allowing it to learn and improve over time. Here, I’m referring to the training process.\n\nIn supervised learning, there are input columns and output columns, also called target columns. For example, in spam detection—a classification problem—the input is the email, and the target is whether the email is spam or not. That’s it!\nThis process resembles a student-teacher scenario where the teacher is a human, and the student is the model. The dataset serves as the knowledge used to train the student (model)\nSee content credentials\n\nhumand and model\nIn Unsupervised learning, we are not aware of the data labels. Instead, we separate the input data by analyzing similarities and grouping them into different clusters.\nOnce the clusters are formed, we can assign custom labels to each one. This technique is widely used to identify product relationships in online shopping and to recommend new products based on a user’s purchase history.\n\nUnsupervised Learning Clusters\nHere three clusters with three different image categories are formed\n\nSemi-Supervised Learning\nSemi-Supervised Learning is a combination of supervised and unsupervised learning, where some data is labeled and some data is unlabeled. A good example of this is Google Photos, which automatically separates new photos into their respective groups based on whether they contain a particular person in each image.\n\nThere are several techniques under semi-supervised learning; here, we will focus on the following:\nSelf Learning\nConsistency Regularization\nGenerative Models\nGraph-Based Learning\n\nLet's discuss them one by one:\n\nSelf Learning\nSelf Learning trains a model with labeled data and generates pseudo-labels for the unlabeled data. \n\nSelf learning\nThen, the model is trained on the entire dataset, which includes both the generated pseudo-labels and the labeled data.\n\nConsistency Regularization\nThis technique uses data augmentation to generate similar data, and then the model is enforced to predict the same outcome for both the augmented and original data. It helps create a model that can find similarities in both labeled and unlabeled data, predicting the same output for unlabeled data as it would for labeled data.\n\nData augmentation includes techniques like image flipping, blurring, rotation, etc. After augmenting the data, the model is trained to predict the same class for both the augmented and original images.\n\nGenerative Models\nGenerative models create synthetic data points and learn the underlying structure of the training data. These models can generate new datasets using encoders. \n\nExamples include Generative Adversarial Networks (GANs) and Variational Autoencoders (VAEs).\n\nGraph-Based Learning\nThis technique creates nodes for both labeled and unlabeled data. Similar nodes are then connected to each other.\nFor example, when dealing with images, each image starts as a separate node. If similar images are found based on their features, they are grouped or connected by an edge.\nRefer below graph to understand graph-based learning\nGraph based learning\nHere label 1 is came from labeled image of group 1, simillarly for label 2 and label 3. Finally we came up with three labels that means labeled data has three unique labels.\nBatch (Offline) and Online Learning\nBatch Learning\nBatch learning, also known as offline learning, is a technique where the developer needs to stop the deployed ML model, retrain it on new data, and then redeploy it. \nThis technique is useful when frequent updates are not required. For example, product recommendation systems can often be retrained weekly or even over several days.\nBatch learning can be costly when training the model on the entire dataset every time. However, using incremental learning models can help avoid unnecessary retraining, making the process more efficient.\nHere are some examples on incremental learning models, as of now we are focusing on general concepts so don't go much deeper into these types.\nStochastic Gradient Descent (SGD)\nNaive Bayes (Online Version)\nIncremental Support Vector Machines (ISVM)\nOnline Random Forests\nIncremental Decision Trees\nYou can refer this article to learn more about incremental learning\nHere’s your text with grammatical corrections and improved clarity:\nOnline Learning\nOnline learning is used when an ML model needs to be updated continuously with new inputs, such as in stock price prediction. In this scenario, the model must remain aware of the most recent data.\n\nBatch and online learning\nOnly models capable of incremental learning are used in online learning. Each new instance is fed back into the model to update its internal weights based on the latest input.\n\nInstance-Based and Model-Based Learning\nInstance-Based Learning\nInstance-based learning involves comparing a new input with stored data. If similarity is found, the model returns the label of the corresponding input. This approach uses pattern matching techniques. \nHere model works as a search engin not exactly same but it will find simmilarity from stored data.\nSome common models used in instance-based learning include:\nK-Nearest Neighbors (K-NN) with labels\nLocally Weighted Regression, etc.\nModel-Based Learning\nIn model-based learning, we train the model using a training dataset. During the training process, the model creates its own complex patterns (mathematical equations) to make predictions for future inputs. \nA simple example is the equation y=mx+cy = mx + c, which represents a trained model. Some common models used in model-based learning include:\nLinear Regression\nLogistic Regression\nDecision Tree, etc.\n\nSummary\nMachine Learning techniques can be broadly categorized into supervised, unsupervised, and semi-supervised learning. \nSupervised learning uses labeled data, like spam detection, while unsupervised learning works with unlabeled data, grouping similar points into clusters. \nSemi-supervised learning combines both, employing techniques like self-learning and consistency regularization to leverage partially labeled datasets.\nOther approaches include batch learning, where models are retrained periodically, and online learning, which updates models continuously with new data, suitable for dynamic tasks like stock price prediction. \nInstance-based learning relies on pattern matching (e.g., K-Nearest Neighbors), while model-based learning creates mathematical models (e.g., Linear Regression) to make predictions.\nThis is all about Machine Learning techniques. If you learned something, let me know in the comments. Your suggestions will help me improve my blogs.\nThanks for reading!\n\n"}, {'content': '\nBlogging\n\nMastering the End-to-End Machine Learning Lifecycle: From Data to Deployment\n\nNovember 24, 2024\n This article takes you through the complete lifecycle of a Machine Learning project. From ETL to deployment, I’ll share every detail of how I brought this project to life.\nData Engineering\nData Engineering serves as the starting point in the Machine Learning project lifecycle, bringing all distributed data together in one place.\nFor this project, I utilized an ETL pipeline—a core concept in Data Engineering. It enabled me to extract raw data, transform it into a meaningful format, and load it into a suitable location for further processing.\nLet\'s deep dive into ETL pipeline\nETL stands for Extract, Transform, and Load. These pipelines can be executed periodically (e.g., daily or hourly) to fetch real-time data, enhancing the predictive power of our Machine Learning model for real-world applications.\nThis process can be automated using tools like Apache Airflow, Kubeflow Pipelines, AWS Step Functions, and more, streamlining the workflow for consistent and efficient data updates.\n\nETL Pipeline:\nETL Pipeline\nExtract Operation\nThe extract operation is responsible for fetching data from various sources such as websites, databases, and APIs, as illustrated in the flowchart above.\n\nTransform Operation\nThe transform operation focuses on data cleaning, feature selection, and manipulation. In my project, I used this step to extract only the required features, ensuring the data is ready for the next phase.\n\nLoad Operation\nThe load operation transfers the transformed data to its destination, where we can choose the appropriate storage format. Typically, the processed data is stored in a database for further analysis and model training.\n\nFor my project, I used PySpark to build the ETL pipeline, as it enables efficient processing of large datasets.\nWhy not Pandas?\nWhile Pandas is excellent for small to medium-sized datasets, it stores DataFrames in RAM, which can lead to out-of-memory exceptions when handling large datasets.\nIn contrast, PySpark creates a session and processes data in chunks, storing it in ROM, making it ideal for handling large-scale data.\nFor the actual implementation of the pipeline using PySpark, please refer to the accompanying jupyter notebook.\nEDA (Data Analysis)\nData understanding is a critical stage before building any Machine Learning model. It allows us to analyze the data, plan the data processing steps, and gain insights into its structure and quality.\nIn this project, I examined the balance of data in my training and testing datasets and found it to be well-distributed across all four categories.\n\nBalanceness Checking on train and test data\nThroughout the entire process, I focused on two columns:\nTweets\nSentiments\nThe Tweets column contains the raw Twitter text data, while the Sentiments column serves as the target variable for prediction.\nNatural Language Processing\nI applied several key NLP techniques to preprocess the data and prepare it for Machine Learning model building. Below are the main steps I executed:\n\nText Processing\nRegex: I applied regular expressions to clean the text by removing URLs, hashtags, HTML tags, and keeping only alphanumeric characters. This helped eliminate unnecessary noise from the data.\nNLTK: Using the Natural Language Toolkit (NLTK), I performed word tokenization, stemming and lemmatization.\nWord tokenization is just splitting sentence into words, so we can processing each word from sentence individually.\nStemming helps us to truncate prefix or suffix of text to reduce count on unique words from corpus (paragraph).\nLemmatization is the proecss of converting any word into it\'s base word, for eg. Played will convert to play.\n\nWordCloud \nWord cloud concept help you to understand importance of words from given data. I splitted my data into four sections.\ndata for negative sentiments\ndata for positive sentiments\ndata for neutral sentiments\ndata for irrelevant sentiments\n\nHere is the representation of most frequent words for each category.\nCategorywise Word Cloud Presentation\nHere you can clearly see that there is no much difference in negative and positive sentiments data.\nthis is representation of bad data, here we can filterout our data for further processing, it might reduce data but you can do data augmentation techniques here to increase your data.\naugmenting of data in NLP with TF-IDF will not bad idea because TF-IDF and any other porcessing technique that I used is not able to detect sentence grammer or it doesn,t require sophisticated text. \nYou can think our input text will work as bag of word for model. you can understand by refering below vectorization technique.\nVectorization with TF-IDF \nI used Term Frequency-Inverse Document Frequency (TF-IDF) for text vectorization. This method transforms text into a numerical format, considering the importance of each word across documents, which prevents frequent words from dominating the model.\n\nTF-IDF\nTF-IDF stands for Term Frequency-Inverse Document Frequency. \nTerm Frequency (TF): This measures how frequently a term (t) appears in a specific document (d). It\'s calculated by dividing the number of occurrences of the term in the document by the total number of terms in that document.\nInverse Document Frequency (IDF): This measures how important a term is across the entire collection of documents. It\'s calculated by taking the logarithm of the ratio of the total number of documents (N) to the number of documents containing the term (df(t)). A higher IDF value indicates a rarer term, making it more significant.\nHere is the resulant data we got from TF-IDF\nAfter all preprocessing of text it\'s time to save our data for mlflow experiments. To see actual implementation of end to end process of NLP you can refer jupyter notebook.\nmlflow experiments\nMLflow experiments allow you to conduct multiple experiments with your trained model, helping you track and compare results over time. For running MLflow experiments, I prefer using Databricks as it offers an integrated experiment section, making the process much more streamlined and efficient.\nIn Databricks, you can easily connect your experiment by passing the experiment ID into the MLflow code. This integration simplifies the entire workflow, enabling better experiment tracking and easier comparison of model performance.\nFor my experiments, I worked with several models, including Logistic Regression, Multinomial Naive Bayes, and Decision Tree Classifier. By applying different combinations of parameters, I was able to experiment with and compare the performance of each model. Here\'s a preview of the Logistic Regression model’s F1 score, which highlights the model\'s ability to balance precision and recall:\nLogistic Regression F1 Score: 0.58\nThis approach allowed me to track the effectiveness of each model and make adjustments as needed for improving performance.\n\nF1 score with different parameters (Logistic Regression)\nI recommend running the notebook below in your own account to see the results. You\'ll definitely start appreciating the power of MLflow.\nFor more information, please refer to the accompanying Databricks notebook.\n\nHyperparameter tuning\nHyperparameter tuning is crucial for identifying the best parameter combination for your model. This technique involves an iterative process where, for every parameter combination, the model is trained and tested on a dataset. It is often described as a "trial and error" method.\nHowever, this approach can be computationally expensive, especially when working with complex or heavy machine learning models. For large-scale problems, hyperparameter tuning can be made more efficient through sampling or batch methods. In these methods, you don\'t use the entire dataset for training the model; instead, you choose random or stratified data points from the dataset to train the model. Although this may slightly reduce accuracy, it is more feasible when working with large datasets.\nFor my project, I used Grid Search CV to find the best hyperparameter combination for the model. Below are some common techniques for hyperparameter tuning, especially for large datasets:\nGrid Search This technique exhaustively searches through a specified set of hyperparameter values, trying all possible combinations. While effective, it can be computationally expensive for larger datasets due to the exhaustive nature of the search.\nRandom Search Randomly samples from the hyperparameter space, offering a faster alternative to grid search. This method explores a wider range of hyperparameters with fewer evaluations, making it more efficient for larger datasets.\nBayesian Optimization This method uses probabilistic models to predict the performance of different hyperparameters. It selects the next set of hyperparameters to evaluate based on previous results, making it more efficient and suitable for large datasets.\nGenetic Algorithms Inspired by natural selection, these algorithms iteratively evolve a population of hyperparameter sets to improve model performance. This method works well with complex search spaces.\nHyperband Hyperband combines random search with early stopping to dynamically allocate resources across multiple configurations, identifying promising hyperparameters quickly without excessive computational costs.\nBayesian Optimization with Gaussian Processes This technique models the hyperparameter search space using Gaussian processes, focusing on regions that are likely to yield better results, which is particularly useful for large datasets where computational resources are limited.\n\nTo optimize hyperparameter tuning for larger datasets, these techniques can be combined with parallel computing and distributed processing frameworks such as Dask, Spark, or multi-GPU setups. This enables more efficient hyperparameter search and reduces the overall computational overhead.\nI choosed Logistc Regression model and trained my Model with best parameters.\nSource Distribution for Model Packaging\nPackaging your machine learning model is a best practice, especially if you don’t plan to update it frequently. Imagine thousands of lines of code that can now be utilized with just a single line—this is the power of model packaging.\n\n1. Project Folder Setup\nTo ensure better organization, I created a main folder called sentiment_prediction and moved all machine learning pipeline files and dependencies into this folder. This helped in maintaining a clean structure and simplified the management of the entire project.\nBefore moving forward I recommend you to visit this pdf it will practically show you step by step process for building python package.\nPDF : step by step guid for python package building\n\n2. Manifest.in\nThe Manifest.in file plays a crucial role in controlling which files and folders should be included or excluded during the packaging process. It helps to specify the structure of the package for distribution.\nKey commands used in the Manifest.in file include:\ninclude <file/folder>: Include specific files or folders.\nexclude <file/folder>: Exclude specific files or folders.\nrecursive-include <path>: Include all files from a directory recursively.\nrecursive-exclude <path>: Exclude all files from a directory recursively.\n3. Setup.py\nThe setup.py file contains the project\'s metadata and is essential for creating the package. It defines key information about the project, such as:\nProject name, version, description, and author details.\nDependencies required for the package (install_requires), making it easy to install all necessary libraries.\n\n4. Building the Package\nTo build the package, I used the following command:\n\npython setup.py sdist bdist_wheel\nThis command generates two folders:\n\nbuild/: Contains the entire project package as defined in the Manifest.in.\ndist/: Contains the distributable files: .whl (wheel file) .gz (compressed source archive)\n\n5. Global Access via GitHub\nNow you can access your package gloablly, by refering your repository. I provided my package below go and check out.\n\nRepository: GitHub Repo Link\nYou can install the package directly from GitHub using the following command:\n\npip install git+https://github.com/vijaytakbhate2002/sentiment_prediction_python_package.git \nTo ensure it worked globally, I tested it again:\n\nfrom sentiment_prediction import predict\nprint(predict.predictor("Great progress shared today!")) \noutput:[\'Negative\']\nFlask Application \nBuilding application will help us to give our NLP model experience to people, so I built one flask application.\nWeb Application UI\n\nHere is demonstration of project: project demo\nI left a blank section for user suggestion and feedback, these feedbacks are getting stored in database for future model analysis or any business work.\nDatabase configuration\nFor storing collected user data we need to configure a database. It will help us to improve model as per user need.\nI used Google Cloud MySQL instance for integrating my application with database, GCP is paid but you can use free credit of GCP for first 3 months, for doing almost all Cloud Work.\nYou need to create your GCP account, then create one instance under SQL and by configuring your local system with instance you are good to go.\nDocker containerization\nIf you are not awared about docker and it\'s basic concepts you can refer my previous article which explains you all about docker.\nThis guide will help you build a solid foundation in Docker, enabling you to confidently use it for your projects.\nDocker guide: Comprehensive Docker guide for deploying Flas app\nDeploy\nDeployment of web app will help us to engange people and provide them real actual experience of our services.\nAfter deployment you need to collect user data and store it for future analysis, this data contain user feedback and suggestions.\nAfter deployment it\'s not end of the process we need to collect user feedback and again follow same steps fine tune Model as per user need.\n\nSummary of Blog\nThis blog covers the lifecycle of a Machine Learning project, from ETL to deployment. It details building an ETL pipeline using PySpark for efficient data handling, EDA, and NLP preprocessing techniques like tokenization, TF-IDF, and WordCloud visualization. \nIt highlights ML experiments with MLflow on Databricks and hyperparameter tuning using Grid Search.\nThe model was packaged into a Python package and deployed as a Flask application with a database backend (Google Cloud MySQL) and Dockerized for scalability. \nThe app collects user feedback for continuous improvement. It emphasizes end-to-end integration, including cloud and containerization, to deliver a robust ML solution.\nHappy Learning!\n\n'}, {'content': '\nLanguages I Speak\nEnglish, Marathi, Hindi\n\n'}, {'content': '\n\nSoft skills:\nCritical Thinking, Intellectual Rigor, Problem Solving, Understanding Business Needs'}]
2024-11-25 21:43:25 - root - INFO - Writing documents to the document store...
2024-11-25 21:43:29 - haystack.document_stores.faiss - INFO - Updating embeddings for 14 docs...
2024-11-25 21:43:40 - root - INFO - Documents and embeddings updated.
2024-11-25 21:43:40 - haystack.modeling.utils - INFO - Using devices: CPU - Number of GPUs: 0
2024-11-25 21:43:40 - haystack.modeling.utils - INFO - Using devices: CPU - Number of GPUs: 0
2024-11-25 21:43:41 - haystack.modeling.model.language_model - INFO -  * LOADING MODEL: 'deepset/roberta-base-squad2' (Roberta)
2024-11-25 21:43:42 - haystack.modeling.model.language_model - INFO - Auto-detected model language: english
2024-11-25 21:43:42 - haystack.modeling.model.language_model - INFO - Loaded 'deepset/roberta-base-squad2' (Roberta model) from model hub.
2024-11-25 21:43:43 - haystack.modeling.utils - INFO - Using devices: CPU - Number of GPUs: 0
2024-11-25 21:43:43 - haystack.modeling.data_handler.processor - ERROR - There were 1 errors during preprocessing at positions: {0}
2024-11-25 21:46:58 - root - INFO - Loading existing FAISS document store...
2024-11-25 21:46:59 - haystack.modeling.utils - INFO - Using devices: CPU - Number of GPUs: 0
2024-11-25 21:47:00 - haystack.modeling.model.language_model - WARNING - Using a model of type 'mpnet' which might be incompatible with DPR encoders. Only Bert-based encoders are supported. They need input_ids, token_type_ids, attention_mask as input tensors.
2024-11-25 21:47:02 - haystack.modeling.model.language_model - INFO - Auto-detected model language: english
2024-11-25 21:47:03 - haystack.modeling.model.language_model - WARNING - Using a model of type 'mpnet' which might be incompatible with DPR encoders. Only Bert-based encoders are supported. They need input_ids, token_type_ids, attention_mask as input tensors.
2024-11-25 21:47:06 - haystack.modeling.model.language_model - INFO - Auto-detected model language: english
2024-11-25 21:47:06 - root - INFO - Reading text document
2024-11-25 21:47:06 - root - INFO - loaded document = [{'content': '\nPersonal Information\n\nName: Vijay Takbhate\nEmail: vijaytakbhate20@gmail.com Phone: 8767363681\nGitHub: https://github.com/vijaytakbhate2002\nLinkedIn: https://www.linkedin.com/in/vijay-takbhate-b9231a236/ Kaggle: https://www.kaggle.com/vijay20213\n\n'}, {'content': '\n\nExperience - Fox Solutions Pvt. Ltd.\nRole: Automation Engineer Duration: Jun 2024 - Oct 2024 Location: Maharashtra\nKey Contributions:\n-Completed 2 months of internship plus 4 months of full-time work.\n-Worked with PLC and SCADA systems, focusing on automating processes and optimizing operational efficiency.\n-Collaborated with cross-functional teams to implement automation solutions for industrial applications.\n\n'}, {'content': '\n\nExperience - Cei Design Consultancy Pvt. Ltd.\nRole: Python Developer Intern Duration: Aug 2024 - Sept 2024 Location: Remote, Maharashtra\nKey Contributions:\n-Specialized in data processing using Python and Excel.\n-Utilized OpenCV for image processing tasks.\n\n'}, {'content': '\n\nExperience - Ujucode\nRole: Subject Matter Expert Intern Duration: Aug 2023 - Oct 2023 Location: Remote, Maharashtra\nKey Contributions:\n-Contributed as a Python developer for a ChatBot project.\n-Handled backend development tasks and researched Python modules.\n\n'}, {'content': '\n\nProject - Twitter Post Sentiment Prediction\nDetails:\n-Engineered an ETL pipeline using PySpark and SQL.\n-Conducted sentiment analysis using NLP (TF-IDF) and optimized hyperparameters.\n-Monitored model performance through MLFlow on Databricks.\n-Leveraged Google Cloud Storage and MySQL for data management.\n-Deployed the model using Docker and hosted it on Render.\n\n'}, {'content': '\n\nProject - Text-Text Chat-Bot\nDetails:\n-Designed an advanced Chat-Bot using the NVIDIA API and prompt engineering.\n-Features include paraphrasing, grammar correction, AI detection, plagiarism checking, and content summarization.\n-Targeted at content creators, researchers, and businesses.\n-Technologies Used: HTML, CSS, Python Flask, Cloud Database, and Render.\n\n'}, {'content': '\n\nProject - Hand Gesture Recognition\nDetails:\n-Used Google-s MediaPipe framework for detecting hand landmarks and gestures.\n-Created and labeled a custom dataset of hand gestures for training.\n-Developed a Streamlit application to improve accessibility and flexibility.\n\n'}, {'content': '\n\nTechnical Skills\nLanguages: MySQL, Python, HTML, CSS\nTechnologies: Streamlit, Flask, VS Code, GitHub, MLflow, Docker, PySpark, Databricks, Google Cloud Platform\n\n'}, {'content': '\n\nCertification\nMLOps Bootcamp: Mastering AI Operations for Success (Jun 2024)\n-Learned about the MLOps lifecycle and modular programming.\n-Acquired skills in Git, Python, Flask, and MLflow.\n\n'}, {'content': '\n\nEducation Details:\nBachelor of Technology in Electronics and Telecommunication (May 2024) Institution: SVERI-s College of Engineering, Pandharpur, Maharashtra Score: 81.71\nDiploma in Electronics and Telecommunication (May 2021) Institution: SVERI-s College of Engineering, Pandharpur, Maharashtra Score: 91.73\n\n'}, {'content': "\n\nBlogging\n\n\nSupervised, Unsupervised, and Beyond: ML Techniques Simplified\nNovember 25, 2024\nThere are several techniques for ML training. Among these, I will cover the following:\n\nSupervised and Unsupervised Learning\nSemi-Supervised Learning\nOffline and Online Learning\nInstance-Based and Model-Based Learning\n\nSupervised and Unsupervised Learning\nSupervised learning is like spoon-feeding our ML model in its initial stages, allowing it to learn and improve over time. Here, I’m referring to the training process.\n\nIn supervised learning, there are input columns and output columns, also called target columns. For example, in spam detection—a classification problem—the input is the email, and the target is whether the email is spam or not. That’s it!\nThis process resembles a student-teacher scenario where the teacher is a human, and the student is the model. The dataset serves as the knowledge used to train the student (model)\nSee content credentials\n\nhumand and model\nIn Unsupervised learning, we are not aware of the data labels. Instead, we separate the input data by analyzing similarities and grouping them into different clusters.\nOnce the clusters are formed, we can assign custom labels to each one. This technique is widely used to identify product relationships in online shopping and to recommend new products based on a user’s purchase history.\n\nUnsupervised Learning Clusters\nHere three clusters with three different image categories are formed\n\nSemi-Supervised Learning\nSemi-Supervised Learning is a combination of supervised and unsupervised learning, where some data is labeled and some data is unlabeled. A good example of this is Google Photos, which automatically separates new photos into their respective groups based on whether they contain a particular person in each image.\n\nThere are several techniques under semi-supervised learning; here, we will focus on the following:\nSelf Learning\nConsistency Regularization\nGenerative Models\nGraph-Based Learning\n\nLet's discuss them one by one:\n\nSelf Learning\nSelf Learning trains a model with labeled data and generates pseudo-labels for the unlabeled data. \n\nSelf learning\nThen, the model is trained on the entire dataset, which includes both the generated pseudo-labels and the labeled data.\n\nConsistency Regularization\nThis technique uses data augmentation to generate similar data, and then the model is enforced to predict the same outcome for both the augmented and original data. It helps create a model that can find similarities in both labeled and unlabeled data, predicting the same output for unlabeled data as it would for labeled data.\n\nData augmentation includes techniques like image flipping, blurring, rotation, etc. After augmenting the data, the model is trained to predict the same class for both the augmented and original images.\n\nGenerative Models\nGenerative models create synthetic data points and learn the underlying structure of the training data. These models can generate new datasets using encoders. \n\nExamples include Generative Adversarial Networks (GANs) and Variational Autoencoders (VAEs).\n\nGraph-Based Learning\nThis technique creates nodes for both labeled and unlabeled data. Similar nodes are then connected to each other.\nFor example, when dealing with images, each image starts as a separate node. If similar images are found based on their features, they are grouped or connected by an edge.\nRefer below graph to understand graph-based learning\nGraph based learning\nHere label 1 is came from labeled image of group 1, simillarly for label 2 and label 3. Finally we came up with three labels that means labeled data has three unique labels.\nBatch (Offline) and Online Learning\nBatch Learning\nBatch learning, also known as offline learning, is a technique where the developer needs to stop the deployed ML model, retrain it on new data, and then redeploy it. \nThis technique is useful when frequent updates are not required. For example, product recommendation systems can often be retrained weekly or even over several days.\nBatch learning can be costly when training the model on the entire dataset every time. However, using incremental learning models can help avoid unnecessary retraining, making the process more efficient.\nHere are some examples on incremental learning models, as of now we are focusing on general concepts so don't go much deeper into these types.\nStochastic Gradient Descent (SGD)\nNaive Bayes (Online Version)\nIncremental Support Vector Machines (ISVM)\nOnline Random Forests\nIncremental Decision Trees\nYou can refer this article to learn more about incremental learning\nHere’s your text with grammatical corrections and improved clarity:\nOnline Learning\nOnline learning is used when an ML model needs to be updated continuously with new inputs, such as in stock price prediction. In this scenario, the model must remain aware of the most recent data.\n\nBatch and online learning\nOnly models capable of incremental learning are used in online learning. Each new instance is fed back into the model to update its internal weights based on the latest input.\n\nInstance-Based and Model-Based Learning\nInstance-Based Learning\nInstance-based learning involves comparing a new input with stored data. If similarity is found, the model returns the label of the corresponding input. This approach uses pattern matching techniques. \nHere model works as a search engin not exactly same but it will find simmilarity from stored data.\nSome common models used in instance-based learning include:\nK-Nearest Neighbors (K-NN) with labels\nLocally Weighted Regression, etc.\nModel-Based Learning\nIn model-based learning, we train the model using a training dataset. During the training process, the model creates its own complex patterns (mathematical equations) to make predictions for future inputs. \nA simple example is the equation y=mx+cy = mx + c, which represents a trained model. Some common models used in model-based learning include:\nLinear Regression\nLogistic Regression\nDecision Tree, etc.\n\nSummary\nMachine Learning techniques can be broadly categorized into supervised, unsupervised, and semi-supervised learning. \nSupervised learning uses labeled data, like spam detection, while unsupervised learning works with unlabeled data, grouping similar points into clusters. \nSemi-supervised learning combines both, employing techniques like self-learning and consistency regularization to leverage partially labeled datasets.\nOther approaches include batch learning, where models are retrained periodically, and online learning, which updates models continuously with new data, suitable for dynamic tasks like stock price prediction. \nInstance-based learning relies on pattern matching (e.g., K-Nearest Neighbors), while model-based learning creates mathematical models (e.g., Linear Regression) to make predictions.\nThis is all about Machine Learning techniques. If you learned something, let me know in the comments. Your suggestions will help me improve my blogs.\nThanks for reading!\n\n"}, {'content': '\nBlogging\n\nMastering the End-to-End Machine Learning Lifecycle: From Data to Deployment\n\nNovember 24, 2024\n This article takes you through the complete lifecycle of a Machine Learning project. From ETL to deployment, I’ll share every detail of how I brought this project to life.\nData Engineering\nData Engineering serves as the starting point in the Machine Learning project lifecycle, bringing all distributed data together in one place.\nFor this project, I utilized an ETL pipeline—a core concept in Data Engineering. It enabled me to extract raw data, transform it into a meaningful format, and load it into a suitable location for further processing.\nLet\'s deep dive into ETL pipeline\nETL stands for Extract, Transform, and Load. These pipelines can be executed periodically (e.g., daily or hourly) to fetch real-time data, enhancing the predictive power of our Machine Learning model for real-world applications.\nThis process can be automated using tools like Apache Airflow, Kubeflow Pipelines, AWS Step Functions, and more, streamlining the workflow for consistent and efficient data updates.\n\nETL Pipeline:\nETL Pipeline\nExtract Operation\nThe extract operation is responsible for fetching data from various sources such as websites, databases, and APIs, as illustrated in the flowchart above.\n\nTransform Operation\nThe transform operation focuses on data cleaning, feature selection, and manipulation. In my project, I used this step to extract only the required features, ensuring the data is ready for the next phase.\n\nLoad Operation\nThe load operation transfers the transformed data to its destination, where we can choose the appropriate storage format. Typically, the processed data is stored in a database for further analysis and model training.\n\nFor my project, I used PySpark to build the ETL pipeline, as it enables efficient processing of large datasets.\nWhy not Pandas?\nWhile Pandas is excellent for small to medium-sized datasets, it stores DataFrames in RAM, which can lead to out-of-memory exceptions when handling large datasets.\nIn contrast, PySpark creates a session and processes data in chunks, storing it in ROM, making it ideal for handling large-scale data.\nFor the actual implementation of the pipeline using PySpark, please refer to the accompanying jupyter notebook.\nEDA (Data Analysis)\nData understanding is a critical stage before building any Machine Learning model. It allows us to analyze the data, plan the data processing steps, and gain insights into its structure and quality.\nIn this project, I examined the balance of data in my training and testing datasets and found it to be well-distributed across all four categories.\n\nBalanceness Checking on train and test data\nThroughout the entire process, I focused on two columns:\nTweets\nSentiments\nThe Tweets column contains the raw Twitter text data, while the Sentiments column serves as the target variable for prediction.\nNatural Language Processing\nI applied several key NLP techniques to preprocess the data and prepare it for Machine Learning model building. Below are the main steps I executed:\n\nText Processing\nRegex: I applied regular expressions to clean the text by removing URLs, hashtags, HTML tags, and keeping only alphanumeric characters. This helped eliminate unnecessary noise from the data.\nNLTK: Using the Natural Language Toolkit (NLTK), I performed word tokenization, stemming and lemmatization.\nWord tokenization is just splitting sentence into words, so we can processing each word from sentence individually.\nStemming helps us to truncate prefix or suffix of text to reduce count on unique words from corpus (paragraph).\nLemmatization is the proecss of converting any word into it\'s base word, for eg. Played will convert to play.\n\nWordCloud \nWord cloud concept help you to understand importance of words from given data. I splitted my data into four sections.\ndata for negative sentiments\ndata for positive sentiments\ndata for neutral sentiments\ndata for irrelevant sentiments\n\nHere is the representation of most frequent words for each category.\nCategorywise Word Cloud Presentation\nHere you can clearly see that there is no much difference in negative and positive sentiments data.\nthis is representation of bad data, here we can filterout our data for further processing, it might reduce data but you can do data augmentation techniques here to increase your data.\naugmenting of data in NLP with TF-IDF will not bad idea because TF-IDF and any other porcessing technique that I used is not able to detect sentence grammer or it doesn,t require sophisticated text. \nYou can think our input text will work as bag of word for model. you can understand by refering below vectorization technique.\nVectorization with TF-IDF \nI used Term Frequency-Inverse Document Frequency (TF-IDF) for text vectorization. This method transforms text into a numerical format, considering the importance of each word across documents, which prevents frequent words from dominating the model.\n\nTF-IDF\nTF-IDF stands for Term Frequency-Inverse Document Frequency. \nTerm Frequency (TF): This measures how frequently a term (t) appears in a specific document (d). It\'s calculated by dividing the number of occurrences of the term in the document by the total number of terms in that document.\nInverse Document Frequency (IDF): This measures how important a term is across the entire collection of documents. It\'s calculated by taking the logarithm of the ratio of the total number of documents (N) to the number of documents containing the term (df(t)). A higher IDF value indicates a rarer term, making it more significant.\nHere is the resulant data we got from TF-IDF\nAfter all preprocessing of text it\'s time to save our data for mlflow experiments. To see actual implementation of end to end process of NLP you can refer jupyter notebook.\nmlflow experiments\nMLflow experiments allow you to conduct multiple experiments with your trained model, helping you track and compare results over time. For running MLflow experiments, I prefer using Databricks as it offers an integrated experiment section, making the process much more streamlined and efficient.\nIn Databricks, you can easily connect your experiment by passing the experiment ID into the MLflow code. This integration simplifies the entire workflow, enabling better experiment tracking and easier comparison of model performance.\nFor my experiments, I worked with several models, including Logistic Regression, Multinomial Naive Bayes, and Decision Tree Classifier. By applying different combinations of parameters, I was able to experiment with and compare the performance of each model. Here\'s a preview of the Logistic Regression model’s F1 score, which highlights the model\'s ability to balance precision and recall:\nLogistic Regression F1 Score: 0.58\nThis approach allowed me to track the effectiveness of each model and make adjustments as needed for improving performance.\n\nF1 score with different parameters (Logistic Regression)\nI recommend running the notebook below in your own account to see the results. You\'ll definitely start appreciating the power of MLflow.\nFor more information, please refer to the accompanying Databricks notebook.\n\nHyperparameter tuning\nHyperparameter tuning is crucial for identifying the best parameter combination for your model. This technique involves an iterative process where, for every parameter combination, the model is trained and tested on a dataset. It is often described as a "trial and error" method.\nHowever, this approach can be computationally expensive, especially when working with complex or heavy machine learning models. For large-scale problems, hyperparameter tuning can be made more efficient through sampling or batch methods. In these methods, you don\'t use the entire dataset for training the model; instead, you choose random or stratified data points from the dataset to train the model. Although this may slightly reduce accuracy, it is more feasible when working with large datasets.\nFor my project, I used Grid Search CV to find the best hyperparameter combination for the model. Below are some common techniques for hyperparameter tuning, especially for large datasets:\nGrid Search This technique exhaustively searches through a specified set of hyperparameter values, trying all possible combinations. While effective, it can be computationally expensive for larger datasets due to the exhaustive nature of the search.\nRandom Search Randomly samples from the hyperparameter space, offering a faster alternative to grid search. This method explores a wider range of hyperparameters with fewer evaluations, making it more efficient for larger datasets.\nBayesian Optimization This method uses probabilistic models to predict the performance of different hyperparameters. It selects the next set of hyperparameters to evaluate based on previous results, making it more efficient and suitable for large datasets.\nGenetic Algorithms Inspired by natural selection, these algorithms iteratively evolve a population of hyperparameter sets to improve model performance. This method works well with complex search spaces.\nHyperband Hyperband combines random search with early stopping to dynamically allocate resources across multiple configurations, identifying promising hyperparameters quickly without excessive computational costs.\nBayesian Optimization with Gaussian Processes This technique models the hyperparameter search space using Gaussian processes, focusing on regions that are likely to yield better results, which is particularly useful for large datasets where computational resources are limited.\n\nTo optimize hyperparameter tuning for larger datasets, these techniques can be combined with parallel computing and distributed processing frameworks such as Dask, Spark, or multi-GPU setups. This enables more efficient hyperparameter search and reduces the overall computational overhead.\nI choosed Logistc Regression model and trained my Model with best parameters.\nSource Distribution for Model Packaging\nPackaging your machine learning model is a best practice, especially if you don’t plan to update it frequently. Imagine thousands of lines of code that can now be utilized with just a single line—this is the power of model packaging.\n\n1. Project Folder Setup\nTo ensure better organization, I created a main folder called sentiment_prediction and moved all machine learning pipeline files and dependencies into this folder. This helped in maintaining a clean structure and simplified the management of the entire project.\nBefore moving forward I recommend you to visit this pdf it will practically show you step by step process for building python package.\nPDF : step by step guid for python package building\n\n2. Manifest.in\nThe Manifest.in file plays a crucial role in controlling which files and folders should be included or excluded during the packaging process. It helps to specify the structure of the package for distribution.\nKey commands used in the Manifest.in file include:\ninclude <file/folder>: Include specific files or folders.\nexclude <file/folder>: Exclude specific files or folders.\nrecursive-include <path>: Include all files from a directory recursively.\nrecursive-exclude <path>: Exclude all files from a directory recursively.\n3. Setup.py\nThe setup.py file contains the project\'s metadata and is essential for creating the package. It defines key information about the project, such as:\nProject name, version, description, and author details.\nDependencies required for the package (install_requires), making it easy to install all necessary libraries.\n\n4. Building the Package\nTo build the package, I used the following command:\n\npython setup.py sdist bdist_wheel\nThis command generates two folders:\n\nbuild/: Contains the entire project package as defined in the Manifest.in.\ndist/: Contains the distributable files: .whl (wheel file) .gz (compressed source archive)\n\n5. Global Access via GitHub\nNow you can access your package gloablly, by refering your repository. I provided my package below go and check out.\n\nRepository: GitHub Repo Link\nYou can install the package directly from GitHub using the following command:\n\npip install git+https://github.com/vijaytakbhate2002/sentiment_prediction_python_package.git \nTo ensure it worked globally, I tested it again:\n\nfrom sentiment_prediction import predict\nprint(predict.predictor("Great progress shared today!")) \noutput:[\'Negative\']\nFlask Application \nBuilding application will help us to give our NLP model experience to people, so I built one flask application.\nWeb Application UI\n\nHere is demonstration of project: project demo\nI left a blank section for user suggestion and feedback, these feedbacks are getting stored in database for future model analysis or any business work.\nDatabase configuration\nFor storing collected user data we need to configure a database. It will help us to improve model as per user need.\nI used Google Cloud MySQL instance for integrating my application with database, GCP is paid but you can use free credit of GCP for first 3 months, for doing almost all Cloud Work.\nYou need to create your GCP account, then create one instance under SQL and by configuring your local system with instance you are good to go.\nDocker containerization\nIf you are not awared about docker and it\'s basic concepts you can refer my previous article which explains you all about docker.\nThis guide will help you build a solid foundation in Docker, enabling you to confidently use it for your projects.\nDocker guide: Comprehensive Docker guide for deploying Flas app\nDeploy\nDeployment of web app will help us to engange people and provide them real actual experience of our services.\nAfter deployment you need to collect user data and store it for future analysis, this data contain user feedback and suggestions.\nAfter deployment it\'s not end of the process we need to collect user feedback and again follow same steps fine tune Model as per user need.\n\nSummary of Blog\nThis blog covers the lifecycle of a Machine Learning project, from ETL to deployment. It details building an ETL pipeline using PySpark for efficient data handling, EDA, and NLP preprocessing techniques like tokenization, TF-IDF, and WordCloud visualization. \nIt highlights ML experiments with MLflow on Databricks and hyperparameter tuning using Grid Search.\nThe model was packaged into a Python package and deployed as a Flask application with a database backend (Google Cloud MySQL) and Dockerized for scalability. \nThe app collects user feedback for continuous improvement. It emphasizes end-to-end integration, including cloud and containerization, to deliver a robust ML solution.\nHappy Learning!\n\n'}, {'content': '\nLanguages I Speak\nEnglish, Marathi, Hindi\n\n'}, {'content': '\n\nSoft skills:\nCritical Thinking, Intellectual Rigor, Problem Solving, Understanding Business Needs'}]
2024-11-25 21:47:06 - root - INFO - Document store already contains 14 documents. Skipping write.
2024-11-25 21:47:06 - haystack.modeling.utils - INFO - Using devices: CPU - Number of GPUs: 0
2024-11-25 21:47:06 - haystack.modeling.utils - INFO - Using devices: CPU - Number of GPUs: 0
2024-11-25 21:47:06 - haystack.modeling.model.language_model - INFO -  * LOADING MODEL: 'deepset/roberta-base-squad2' (Roberta)
2024-11-25 21:47:07 - haystack.modeling.model.language_model - INFO - Auto-detected model language: english
2024-11-25 21:47:07 - haystack.modeling.model.language_model - INFO - Loaded 'deepset/roberta-base-squad2' (Roberta model) from model hub.
2024-11-25 21:47:08 - haystack.modeling.utils - INFO - Using devices: CPU - Number of GPUs: 0
2024-11-25 21:47:08 - haystack.modeling.data_handler.processor - ERROR - There were 1 errors during preprocessing at positions: {0}
2024-11-25 21:48:05 - root - INFO - Loading existing FAISS document store...
2024-11-25 21:48:05 - haystack.modeling.utils - INFO - Using devices: CPU - Number of GPUs: 0
2024-11-25 21:48:06 - haystack.modeling.model.language_model - WARNING - Using a model of type 'mpnet' which might be incompatible with DPR encoders. Only Bert-based encoders are supported. They need input_ids, token_type_ids, attention_mask as input tensors.
2024-11-25 21:48:09 - haystack.modeling.model.language_model - INFO - Auto-detected model language: english
2024-11-25 21:48:10 - haystack.modeling.model.language_model - WARNING - Using a model of type 'mpnet' which might be incompatible with DPR encoders. Only Bert-based encoders are supported. They need input_ids, token_type_ids, attention_mask as input tensors.
2024-11-25 21:48:12 - haystack.modeling.model.language_model - INFO - Auto-detected model language: english
2024-11-25 21:48:12 - root - INFO - Reading text document
2024-11-25 21:48:12 - root - INFO - loaded document = [{'content': '\nPersonal Information\n\nName: Vijay Takbhate\nEmail: vijaytakbhate20@gmail.com Phone: 8767363681\nGitHub: https://github.com/vijaytakbhate2002\nLinkedIn: https://www.linkedin.com/in/vijay-takbhate-b9231a236/ Kaggle: https://www.kaggle.com/vijay20213\n\n'}, {'content': '\n\nExperience - Fox Solutions Pvt. Ltd.\nRole: Automation Engineer Duration: Jun 2024 - Oct 2024 Location: Maharashtra\nKey Contributions:\n-Completed 2 months of internship plus 4 months of full-time work.\n-Worked with PLC and SCADA systems, focusing on automating processes and optimizing operational efficiency.\n-Collaborated with cross-functional teams to implement automation solutions for industrial applications.\n\n'}, {'content': '\n\nExperience - Cei Design Consultancy Pvt. Ltd.\nRole: Python Developer Intern Duration: Aug 2024 - Sept 2024 Location: Remote, Maharashtra\nKey Contributions:\n-Specialized in data processing using Python and Excel.\n-Utilized OpenCV for image processing tasks.\n\n'}, {'content': '\n\nExperience - Ujucode\nRole: Subject Matter Expert Intern Duration: Aug 2023 - Oct 2023 Location: Remote, Maharashtra\nKey Contributions:\n-Contributed as a Python developer for a ChatBot project.\n-Handled backend development tasks and researched Python modules.\n\n'}, {'content': '\n\nProject - Twitter Post Sentiment Prediction\nDetails:\n-Engineered an ETL pipeline using PySpark and SQL.\n-Conducted sentiment analysis using NLP (TF-IDF) and optimized hyperparameters.\n-Monitored model performance through MLFlow on Databricks.\n-Leveraged Google Cloud Storage and MySQL for data management.\n-Deployed the model using Docker and hosted it on Render.\n\n'}, {'content': '\n\nProject - Text-Text Chat-Bot\nDetails:\n-Designed an advanced Chat-Bot using the NVIDIA API and prompt engineering.\n-Features include paraphrasing, grammar correction, AI detection, plagiarism checking, and content summarization.\n-Targeted at content creators, researchers, and businesses.\n-Technologies Used: HTML, CSS, Python Flask, Cloud Database, and Render.\n\n'}, {'content': '\n\nProject - Hand Gesture Recognition\nDetails:\n-Used Google-s MediaPipe framework for detecting hand landmarks and gestures.\n-Created and labeled a custom dataset of hand gestures for training.\n-Developed a Streamlit application to improve accessibility and flexibility.\n\n'}, {'content': '\n\nTechnical Skills\nLanguages: MySQL, Python, HTML, CSS\nTechnologies: Streamlit, Flask, VS Code, GitHub, MLflow, Docker, PySpark, Databricks, Google Cloud Platform\n\n'}, {'content': '\n\nCertification\nMLOps Bootcamp: Mastering AI Operations for Success (Jun 2024)\n-Learned about the MLOps lifecycle and modular programming.\n-Acquired skills in Git, Python, Flask, and MLflow.\n\n'}, {'content': '\n\nEducation Details:\nBachelor of Technology in Electronics and Telecommunication (May 2024) Institution: SVERI-s College of Engineering, Pandharpur, Maharashtra Score: 81.71\nDiploma in Electronics and Telecommunication (May 2021) Institution: SVERI-s College of Engineering, Pandharpur, Maharashtra Score: 91.73\n\n'}, {'content': "\n\nBlogging\n\n\nSupervised, Unsupervised, and Beyond: ML Techniques Simplified\nNovember 25, 2024\nThere are several techniques for ML training. Among these, I will cover the following:\n\nSupervised and Unsupervised Learning\nSemi-Supervised Learning\nOffline and Online Learning\nInstance-Based and Model-Based Learning\n\nSupervised and Unsupervised Learning\nSupervised learning is like spoon-feeding our ML model in its initial stages, allowing it to learn and improve over time. Here, I’m referring to the training process.\n\nIn supervised learning, there are input columns and output columns, also called target columns. For example, in spam detection—a classification problem—the input is the email, and the target is whether the email is spam or not. That’s it!\nThis process resembles a student-teacher scenario where the teacher is a human, and the student is the model. The dataset serves as the knowledge used to train the student (model)\nSee content credentials\n\nhumand and model\nIn Unsupervised learning, we are not aware of the data labels. Instead, we separate the input data by analyzing similarities and grouping them into different clusters.\nOnce the clusters are formed, we can assign custom labels to each one. This technique is widely used to identify product relationships in online shopping and to recommend new products based on a user’s purchase history.\n\nUnsupervised Learning Clusters\nHere three clusters with three different image categories are formed\n\nSemi-Supervised Learning\nSemi-Supervised Learning is a combination of supervised and unsupervised learning, where some data is labeled and some data is unlabeled. A good example of this is Google Photos, which automatically separates new photos into their respective groups based on whether they contain a particular person in each image.\n\nThere are several techniques under semi-supervised learning; here, we will focus on the following:\nSelf Learning\nConsistency Regularization\nGenerative Models\nGraph-Based Learning\n\nLet's discuss them one by one:\n\nSelf Learning\nSelf Learning trains a model with labeled data and generates pseudo-labels for the unlabeled data. \n\nSelf learning\nThen, the model is trained on the entire dataset, which includes both the generated pseudo-labels and the labeled data.\n\nConsistency Regularization\nThis technique uses data augmentation to generate similar data, and then the model is enforced to predict the same outcome for both the augmented and original data. It helps create a model that can find similarities in both labeled and unlabeled data, predicting the same output for unlabeled data as it would for labeled data.\n\nData augmentation includes techniques like image flipping, blurring, rotation, etc. After augmenting the data, the model is trained to predict the same class for both the augmented and original images.\n\nGenerative Models\nGenerative models create synthetic data points and learn the underlying structure of the training data. These models can generate new datasets using encoders. \n\nExamples include Generative Adversarial Networks (GANs) and Variational Autoencoders (VAEs).\n\nGraph-Based Learning\nThis technique creates nodes for both labeled and unlabeled data. Similar nodes are then connected to each other.\nFor example, when dealing with images, each image starts as a separate node. If similar images are found based on their features, they are grouped or connected by an edge.\nRefer below graph to understand graph-based learning\nGraph based learning\nHere label 1 is came from labeled image of group 1, simillarly for label 2 and label 3. Finally we came up with three labels that means labeled data has three unique labels.\nBatch (Offline) and Online Learning\nBatch Learning\nBatch learning, also known as offline learning, is a technique where the developer needs to stop the deployed ML model, retrain it on new data, and then redeploy it. \nThis technique is useful when frequent updates are not required. For example, product recommendation systems can often be retrained weekly or even over several days.\nBatch learning can be costly when training the model on the entire dataset every time. However, using incremental learning models can help avoid unnecessary retraining, making the process more efficient.\nHere are some examples on incremental learning models, as of now we are focusing on general concepts so don't go much deeper into these types.\nStochastic Gradient Descent (SGD)\nNaive Bayes (Online Version)\nIncremental Support Vector Machines (ISVM)\nOnline Random Forests\nIncremental Decision Trees\nYou can refer this article to learn more about incremental learning\nHere’s your text with grammatical corrections and improved clarity:\nOnline Learning\nOnline learning is used when an ML model needs to be updated continuously with new inputs, such as in stock price prediction. In this scenario, the model must remain aware of the most recent data.\n\nBatch and online learning\nOnly models capable of incremental learning are used in online learning. Each new instance is fed back into the model to update its internal weights based on the latest input.\n\nInstance-Based and Model-Based Learning\nInstance-Based Learning\nInstance-based learning involves comparing a new input with stored data. If similarity is found, the model returns the label of the corresponding input. This approach uses pattern matching techniques. \nHere model works as a search engin not exactly same but it will find simmilarity from stored data.\nSome common models used in instance-based learning include:\nK-Nearest Neighbors (K-NN) with labels\nLocally Weighted Regression, etc.\nModel-Based Learning\nIn model-based learning, we train the model using a training dataset. During the training process, the model creates its own complex patterns (mathematical equations) to make predictions for future inputs. \nA simple example is the equation y=mx+cy = mx + c, which represents a trained model. Some common models used in model-based learning include:\nLinear Regression\nLogistic Regression\nDecision Tree, etc.\n\nSummary\nMachine Learning techniques can be broadly categorized into supervised, unsupervised, and semi-supervised learning. \nSupervised learning uses labeled data, like spam detection, while unsupervised learning works with unlabeled data, grouping similar points into clusters. \nSemi-supervised learning combines both, employing techniques like self-learning and consistency regularization to leverage partially labeled datasets.\nOther approaches include batch learning, where models are retrained periodically, and online learning, which updates models continuously with new data, suitable for dynamic tasks like stock price prediction. \nInstance-based learning relies on pattern matching (e.g., K-Nearest Neighbors), while model-based learning creates mathematical models (e.g., Linear Regression) to make predictions.\nThis is all about Machine Learning techniques. If you learned something, let me know in the comments. Your suggestions will help me improve my blogs.\nThanks for reading!\n\n"}, {'content': '\nBlogging\n\nMastering the End-to-End Machine Learning Lifecycle: From Data to Deployment\n\nNovember 24, 2024\n This article takes you through the complete lifecycle of a Machine Learning project. From ETL to deployment, I’ll share every detail of how I brought this project to life.\nData Engineering\nData Engineering serves as the starting point in the Machine Learning project lifecycle, bringing all distributed data together in one place.\nFor this project, I utilized an ETL pipeline—a core concept in Data Engineering. It enabled me to extract raw data, transform it into a meaningful format, and load it into a suitable location for further processing.\nLet\'s deep dive into ETL pipeline\nETL stands for Extract, Transform, and Load. These pipelines can be executed periodically (e.g., daily or hourly) to fetch real-time data, enhancing the predictive power of our Machine Learning model for real-world applications.\nThis process can be automated using tools like Apache Airflow, Kubeflow Pipelines, AWS Step Functions, and more, streamlining the workflow for consistent and efficient data updates.\n\nETL Pipeline:\nETL Pipeline\nExtract Operation\nThe extract operation is responsible for fetching data from various sources such as websites, databases, and APIs, as illustrated in the flowchart above.\n\nTransform Operation\nThe transform operation focuses on data cleaning, feature selection, and manipulation. In my project, I used this step to extract only the required features, ensuring the data is ready for the next phase.\n\nLoad Operation\nThe load operation transfers the transformed data to its destination, where we can choose the appropriate storage format. Typically, the processed data is stored in a database for further analysis and model training.\n\nFor my project, I used PySpark to build the ETL pipeline, as it enables efficient processing of large datasets.\nWhy not Pandas?\nWhile Pandas is excellent for small to medium-sized datasets, it stores DataFrames in RAM, which can lead to out-of-memory exceptions when handling large datasets.\nIn contrast, PySpark creates a session and processes data in chunks, storing it in ROM, making it ideal for handling large-scale data.\nFor the actual implementation of the pipeline using PySpark, please refer to the accompanying jupyter notebook.\nEDA (Data Analysis)\nData understanding is a critical stage before building any Machine Learning model. It allows us to analyze the data, plan the data processing steps, and gain insights into its structure and quality.\nIn this project, I examined the balance of data in my training and testing datasets and found it to be well-distributed across all four categories.\n\nBalanceness Checking on train and test data\nThroughout the entire process, I focused on two columns:\nTweets\nSentiments\nThe Tweets column contains the raw Twitter text data, while the Sentiments column serves as the target variable for prediction.\nNatural Language Processing\nI applied several key NLP techniques to preprocess the data and prepare it for Machine Learning model building. Below are the main steps I executed:\n\nText Processing\nRegex: I applied regular expressions to clean the text by removing URLs, hashtags, HTML tags, and keeping only alphanumeric characters. This helped eliminate unnecessary noise from the data.\nNLTK: Using the Natural Language Toolkit (NLTK), I performed word tokenization, stemming and lemmatization.\nWord tokenization is just splitting sentence into words, so we can processing each word from sentence individually.\nStemming helps us to truncate prefix or suffix of text to reduce count on unique words from corpus (paragraph).\nLemmatization is the proecss of converting any word into it\'s base word, for eg. Played will convert to play.\n\nWordCloud \nWord cloud concept help you to understand importance of words from given data. I splitted my data into four sections.\ndata for negative sentiments\ndata for positive sentiments\ndata for neutral sentiments\ndata for irrelevant sentiments\n\nHere is the representation of most frequent words for each category.\nCategorywise Word Cloud Presentation\nHere you can clearly see that there is no much difference in negative and positive sentiments data.\nthis is representation of bad data, here we can filterout our data for further processing, it might reduce data but you can do data augmentation techniques here to increase your data.\naugmenting of data in NLP with TF-IDF will not bad idea because TF-IDF and any other porcessing technique that I used is not able to detect sentence grammer or it doesn,t require sophisticated text. \nYou can think our input text will work as bag of word for model. you can understand by refering below vectorization technique.\nVectorization with TF-IDF \nI used Term Frequency-Inverse Document Frequency (TF-IDF) for text vectorization. This method transforms text into a numerical format, considering the importance of each word across documents, which prevents frequent words from dominating the model.\n\nTF-IDF\nTF-IDF stands for Term Frequency-Inverse Document Frequency. \nTerm Frequency (TF): This measures how frequently a term (t) appears in a specific document (d). It\'s calculated by dividing the number of occurrences of the term in the document by the total number of terms in that document.\nInverse Document Frequency (IDF): This measures how important a term is across the entire collection of documents. It\'s calculated by taking the logarithm of the ratio of the total number of documents (N) to the number of documents containing the term (df(t)). A higher IDF value indicates a rarer term, making it more significant.\nHere is the resulant data we got from TF-IDF\nAfter all preprocessing of text it\'s time to save our data for mlflow experiments. To see actual implementation of end to end process of NLP you can refer jupyter notebook.\nmlflow experiments\nMLflow experiments allow you to conduct multiple experiments with your trained model, helping you track and compare results over time. For running MLflow experiments, I prefer using Databricks as it offers an integrated experiment section, making the process much more streamlined and efficient.\nIn Databricks, you can easily connect your experiment by passing the experiment ID into the MLflow code. This integration simplifies the entire workflow, enabling better experiment tracking and easier comparison of model performance.\nFor my experiments, I worked with several models, including Logistic Regression, Multinomial Naive Bayes, and Decision Tree Classifier. By applying different combinations of parameters, I was able to experiment with and compare the performance of each model. Here\'s a preview of the Logistic Regression model’s F1 score, which highlights the model\'s ability to balance precision and recall:\nLogistic Regression F1 Score: 0.58\nThis approach allowed me to track the effectiveness of each model and make adjustments as needed for improving performance.\n\nF1 score with different parameters (Logistic Regression)\nI recommend running the notebook below in your own account to see the results. You\'ll definitely start appreciating the power of MLflow.\nFor more information, please refer to the accompanying Databricks notebook.\n\nHyperparameter tuning\nHyperparameter tuning is crucial for identifying the best parameter combination for your model. This technique involves an iterative process where, for every parameter combination, the model is trained and tested on a dataset. It is often described as a "trial and error" method.\nHowever, this approach can be computationally expensive, especially when working with complex or heavy machine learning models. For large-scale problems, hyperparameter tuning can be made more efficient through sampling or batch methods. In these methods, you don\'t use the entire dataset for training the model; instead, you choose random or stratified data points from the dataset to train the model. Although this may slightly reduce accuracy, it is more feasible when working with large datasets.\nFor my project, I used Grid Search CV to find the best hyperparameter combination for the model. Below are some common techniques for hyperparameter tuning, especially for large datasets:\nGrid Search This technique exhaustively searches through a specified set of hyperparameter values, trying all possible combinations. While effective, it can be computationally expensive for larger datasets due to the exhaustive nature of the search.\nRandom Search Randomly samples from the hyperparameter space, offering a faster alternative to grid search. This method explores a wider range of hyperparameters with fewer evaluations, making it more efficient for larger datasets.\nBayesian Optimization This method uses probabilistic models to predict the performance of different hyperparameters. It selects the next set of hyperparameters to evaluate based on previous results, making it more efficient and suitable for large datasets.\nGenetic Algorithms Inspired by natural selection, these algorithms iteratively evolve a population of hyperparameter sets to improve model performance. This method works well with complex search spaces.\nHyperband Hyperband combines random search with early stopping to dynamically allocate resources across multiple configurations, identifying promising hyperparameters quickly without excessive computational costs.\nBayesian Optimization with Gaussian Processes This technique models the hyperparameter search space using Gaussian processes, focusing on regions that are likely to yield better results, which is particularly useful for large datasets where computational resources are limited.\n\nTo optimize hyperparameter tuning for larger datasets, these techniques can be combined with parallel computing and distributed processing frameworks such as Dask, Spark, or multi-GPU setups. This enables more efficient hyperparameter search and reduces the overall computational overhead.\nI choosed Logistc Regression model and trained my Model with best parameters.\nSource Distribution for Model Packaging\nPackaging your machine learning model is a best practice, especially if you don’t plan to update it frequently. Imagine thousands of lines of code that can now be utilized with just a single line—this is the power of model packaging.\n\n1. Project Folder Setup\nTo ensure better organization, I created a main folder called sentiment_prediction and moved all machine learning pipeline files and dependencies into this folder. This helped in maintaining a clean structure and simplified the management of the entire project.\nBefore moving forward I recommend you to visit this pdf it will practically show you step by step process for building python package.\nPDF : step by step guid for python package building\n\n2. Manifest.in\nThe Manifest.in file plays a crucial role in controlling which files and folders should be included or excluded during the packaging process. It helps to specify the structure of the package for distribution.\nKey commands used in the Manifest.in file include:\ninclude <file/folder>: Include specific files or folders.\nexclude <file/folder>: Exclude specific files or folders.\nrecursive-include <path>: Include all files from a directory recursively.\nrecursive-exclude <path>: Exclude all files from a directory recursively.\n3. Setup.py\nThe setup.py file contains the project\'s metadata and is essential for creating the package. It defines key information about the project, such as:\nProject name, version, description, and author details.\nDependencies required for the package (install_requires), making it easy to install all necessary libraries.\n\n4. Building the Package\nTo build the package, I used the following command:\n\npython setup.py sdist bdist_wheel\nThis command generates two folders:\n\nbuild/: Contains the entire project package as defined in the Manifest.in.\ndist/: Contains the distributable files: .whl (wheel file) .gz (compressed source archive)\n\n5. Global Access via GitHub\nNow you can access your package gloablly, by refering your repository. I provided my package below go and check out.\n\nRepository: GitHub Repo Link\nYou can install the package directly from GitHub using the following command:\n\npip install git+https://github.com/vijaytakbhate2002/sentiment_prediction_python_package.git \nTo ensure it worked globally, I tested it again:\n\nfrom sentiment_prediction import predict\nprint(predict.predictor("Great progress shared today!")) \noutput:[\'Negative\']\nFlask Application \nBuilding application will help us to give our NLP model experience to people, so I built one flask application.\nWeb Application UI\n\nHere is demonstration of project: project demo\nI left a blank section for user suggestion and feedback, these feedbacks are getting stored in database for future model analysis or any business work.\nDatabase configuration\nFor storing collected user data we need to configure a database. It will help us to improve model as per user need.\nI used Google Cloud MySQL instance for integrating my application with database, GCP is paid but you can use free credit of GCP for first 3 months, for doing almost all Cloud Work.\nYou need to create your GCP account, then create one instance under SQL and by configuring your local system with instance you are good to go.\nDocker containerization\nIf you are not awared about docker and it\'s basic concepts you can refer my previous article which explains you all about docker.\nThis guide will help you build a solid foundation in Docker, enabling you to confidently use it for your projects.\nDocker guide: Comprehensive Docker guide for deploying Flas app\nDeploy\nDeployment of web app will help us to engange people and provide them real actual experience of our services.\nAfter deployment you need to collect user data and store it for future analysis, this data contain user feedback and suggestions.\nAfter deployment it\'s not end of the process we need to collect user feedback and again follow same steps fine tune Model as per user need.\n\nSummary of Blog\nThis blog covers the lifecycle of a Machine Learning project, from ETL to deployment. It details building an ETL pipeline using PySpark for efficient data handling, EDA, and NLP preprocessing techniques like tokenization, TF-IDF, and WordCloud visualization. \nIt highlights ML experiments with MLflow on Databricks and hyperparameter tuning using Grid Search.\nThe model was packaged into a Python package and deployed as a Flask application with a database backend (Google Cloud MySQL) and Dockerized for scalability. \nThe app collects user feedback for continuous improvement. It emphasizes end-to-end integration, including cloud and containerization, to deliver a robust ML solution.\nHappy Learning!\n\n'}, {'content': '\nLanguages I Speak\nEnglish, Marathi, Hindi\n\n'}, {'content': '\n\nSoft skills:\nCritical Thinking, Intellectual Rigor, Problem Solving, Understanding Business Needs'}]
2024-11-25 21:48:12 - root - INFO - Document store already contains 14 documents. Skipping write.
2024-11-25 21:48:12 - haystack.modeling.utils - INFO - Using devices: CPU - Number of GPUs: 0
2024-11-25 21:48:12 - haystack.modeling.utils - INFO - Using devices: CPU - Number of GPUs: 0
2024-11-25 21:48:12 - haystack.modeling.model.language_model - INFO -  * LOADING MODEL: 'deepset/roberta-base-squad2' (Roberta)
2024-11-25 21:48:13 - haystack.modeling.model.language_model - INFO - Auto-detected model language: english
2024-11-25 21:48:13 - haystack.modeling.model.language_model - INFO - Loaded 'deepset/roberta-base-squad2' (Roberta model) from model hub.
2024-11-25 21:48:14 - haystack.modeling.utils - INFO - Using devices: CPU - Number of GPUs: 0
2024-11-25 21:48:14 - haystack.modeling.data_handler.processor - ERROR - There were 1 errors during preprocessing at positions: {0}
2024-11-25 21:48:20 - root - INFO - Loading existing FAISS document store...
2024-11-25 21:48:20 - haystack.modeling.utils - INFO - Using devices: CPU - Number of GPUs: 0
2024-11-25 21:48:20 - haystack.modeling.model.language_model - WARNING - Using a model of type 'mpnet' which might be incompatible with DPR encoders. Only Bert-based encoders are supported. They need input_ids, token_type_ids, attention_mask as input tensors.
2024-11-25 21:48:22 - haystack.modeling.model.language_model - INFO - Auto-detected model language: english
2024-11-25 21:48:24 - haystack.modeling.model.language_model - WARNING - Using a model of type 'mpnet' which might be incompatible with DPR encoders. Only Bert-based encoders are supported. They need input_ids, token_type_ids, attention_mask as input tensors.
2024-11-25 21:48:25 - haystack.modeling.model.language_model - INFO - Auto-detected model language: english
2024-11-25 21:48:25 - root - INFO - Reading text document
2024-11-25 21:48:25 - root - INFO - loaded document = [{'content': '\nPersonal Information\n\nName: Vijay Takbhate\nEmail: vijaytakbhate20@gmail.com Phone: 8767363681\nGitHub: https://github.com/vijaytakbhate2002\nLinkedIn: https://www.linkedin.com/in/vijay-takbhate-b9231a236/ Kaggle: https://www.kaggle.com/vijay20213\n\n'}, {'content': '\n\nExperience - Fox Solutions Pvt. Ltd.\nRole: Automation Engineer Duration: Jun 2024 - Oct 2024 Location: Maharashtra\nKey Contributions:\n-Completed 2 months of internship plus 4 months of full-time work.\n-Worked with PLC and SCADA systems, focusing on automating processes and optimizing operational efficiency.\n-Collaborated with cross-functional teams to implement automation solutions for industrial applications.\n\n'}, {'content': '\n\nExperience - Cei Design Consultancy Pvt. Ltd.\nRole: Python Developer Intern Duration: Aug 2024 - Sept 2024 Location: Remote, Maharashtra\nKey Contributions:\n-Specialized in data processing using Python and Excel.\n-Utilized OpenCV for image processing tasks.\n\n'}, {'content': '\n\nExperience - Ujucode\nRole: Subject Matter Expert Intern Duration: Aug 2023 - Oct 2023 Location: Remote, Maharashtra\nKey Contributions:\n-Contributed as a Python developer for a ChatBot project.\n-Handled backend development tasks and researched Python modules.\n\n'}, {'content': '\n\nProject - Twitter Post Sentiment Prediction\nDetails:\n-Engineered an ETL pipeline using PySpark and SQL.\n-Conducted sentiment analysis using NLP (TF-IDF) and optimized hyperparameters.\n-Monitored model performance through MLFlow on Databricks.\n-Leveraged Google Cloud Storage and MySQL for data management.\n-Deployed the model using Docker and hosted it on Render.\n\n'}, {'content': '\n\nProject - Text-Text Chat-Bot\nDetails:\n-Designed an advanced Chat-Bot using the NVIDIA API and prompt engineering.\n-Features include paraphrasing, grammar correction, AI detection, plagiarism checking, and content summarization.\n-Targeted at content creators, researchers, and businesses.\n-Technologies Used: HTML, CSS, Python Flask, Cloud Database, and Render.\n\n'}, {'content': '\n\nProject - Hand Gesture Recognition\nDetails:\n-Used Google-s MediaPipe framework for detecting hand landmarks and gestures.\n-Created and labeled a custom dataset of hand gestures for training.\n-Developed a Streamlit application to improve accessibility and flexibility.\n\n'}, {'content': '\n\nTechnical Skills\nLanguages: MySQL, Python, HTML, CSS\nTechnologies: Streamlit, Flask, VS Code, GitHub, MLflow, Docker, PySpark, Databricks, Google Cloud Platform\n\n'}, {'content': '\n\nCertification\nMLOps Bootcamp: Mastering AI Operations for Success (Jun 2024)\n-Learned about the MLOps lifecycle and modular programming.\n-Acquired skills in Git, Python, Flask, and MLflow.\n\n'}, {'content': '\n\nEducation Details:\nBachelor of Technology in Electronics and Telecommunication (May 2024) Institution: SVERI-s College of Engineering, Pandharpur, Maharashtra Score: 81.71\nDiploma in Electronics and Telecommunication (May 2021) Institution: SVERI-s College of Engineering, Pandharpur, Maharashtra Score: 91.73\n\n'}, {'content': "\n\nBlogging\n\n\nSupervised, Unsupervised, and Beyond: ML Techniques Simplified\nNovember 25, 2024\nThere are several techniques for ML training. Among these, I will cover the following:\n\nSupervised and Unsupervised Learning\nSemi-Supervised Learning\nOffline and Online Learning\nInstance-Based and Model-Based Learning\n\nSupervised and Unsupervised Learning\nSupervised learning is like spoon-feeding our ML model in its initial stages, allowing it to learn and improve over time. Here, I’m referring to the training process.\n\nIn supervised learning, there are input columns and output columns, also called target columns. For example, in spam detection—a classification problem—the input is the email, and the target is whether the email is spam or not. That’s it!\nThis process resembles a student-teacher scenario where the teacher is a human, and the student is the model. The dataset serves as the knowledge used to train the student (model)\nSee content credentials\n\nhumand and model\nIn Unsupervised learning, we are not aware of the data labels. Instead, we separate the input data by analyzing similarities and grouping them into different clusters.\nOnce the clusters are formed, we can assign custom labels to each one. This technique is widely used to identify product relationships in online shopping and to recommend new products based on a user’s purchase history.\n\nUnsupervised Learning Clusters\nHere three clusters with three different image categories are formed\n\nSemi-Supervised Learning\nSemi-Supervised Learning is a combination of supervised and unsupervised learning, where some data is labeled and some data is unlabeled. A good example of this is Google Photos, which automatically separates new photos into their respective groups based on whether they contain a particular person in each image.\n\nThere are several techniques under semi-supervised learning; here, we will focus on the following:\nSelf Learning\nConsistency Regularization\nGenerative Models\nGraph-Based Learning\n\nLet's discuss them one by one:\n\nSelf Learning\nSelf Learning trains a model with labeled data and generates pseudo-labels for the unlabeled data. \n\nSelf learning\nThen, the model is trained on the entire dataset, which includes both the generated pseudo-labels and the labeled data.\n\nConsistency Regularization\nThis technique uses data augmentation to generate similar data, and then the model is enforced to predict the same outcome for both the augmented and original data. It helps create a model that can find similarities in both labeled and unlabeled data, predicting the same output for unlabeled data as it would for labeled data.\n\nData augmentation includes techniques like image flipping, blurring, rotation, etc. After augmenting the data, the model is trained to predict the same class for both the augmented and original images.\n\nGenerative Models\nGenerative models create synthetic data points and learn the underlying structure of the training data. These models can generate new datasets using encoders. \n\nExamples include Generative Adversarial Networks (GANs) and Variational Autoencoders (VAEs).\n\nGraph-Based Learning\nThis technique creates nodes for both labeled and unlabeled data. Similar nodes are then connected to each other.\nFor example, when dealing with images, each image starts as a separate node. If similar images are found based on their features, they are grouped or connected by an edge.\nRefer below graph to understand graph-based learning\nGraph based learning\nHere label 1 is came from labeled image of group 1, simillarly for label 2 and label 3. Finally we came up with three labels that means labeled data has three unique labels.\nBatch (Offline) and Online Learning\nBatch Learning\nBatch learning, also known as offline learning, is a technique where the developer needs to stop the deployed ML model, retrain it on new data, and then redeploy it. \nThis technique is useful when frequent updates are not required. For example, product recommendation systems can often be retrained weekly or even over several days.\nBatch learning can be costly when training the model on the entire dataset every time. However, using incremental learning models can help avoid unnecessary retraining, making the process more efficient.\nHere are some examples on incremental learning models, as of now we are focusing on general concepts so don't go much deeper into these types.\nStochastic Gradient Descent (SGD)\nNaive Bayes (Online Version)\nIncremental Support Vector Machines (ISVM)\nOnline Random Forests\nIncremental Decision Trees\nYou can refer this article to learn more about incremental learning\nHere’s your text with grammatical corrections and improved clarity:\nOnline Learning\nOnline learning is used when an ML model needs to be updated continuously with new inputs, such as in stock price prediction. In this scenario, the model must remain aware of the most recent data.\n\nBatch and online learning\nOnly models capable of incremental learning are used in online learning. Each new instance is fed back into the model to update its internal weights based on the latest input.\n\nInstance-Based and Model-Based Learning\nInstance-Based Learning\nInstance-based learning involves comparing a new input with stored data. If similarity is found, the model returns the label of the corresponding input. This approach uses pattern matching techniques. \nHere model works as a search engin not exactly same but it will find simmilarity from stored data.\nSome common models used in instance-based learning include:\nK-Nearest Neighbors (K-NN) with labels\nLocally Weighted Regression, etc.\nModel-Based Learning\nIn model-based learning, we train the model using a training dataset. During the training process, the model creates its own complex patterns (mathematical equations) to make predictions for future inputs. \nA simple example is the equation y=mx+cy = mx + c, which represents a trained model. Some common models used in model-based learning include:\nLinear Regression\nLogistic Regression\nDecision Tree, etc.\n\nSummary\nMachine Learning techniques can be broadly categorized into supervised, unsupervised, and semi-supervised learning. \nSupervised learning uses labeled data, like spam detection, while unsupervised learning works with unlabeled data, grouping similar points into clusters. \nSemi-supervised learning combines both, employing techniques like self-learning and consistency regularization to leverage partially labeled datasets.\nOther approaches include batch learning, where models are retrained periodically, and online learning, which updates models continuously with new data, suitable for dynamic tasks like stock price prediction. \nInstance-based learning relies on pattern matching (e.g., K-Nearest Neighbors), while model-based learning creates mathematical models (e.g., Linear Regression) to make predictions.\nThis is all about Machine Learning techniques. If you learned something, let me know in the comments. Your suggestions will help me improve my blogs.\nThanks for reading!\n\n"}, {'content': '\nBlogging\n\nMastering the End-to-End Machine Learning Lifecycle: From Data to Deployment\n\nNovember 24, 2024\n This article takes you through the complete lifecycle of a Machine Learning project. From ETL to deployment, I’ll share every detail of how I brought this project to life.\nData Engineering\nData Engineering serves as the starting point in the Machine Learning project lifecycle, bringing all distributed data together in one place.\nFor this project, I utilized an ETL pipeline—a core concept in Data Engineering. It enabled me to extract raw data, transform it into a meaningful format, and load it into a suitable location for further processing.\nLet\'s deep dive into ETL pipeline\nETL stands for Extract, Transform, and Load. These pipelines can be executed periodically (e.g., daily or hourly) to fetch real-time data, enhancing the predictive power of our Machine Learning model for real-world applications.\nThis process can be automated using tools like Apache Airflow, Kubeflow Pipelines, AWS Step Functions, and more, streamlining the workflow for consistent and efficient data updates.\n\nETL Pipeline:\nETL Pipeline\nExtract Operation\nThe extract operation is responsible for fetching data from various sources such as websites, databases, and APIs, as illustrated in the flowchart above.\n\nTransform Operation\nThe transform operation focuses on data cleaning, feature selection, and manipulation. In my project, I used this step to extract only the required features, ensuring the data is ready for the next phase.\n\nLoad Operation\nThe load operation transfers the transformed data to its destination, where we can choose the appropriate storage format. Typically, the processed data is stored in a database for further analysis and model training.\n\nFor my project, I used PySpark to build the ETL pipeline, as it enables efficient processing of large datasets.\nWhy not Pandas?\nWhile Pandas is excellent for small to medium-sized datasets, it stores DataFrames in RAM, which can lead to out-of-memory exceptions when handling large datasets.\nIn contrast, PySpark creates a session and processes data in chunks, storing it in ROM, making it ideal for handling large-scale data.\nFor the actual implementation of the pipeline using PySpark, please refer to the accompanying jupyter notebook.\nEDA (Data Analysis)\nData understanding is a critical stage before building any Machine Learning model. It allows us to analyze the data, plan the data processing steps, and gain insights into its structure and quality.\nIn this project, I examined the balance of data in my training and testing datasets and found it to be well-distributed across all four categories.\n\nBalanceness Checking on train and test data\nThroughout the entire process, I focused on two columns:\nTweets\nSentiments\nThe Tweets column contains the raw Twitter text data, while the Sentiments column serves as the target variable for prediction.\nNatural Language Processing\nI applied several key NLP techniques to preprocess the data and prepare it for Machine Learning model building. Below are the main steps I executed:\n\nText Processing\nRegex: I applied regular expressions to clean the text by removing URLs, hashtags, HTML tags, and keeping only alphanumeric characters. This helped eliminate unnecessary noise from the data.\nNLTK: Using the Natural Language Toolkit (NLTK), I performed word tokenization, stemming and lemmatization.\nWord tokenization is just splitting sentence into words, so we can processing each word from sentence individually.\nStemming helps us to truncate prefix or suffix of text to reduce count on unique words from corpus (paragraph).\nLemmatization is the proecss of converting any word into it\'s base word, for eg. Played will convert to play.\n\nWordCloud \nWord cloud concept help you to understand importance of words from given data. I splitted my data into four sections.\ndata for negative sentiments\ndata for positive sentiments\ndata for neutral sentiments\ndata for irrelevant sentiments\n\nHere is the representation of most frequent words for each category.\nCategorywise Word Cloud Presentation\nHere you can clearly see that there is no much difference in negative and positive sentiments data.\nthis is representation of bad data, here we can filterout our data for further processing, it might reduce data but you can do data augmentation techniques here to increase your data.\naugmenting of data in NLP with TF-IDF will not bad idea because TF-IDF and any other porcessing technique that I used is not able to detect sentence grammer or it doesn,t require sophisticated text. \nYou can think our input text will work as bag of word for model. you can understand by refering below vectorization technique.\nVectorization with TF-IDF \nI used Term Frequency-Inverse Document Frequency (TF-IDF) for text vectorization. This method transforms text into a numerical format, considering the importance of each word across documents, which prevents frequent words from dominating the model.\n\nTF-IDF\nTF-IDF stands for Term Frequency-Inverse Document Frequency. \nTerm Frequency (TF): This measures how frequently a term (t) appears in a specific document (d). It\'s calculated by dividing the number of occurrences of the term in the document by the total number of terms in that document.\nInverse Document Frequency (IDF): This measures how important a term is across the entire collection of documents. It\'s calculated by taking the logarithm of the ratio of the total number of documents (N) to the number of documents containing the term (df(t)). A higher IDF value indicates a rarer term, making it more significant.\nHere is the resulant data we got from TF-IDF\nAfter all preprocessing of text it\'s time to save our data for mlflow experiments. To see actual implementation of end to end process of NLP you can refer jupyter notebook.\nmlflow experiments\nMLflow experiments allow you to conduct multiple experiments with your trained model, helping you track and compare results over time. For running MLflow experiments, I prefer using Databricks as it offers an integrated experiment section, making the process much more streamlined and efficient.\nIn Databricks, you can easily connect your experiment by passing the experiment ID into the MLflow code. This integration simplifies the entire workflow, enabling better experiment tracking and easier comparison of model performance.\nFor my experiments, I worked with several models, including Logistic Regression, Multinomial Naive Bayes, and Decision Tree Classifier. By applying different combinations of parameters, I was able to experiment with and compare the performance of each model. Here\'s a preview of the Logistic Regression model’s F1 score, which highlights the model\'s ability to balance precision and recall:\nLogistic Regression F1 Score: 0.58\nThis approach allowed me to track the effectiveness of each model and make adjustments as needed for improving performance.\n\nF1 score with different parameters (Logistic Regression)\nI recommend running the notebook below in your own account to see the results. You\'ll definitely start appreciating the power of MLflow.\nFor more information, please refer to the accompanying Databricks notebook.\n\nHyperparameter tuning\nHyperparameter tuning is crucial for identifying the best parameter combination for your model. This technique involves an iterative process where, for every parameter combination, the model is trained and tested on a dataset. It is often described as a "trial and error" method.\nHowever, this approach can be computationally expensive, especially when working with complex or heavy machine learning models. For large-scale problems, hyperparameter tuning can be made more efficient through sampling or batch methods. In these methods, you don\'t use the entire dataset for training the model; instead, you choose random or stratified data points from the dataset to train the model. Although this may slightly reduce accuracy, it is more feasible when working with large datasets.\nFor my project, I used Grid Search CV to find the best hyperparameter combination for the model. Below are some common techniques for hyperparameter tuning, especially for large datasets:\nGrid Search This technique exhaustively searches through a specified set of hyperparameter values, trying all possible combinations. While effective, it can be computationally expensive for larger datasets due to the exhaustive nature of the search.\nRandom Search Randomly samples from the hyperparameter space, offering a faster alternative to grid search. This method explores a wider range of hyperparameters with fewer evaluations, making it more efficient for larger datasets.\nBayesian Optimization This method uses probabilistic models to predict the performance of different hyperparameters. It selects the next set of hyperparameters to evaluate based on previous results, making it more efficient and suitable for large datasets.\nGenetic Algorithms Inspired by natural selection, these algorithms iteratively evolve a population of hyperparameter sets to improve model performance. This method works well with complex search spaces.\nHyperband Hyperband combines random search with early stopping to dynamically allocate resources across multiple configurations, identifying promising hyperparameters quickly without excessive computational costs.\nBayesian Optimization with Gaussian Processes This technique models the hyperparameter search space using Gaussian processes, focusing on regions that are likely to yield better results, which is particularly useful for large datasets where computational resources are limited.\n\nTo optimize hyperparameter tuning for larger datasets, these techniques can be combined with parallel computing and distributed processing frameworks such as Dask, Spark, or multi-GPU setups. This enables more efficient hyperparameter search and reduces the overall computational overhead.\nI choosed Logistc Regression model and trained my Model with best parameters.\nSource Distribution for Model Packaging\nPackaging your machine learning model is a best practice, especially if you don’t plan to update it frequently. Imagine thousands of lines of code that can now be utilized with just a single line—this is the power of model packaging.\n\n1. Project Folder Setup\nTo ensure better organization, I created a main folder called sentiment_prediction and moved all machine learning pipeline files and dependencies into this folder. This helped in maintaining a clean structure and simplified the management of the entire project.\nBefore moving forward I recommend you to visit this pdf it will practically show you step by step process for building python package.\nPDF : step by step guid for python package building\n\n2. Manifest.in\nThe Manifest.in file plays a crucial role in controlling which files and folders should be included or excluded during the packaging process. It helps to specify the structure of the package for distribution.\nKey commands used in the Manifest.in file include:\ninclude <file/folder>: Include specific files or folders.\nexclude <file/folder>: Exclude specific files or folders.\nrecursive-include <path>: Include all files from a directory recursively.\nrecursive-exclude <path>: Exclude all files from a directory recursively.\n3. Setup.py\nThe setup.py file contains the project\'s metadata and is essential for creating the package. It defines key information about the project, such as:\nProject name, version, description, and author details.\nDependencies required for the package (install_requires), making it easy to install all necessary libraries.\n\n4. Building the Package\nTo build the package, I used the following command:\n\npython setup.py sdist bdist_wheel\nThis command generates two folders:\n\nbuild/: Contains the entire project package as defined in the Manifest.in.\ndist/: Contains the distributable files: .whl (wheel file) .gz (compressed source archive)\n\n5. Global Access via GitHub\nNow you can access your package gloablly, by refering your repository. I provided my package below go and check out.\n\nRepository: GitHub Repo Link\nYou can install the package directly from GitHub using the following command:\n\npip install git+https://github.com/vijaytakbhate2002/sentiment_prediction_python_package.git \nTo ensure it worked globally, I tested it again:\n\nfrom sentiment_prediction import predict\nprint(predict.predictor("Great progress shared today!")) \noutput:[\'Negative\']\nFlask Application \nBuilding application will help us to give our NLP model experience to people, so I built one flask application.\nWeb Application UI\n\nHere is demonstration of project: project demo\nI left a blank section for user suggestion and feedback, these feedbacks are getting stored in database for future model analysis or any business work.\nDatabase configuration\nFor storing collected user data we need to configure a database. It will help us to improve model as per user need.\nI used Google Cloud MySQL instance for integrating my application with database, GCP is paid but you can use free credit of GCP for first 3 months, for doing almost all Cloud Work.\nYou need to create your GCP account, then create one instance under SQL and by configuring your local system with instance you are good to go.\nDocker containerization\nIf you are not awared about docker and it\'s basic concepts you can refer my previous article which explains you all about docker.\nThis guide will help you build a solid foundation in Docker, enabling you to confidently use it for your projects.\nDocker guide: Comprehensive Docker guide for deploying Flas app\nDeploy\nDeployment of web app will help us to engange people and provide them real actual experience of our services.\nAfter deployment you need to collect user data and store it for future analysis, this data contain user feedback and suggestions.\nAfter deployment it\'s not end of the process we need to collect user feedback and again follow same steps fine tune Model as per user need.\n\nSummary of Blog\nThis blog covers the lifecycle of a Machine Learning project, from ETL to deployment. It details building an ETL pipeline using PySpark for efficient data handling, EDA, and NLP preprocessing techniques like tokenization, TF-IDF, and WordCloud visualization. \nIt highlights ML experiments with MLflow on Databricks and hyperparameter tuning using Grid Search.\nThe model was packaged into a Python package and deployed as a Flask application with a database backend (Google Cloud MySQL) and Dockerized for scalability. \nThe app collects user feedback for continuous improvement. It emphasizes end-to-end integration, including cloud and containerization, to deliver a robust ML solution.\nHappy Learning!\n\n'}, {'content': '\nLanguages I Speak\nEnglish, Marathi, Hindi\n\n'}, {'content': '\n\nSoft skills:\nCritical Thinking, Intellectual Rigor, Problem Solving, Understanding Business Needs'}]
2024-11-25 21:48:25 - root - INFO - Document store already contains 14 documents. Skipping write.
2024-11-25 21:48:25 - haystack.modeling.utils - INFO - Using devices: CPU - Number of GPUs: 0
2024-11-25 21:48:25 - haystack.modeling.utils - INFO - Using devices: CPU - Number of GPUs: 0
2024-11-25 21:48:26 - haystack.modeling.model.language_model - INFO -  * LOADING MODEL: 'deepset/roberta-base-squad2' (Roberta)
2024-11-25 21:48:26 - haystack.modeling.model.language_model - INFO - Auto-detected model language: english
2024-11-25 21:48:26 - haystack.modeling.model.language_model - INFO - Loaded 'deepset/roberta-base-squad2' (Roberta model) from model hub.
2024-11-25 21:48:27 - haystack.modeling.utils - INFO - Using devices: CPU - Number of GPUs: 0
2024-11-25 21:48:50 - root - INFO - Loading existing FAISS document store...
2024-11-25 21:48:50 - haystack.modeling.utils - INFO - Using devices: CPU - Number of GPUs: 0
2024-11-25 21:48:51 - haystack.modeling.model.language_model - WARNING - Using a model of type 'mpnet' which might be incompatible with DPR encoders. Only Bert-based encoders are supported. They need input_ids, token_type_ids, attention_mask as input tensors.
2024-11-25 21:49:17 - haystack.modeling.model.language_model - INFO - Auto-detected model language: english
2024-11-25 21:49:19 - haystack.modeling.model.language_model - WARNING - Using a model of type 'mpnet' which might be incompatible with DPR encoders. Only Bert-based encoders are supported. They need input_ids, token_type_ids, attention_mask as input tensors.
2024-11-25 21:49:35 - haystack.modeling.model.language_model - INFO - Auto-detected model language: english
2024-11-25 21:49:35 - root - INFO - Reading text document
2024-11-25 21:49:35 - root - INFO - loaded document = [{'content': '\nPersonal Information\n\nName: Vijay Takbhate\nEmail: vijaytakbhate20@gmail.com Phone: 8767363681\nGitHub: https://github.com/vijaytakbhate2002\nLinkedIn: https://www.linkedin.com/in/vijay-takbhate-b9231a236/ Kaggle: https://www.kaggle.com/vijay20213\n\n'}, {'content': '\n\nExperience - Fox Solutions Pvt. Ltd.\nRole: Automation Engineer Duration: Jun 2024 - Oct 2024 Location: Maharashtra\nKey Contributions:\n-Completed 2 months of internship plus 4 months of full-time work.\n-Worked with PLC and SCADA systems, focusing on automating processes and optimizing operational efficiency.\n-Collaborated with cross-functional teams to implement automation solutions for industrial applications.\n\n'}, {'content': '\n\nExperience - Cei Design Consultancy Pvt. Ltd.\nRole: Python Developer Intern Duration: Aug 2024 - Sept 2024 Location: Remote, Maharashtra\nKey Contributions:\n-Specialized in data processing using Python and Excel.\n-Utilized OpenCV for image processing tasks.\n\n'}, {'content': '\n\nExperience - Ujucode\nRole: Subject Matter Expert Intern Duration: Aug 2023 - Oct 2023 Location: Remote, Maharashtra\nKey Contributions:\n-Contributed as a Python developer for a ChatBot project.\n-Handled backend development tasks and researched Python modules.\n\n'}, {'content': '\n\nProject - Twitter Post Sentiment Prediction\nDetails:\n-Engineered an ETL pipeline using PySpark and SQL.\n-Conducted sentiment analysis using NLP (TF-IDF) and optimized hyperparameters.\n-Monitored model performance through MLFlow on Databricks.\n-Leveraged Google Cloud Storage and MySQL for data management.\n-Deployed the model using Docker and hosted it on Render.\n\n'}, {'content': '\n\nProject - Text-Text Chat-Bot\nDetails:\n-Designed an advanced Chat-Bot using the NVIDIA API and prompt engineering.\n-Features include paraphrasing, grammar correction, AI detection, plagiarism checking, and content summarization.\n-Targeted at content creators, researchers, and businesses.\n-Technologies Used: HTML, CSS, Python Flask, Cloud Database, and Render.\n\n'}, {'content': '\n\nProject - Hand Gesture Recognition\nDetails:\n-Used Google-s MediaPipe framework for detecting hand landmarks and gestures.\n-Created and labeled a custom dataset of hand gestures for training.\n-Developed a Streamlit application to improve accessibility and flexibility.\n\n'}, {'content': '\n\nTechnical Skills\nLanguages: MySQL, Python, HTML, CSS\nTechnologies: Streamlit, Flask, VS Code, GitHub, MLflow, Docker, PySpark, Databricks, Google Cloud Platform\n\n'}, {'content': '\n\nCertification\nMLOps Bootcamp: Mastering AI Operations for Success (Jun 2024)\n-Learned about the MLOps lifecycle and modular programming.\n-Acquired skills in Git, Python, Flask, and MLflow.\n\n'}, {'content': '\n\nEducation Details:\nBachelor of Technology in Electronics and Telecommunication (May 2024) Institution: SVERI-s College of Engineering, Pandharpur, Maharashtra Score: 81.71\nDiploma in Electronics and Telecommunication (May 2021) Institution: SVERI-s College of Engineering, Pandharpur, Maharashtra Score: 91.73\n\n'}, {'content': "\n\nBlogging\n\n\nSupervised, Unsupervised, and Beyond: ML Techniques Simplified\nNovember 25, 2024\nThere are several techniques for ML training. Among these, I will cover the following:\n\nSupervised and Unsupervised Learning\nSemi-Supervised Learning\nOffline and Online Learning\nInstance-Based and Model-Based Learning\n\nSupervised and Unsupervised Learning\nSupervised learning is like spoon-feeding our ML model in its initial stages, allowing it to learn and improve over time. Here, I’m referring to the training process.\n\nIn supervised learning, there are input columns and output columns, also called target columns. For example, in spam detection—a classification problem—the input is the email, and the target is whether the email is spam or not. That’s it!\nThis process resembles a student-teacher scenario where the teacher is a human, and the student is the model. The dataset serves as the knowledge used to train the student (model)\nSee content credentials\n\nhumand and model\nIn Unsupervised learning, we are not aware of the data labels. Instead, we separate the input data by analyzing similarities and grouping them into different clusters.\nOnce the clusters are formed, we can assign custom labels to each one. This technique is widely used to identify product relationships in online shopping and to recommend new products based on a user’s purchase history.\n\nUnsupervised Learning Clusters\nHere three clusters with three different image categories are formed\n\nSemi-Supervised Learning\nSemi-Supervised Learning is a combination of supervised and unsupervised learning, where some data is labeled and some data is unlabeled. A good example of this is Google Photos, which automatically separates new photos into their respective groups based on whether they contain a particular person in each image.\n\nThere are several techniques under semi-supervised learning; here, we will focus on the following:\nSelf Learning\nConsistency Regularization\nGenerative Models\nGraph-Based Learning\n\nLet's discuss them one by one:\n\nSelf Learning\nSelf Learning trains a model with labeled data and generates pseudo-labels for the unlabeled data. \n\nSelf learning\nThen, the model is trained on the entire dataset, which includes both the generated pseudo-labels and the labeled data.\n\nConsistency Regularization\nThis technique uses data augmentation to generate similar data, and then the model is enforced to predict the same outcome for both the augmented and original data. It helps create a model that can find similarities in both labeled and unlabeled data, predicting the same output for unlabeled data as it would for labeled data.\n\nData augmentation includes techniques like image flipping, blurring, rotation, etc. After augmenting the data, the model is trained to predict the same class for both the augmented and original images.\n\nGenerative Models\nGenerative models create synthetic data points and learn the underlying structure of the training data. These models can generate new datasets using encoders. \n\nExamples include Generative Adversarial Networks (GANs) and Variational Autoencoders (VAEs).\n\nGraph-Based Learning\nThis technique creates nodes for both labeled and unlabeled data. Similar nodes are then connected to each other.\nFor example, when dealing with images, each image starts as a separate node. If similar images are found based on their features, they are grouped or connected by an edge.\nRefer below graph to understand graph-based learning\nGraph based learning\nHere label 1 is came from labeled image of group 1, simillarly for label 2 and label 3. Finally we came up with three labels that means labeled data has three unique labels.\nBatch (Offline) and Online Learning\nBatch Learning\nBatch learning, also known as offline learning, is a technique where the developer needs to stop the deployed ML model, retrain it on new data, and then redeploy it. \nThis technique is useful when frequent updates are not required. For example, product recommendation systems can often be retrained weekly or even over several days.\nBatch learning can be costly when training the model on the entire dataset every time. However, using incremental learning models can help avoid unnecessary retraining, making the process more efficient.\nHere are some examples on incremental learning models, as of now we are focusing on general concepts so don't go much deeper into these types.\nStochastic Gradient Descent (SGD)\nNaive Bayes (Online Version)\nIncremental Support Vector Machines (ISVM)\nOnline Random Forests\nIncremental Decision Trees\nYou can refer this article to learn more about incremental learning\nHere’s your text with grammatical corrections and improved clarity:\nOnline Learning\nOnline learning is used when an ML model needs to be updated continuously with new inputs, such as in stock price prediction. In this scenario, the model must remain aware of the most recent data.\n\nBatch and online learning\nOnly models capable of incremental learning are used in online learning. Each new instance is fed back into the model to update its internal weights based on the latest input.\n\nInstance-Based and Model-Based Learning\nInstance-Based Learning\nInstance-based learning involves comparing a new input with stored data. If similarity is found, the model returns the label of the corresponding input. This approach uses pattern matching techniques. \nHere model works as a search engin not exactly same but it will find simmilarity from stored data.\nSome common models used in instance-based learning include:\nK-Nearest Neighbors (K-NN) with labels\nLocally Weighted Regression, etc.\nModel-Based Learning\nIn model-based learning, we train the model using a training dataset. During the training process, the model creates its own complex patterns (mathematical equations) to make predictions for future inputs. \nA simple example is the equation y=mx+cy = mx + c, which represents a trained model. Some common models used in model-based learning include:\nLinear Regression\nLogistic Regression\nDecision Tree, etc.\n\nSummary\nMachine Learning techniques can be broadly categorized into supervised, unsupervised, and semi-supervised learning. \nSupervised learning uses labeled data, like spam detection, while unsupervised learning works with unlabeled data, grouping similar points into clusters. \nSemi-supervised learning combines both, employing techniques like self-learning and consistency regularization to leverage partially labeled datasets.\nOther approaches include batch learning, where models are retrained periodically, and online learning, which updates models continuously with new data, suitable for dynamic tasks like stock price prediction. \nInstance-based learning relies on pattern matching (e.g., K-Nearest Neighbors), while model-based learning creates mathematical models (e.g., Linear Regression) to make predictions.\nThis is all about Machine Learning techniques. If you learned something, let me know in the comments. Your suggestions will help me improve my blogs.\nThanks for reading!\n\n"}, {'content': '\nBlogging\n\nMastering the End-to-End Machine Learning Lifecycle: From Data to Deployment\n\nNovember 24, 2024\n This article takes you through the complete lifecycle of a Machine Learning project. From ETL to deployment, I’ll share every detail of how I brought this project to life.\nData Engineering\nData Engineering serves as the starting point in the Machine Learning project lifecycle, bringing all distributed data together in one place.\nFor this project, I utilized an ETL pipeline—a core concept in Data Engineering. It enabled me to extract raw data, transform it into a meaningful format, and load it into a suitable location for further processing.\nLet\'s deep dive into ETL pipeline\nETL stands for Extract, Transform, and Load. These pipelines can be executed periodically (e.g., daily or hourly) to fetch real-time data, enhancing the predictive power of our Machine Learning model for real-world applications.\nThis process can be automated using tools like Apache Airflow, Kubeflow Pipelines, AWS Step Functions, and more, streamlining the workflow for consistent and efficient data updates.\n\nETL Pipeline:\nETL Pipeline\nExtract Operation\nThe extract operation is responsible for fetching data from various sources such as websites, databases, and APIs, as illustrated in the flowchart above.\n\nTransform Operation\nThe transform operation focuses on data cleaning, feature selection, and manipulation. In my project, I used this step to extract only the required features, ensuring the data is ready for the next phase.\n\nLoad Operation\nThe load operation transfers the transformed data to its destination, where we can choose the appropriate storage format. Typically, the processed data is stored in a database for further analysis and model training.\n\nFor my project, I used PySpark to build the ETL pipeline, as it enables efficient processing of large datasets.\nWhy not Pandas?\nWhile Pandas is excellent for small to medium-sized datasets, it stores DataFrames in RAM, which can lead to out-of-memory exceptions when handling large datasets.\nIn contrast, PySpark creates a session and processes data in chunks, storing it in ROM, making it ideal for handling large-scale data.\nFor the actual implementation of the pipeline using PySpark, please refer to the accompanying jupyter notebook.\nEDA (Data Analysis)\nData understanding is a critical stage before building any Machine Learning model. It allows us to analyze the data, plan the data processing steps, and gain insights into its structure and quality.\nIn this project, I examined the balance of data in my training and testing datasets and found it to be well-distributed across all four categories.\n\nBalanceness Checking on train and test data\nThroughout the entire process, I focused on two columns:\nTweets\nSentiments\nThe Tweets column contains the raw Twitter text data, while the Sentiments column serves as the target variable for prediction.\nNatural Language Processing\nI applied several key NLP techniques to preprocess the data and prepare it for Machine Learning model building. Below are the main steps I executed:\n\nText Processing\nRegex: I applied regular expressions to clean the text by removing URLs, hashtags, HTML tags, and keeping only alphanumeric characters. This helped eliminate unnecessary noise from the data.\nNLTK: Using the Natural Language Toolkit (NLTK), I performed word tokenization, stemming and lemmatization.\nWord tokenization is just splitting sentence into words, so we can processing each word from sentence individually.\nStemming helps us to truncate prefix or suffix of text to reduce count on unique words from corpus (paragraph).\nLemmatization is the proecss of converting any word into it\'s base word, for eg. Played will convert to play.\n\nWordCloud \nWord cloud concept help you to understand importance of words from given data. I splitted my data into four sections.\ndata for negative sentiments\ndata for positive sentiments\ndata for neutral sentiments\ndata for irrelevant sentiments\n\nHere is the representation of most frequent words for each category.\nCategorywise Word Cloud Presentation\nHere you can clearly see that there is no much difference in negative and positive sentiments data.\nthis is representation of bad data, here we can filterout our data for further processing, it might reduce data but you can do data augmentation techniques here to increase your data.\naugmenting of data in NLP with TF-IDF will not bad idea because TF-IDF and any other porcessing technique that I used is not able to detect sentence grammer or it doesn,t require sophisticated text. \nYou can think our input text will work as bag of word for model. you can understand by refering below vectorization technique.\nVectorization with TF-IDF \nI used Term Frequency-Inverse Document Frequency (TF-IDF) for text vectorization. This method transforms text into a numerical format, considering the importance of each word across documents, which prevents frequent words from dominating the model.\n\nTF-IDF\nTF-IDF stands for Term Frequency-Inverse Document Frequency. \nTerm Frequency (TF): This measures how frequently a term (t) appears in a specific document (d). It\'s calculated by dividing the number of occurrences of the term in the document by the total number of terms in that document.\nInverse Document Frequency (IDF): This measures how important a term is across the entire collection of documents. It\'s calculated by taking the logarithm of the ratio of the total number of documents (N) to the number of documents containing the term (df(t)). A higher IDF value indicates a rarer term, making it more significant.\nHere is the resulant data we got from TF-IDF\nAfter all preprocessing of text it\'s time to save our data for mlflow experiments. To see actual implementation of end to end process of NLP you can refer jupyter notebook.\nmlflow experiments\nMLflow experiments allow you to conduct multiple experiments with your trained model, helping you track and compare results over time. For running MLflow experiments, I prefer using Databricks as it offers an integrated experiment section, making the process much more streamlined and efficient.\nIn Databricks, you can easily connect your experiment by passing the experiment ID into the MLflow code. This integration simplifies the entire workflow, enabling better experiment tracking and easier comparison of model performance.\nFor my experiments, I worked with several models, including Logistic Regression, Multinomial Naive Bayes, and Decision Tree Classifier. By applying different combinations of parameters, I was able to experiment with and compare the performance of each model. Here\'s a preview of the Logistic Regression model’s F1 score, which highlights the model\'s ability to balance precision and recall:\nLogistic Regression F1 Score: 0.58\nThis approach allowed me to track the effectiveness of each model and make adjustments as needed for improving performance.\n\nF1 score with different parameters (Logistic Regression)\nI recommend running the notebook below in your own account to see the results. You\'ll definitely start appreciating the power of MLflow.\nFor more information, please refer to the accompanying Databricks notebook.\n\nHyperparameter tuning\nHyperparameter tuning is crucial for identifying the best parameter combination for your model. This technique involves an iterative process where, for every parameter combination, the model is trained and tested on a dataset. It is often described as a "trial and error" method.\nHowever, this approach can be computationally expensive, especially when working with complex or heavy machine learning models. For large-scale problems, hyperparameter tuning can be made more efficient through sampling or batch methods. In these methods, you don\'t use the entire dataset for training the model; instead, you choose random or stratified data points from the dataset to train the model. Although this may slightly reduce accuracy, it is more feasible when working with large datasets.\nFor my project, I used Grid Search CV to find the best hyperparameter combination for the model. Below are some common techniques for hyperparameter tuning, especially for large datasets:\nGrid Search This technique exhaustively searches through a specified set of hyperparameter values, trying all possible combinations. While effective, it can be computationally expensive for larger datasets due to the exhaustive nature of the search.\nRandom Search Randomly samples from the hyperparameter space, offering a faster alternative to grid search. This method explores a wider range of hyperparameters with fewer evaluations, making it more efficient for larger datasets.\nBayesian Optimization This method uses probabilistic models to predict the performance of different hyperparameters. It selects the next set of hyperparameters to evaluate based on previous results, making it more efficient and suitable for large datasets.\nGenetic Algorithms Inspired by natural selection, these algorithms iteratively evolve a population of hyperparameter sets to improve model performance. This method works well with complex search spaces.\nHyperband Hyperband combines random search with early stopping to dynamically allocate resources across multiple configurations, identifying promising hyperparameters quickly without excessive computational costs.\nBayesian Optimization with Gaussian Processes This technique models the hyperparameter search space using Gaussian processes, focusing on regions that are likely to yield better results, which is particularly useful for large datasets where computational resources are limited.\n\nTo optimize hyperparameter tuning for larger datasets, these techniques can be combined with parallel computing and distributed processing frameworks such as Dask, Spark, or multi-GPU setups. This enables more efficient hyperparameter search and reduces the overall computational overhead.\nI choosed Logistc Regression model and trained my Model with best parameters.\nSource Distribution for Model Packaging\nPackaging your machine learning model is a best practice, especially if you don’t plan to update it frequently. Imagine thousands of lines of code that can now be utilized with just a single line—this is the power of model packaging.\n\n1. Project Folder Setup\nTo ensure better organization, I created a main folder called sentiment_prediction and moved all machine learning pipeline files and dependencies into this folder. This helped in maintaining a clean structure and simplified the management of the entire project.\nBefore moving forward I recommend you to visit this pdf it will practically show you step by step process for building python package.\nPDF : step by step guid for python package building\n\n2. Manifest.in\nThe Manifest.in file plays a crucial role in controlling which files and folders should be included or excluded during the packaging process. It helps to specify the structure of the package for distribution.\nKey commands used in the Manifest.in file include:\ninclude <file/folder>: Include specific files or folders.\nexclude <file/folder>: Exclude specific files or folders.\nrecursive-include <path>: Include all files from a directory recursively.\nrecursive-exclude <path>: Exclude all files from a directory recursively.\n3. Setup.py\nThe setup.py file contains the project\'s metadata and is essential for creating the package. It defines key information about the project, such as:\nProject name, version, description, and author details.\nDependencies required for the package (install_requires), making it easy to install all necessary libraries.\n\n4. Building the Package\nTo build the package, I used the following command:\n\npython setup.py sdist bdist_wheel\nThis command generates two folders:\n\nbuild/: Contains the entire project package as defined in the Manifest.in.\ndist/: Contains the distributable files: .whl (wheel file) .gz (compressed source archive)\n\n5. Global Access via GitHub\nNow you can access your package gloablly, by refering your repository. I provided my package below go and check out.\n\nRepository: GitHub Repo Link\nYou can install the package directly from GitHub using the following command:\n\npip install git+https://github.com/vijaytakbhate2002/sentiment_prediction_python_package.git \nTo ensure it worked globally, I tested it again:\n\nfrom sentiment_prediction import predict\nprint(predict.predictor("Great progress shared today!")) \noutput:[\'Negative\']\nFlask Application \nBuilding application will help us to give our NLP model experience to people, so I built one flask application.\nWeb Application UI\n\nHere is demonstration of project: project demo\nI left a blank section for user suggestion and feedback, these feedbacks are getting stored in database for future model analysis or any business work.\nDatabase configuration\nFor storing collected user data we need to configure a database. It will help us to improve model as per user need.\nI used Google Cloud MySQL instance for integrating my application with database, GCP is paid but you can use free credit of GCP for first 3 months, for doing almost all Cloud Work.\nYou need to create your GCP account, then create one instance under SQL and by configuring your local system with instance you are good to go.\nDocker containerization\nIf you are not awared about docker and it\'s basic concepts you can refer my previous article which explains you all about docker.\nThis guide will help you build a solid foundation in Docker, enabling you to confidently use it for your projects.\nDocker guide: Comprehensive Docker guide for deploying Flas app\nDeploy\nDeployment of web app will help us to engange people and provide them real actual experience of our services.\nAfter deployment you need to collect user data and store it for future analysis, this data contain user feedback and suggestions.\nAfter deployment it\'s not end of the process we need to collect user feedback and again follow same steps fine tune Model as per user need.\n\nSummary of Blog\nThis blog covers the lifecycle of a Machine Learning project, from ETL to deployment. It details building an ETL pipeline using PySpark for efficient data handling, EDA, and NLP preprocessing techniques like tokenization, TF-IDF, and WordCloud visualization. \nIt highlights ML experiments with MLflow on Databricks and hyperparameter tuning using Grid Search.\nThe model was packaged into a Python package and deployed as a Flask application with a database backend (Google Cloud MySQL) and Dockerized for scalability. \nThe app collects user feedback for continuous improvement. It emphasizes end-to-end integration, including cloud and containerization, to deliver a robust ML solution.\nHappy Learning!\n\n'}, {'content': '\nLanguages I Speak\nEnglish, Marathi, Hindi\n\n'}, {'content': '\n\nSoft skills:\nCritical Thinking, Intellectual Rigor, Problem Solving, Understanding Business Needs'}]
2024-11-25 21:49:35 - root - INFO - Document store already contains 14 documents. Skipping write.
2024-11-25 21:49:35 - haystack.modeling.utils - INFO - Using devices: CPU - Number of GPUs: 0
2024-11-25 21:49:35 - haystack.modeling.utils - INFO - Using devices: CPU - Number of GPUs: 0
2024-11-25 21:49:35 - haystack.modeling.model.language_model - INFO -  * LOADING MODEL: 'deepset/roberta-base-squad2' (Roberta)
2024-11-25 21:49:36 - haystack.modeling.model.language_model - INFO - Auto-detected model language: english
2024-11-25 21:49:36 - haystack.modeling.model.language_model - INFO - Loaded 'deepset/roberta-base-squad2' (Roberta model) from model hub.
2024-11-25 21:49:37 - haystack.modeling.utils - INFO - Using devices: CPU - Number of GPUs: 0
2024-11-25 21:50:09 - root - INFO - Reading text document
2024-11-25 21:50:09 - root - INFO - loaded document = [{'content': '\nPersonal Information\n\nName: Vijay Takbhate\nEmail: vijaytakbhate20@gmail.com Phone: 8767363681\nGitHub: https://github.com/vijaytakbhate2002\nLinkedIn: https://www.linkedin.com/in/vijay-takbhate-b9231a236/ Kaggle: https://www.kaggle.com/vijay20213\n\n'}, {'content': '\n\nExperience - Fox Solutions Pvt. Ltd.\nRole: Automation Engineer Duration: Jun 2024 - Oct 2024 Location: Maharashtra\nKey Contributions:\n-Completed 2 months of internship plus 4 months of full-time work.\n-Worked with PLC and SCADA systems, focusing on automating processes and optimizing operational efficiency.\n-Collaborated with cross-functional teams to implement automation solutions for industrial applications.\n\n'}, {'content': '\n\nExperience - Cei Design Consultancy Pvt. Ltd.\nRole: Python Developer Intern Duration: Aug 2024 - Sept 2024 Location: Remote, Maharashtra\nKey Contributions:\n-Specialized in data processing using Python and Excel.\n-Utilized OpenCV for image processing tasks.\n\n'}, {'content': '\n\nExperience - Ujucode\nRole: Subject Matter Expert Intern Duration: Aug 2023 - Oct 2023 Location: Remote, Maharashtra\nKey Contributions:\n-Contributed as a Python developer for a ChatBot project.\n-Handled backend development tasks and researched Python modules.\n\n'}, {'content': '\n\nProject - Twitter Post Sentiment Prediction\nDetails:\n-Engineered an ETL pipeline using PySpark and SQL.\n-Conducted sentiment analysis using NLP (TF-IDF) and optimized hyperparameters.\n-Monitored model performance through MLFlow on Databricks.\n-Leveraged Google Cloud Storage and MySQL for data management.\n-Deployed the model using Docker and hosted it on Render.\n\n'}, {'content': '\n\nProject - Text-Text Chat-Bot\nDetails:\n-Designed an advanced Chat-Bot using the NVIDIA API and prompt engineering.\n-Features include paraphrasing, grammar correction, AI detection, plagiarism checking, and content summarization.\n-Targeted at content creators, researchers, and businesses.\n-Technologies Used: HTML, CSS, Python Flask, Cloud Database, and Render.\n\n'}, {'content': '\n\nProject - Hand Gesture Recognition\nDetails:\n-Used Google-s MediaPipe framework for detecting hand landmarks and gestures.\n-Created and labeled a custom dataset of hand gestures for training.\n-Developed a Streamlit application to improve accessibility and flexibility.\n\n'}, {'content': '\n\nTechnical Skills\nLanguages: MySQL, Python, HTML, CSS\nTechnologies: Streamlit, Flask, VS Code, GitHub, MLflow, Docker, PySpark, Databricks, Google Cloud Platform\n\n'}, {'content': '\n\nCertification\nMLOps Bootcamp: Mastering AI Operations for Success (Jun 2024)\n-Learned about the MLOps lifecycle and modular programming.\n-Acquired skills in Git, Python, Flask, and MLflow.\n\n'}, {'content': '\n\nEducation Details:\nBachelor of Technology in Electronics and Telecommunication (May 2024) Institution: SVERI-s College of Engineering, Pandharpur, Maharashtra Score: 81.71\nDiploma in Electronics and Telecommunication (May 2021) Institution: SVERI-s College of Engineering, Pandharpur, Maharashtra Score: 91.73\n\n'}, {'content': "\n\nBlogging\n\n\nSupervised, Unsupervised, and Beyond: ML Techniques Simplified\nNovember 25, 2024\nThere are several techniques for ML training. Among these, I will cover the following:\n\nSupervised and Unsupervised Learning\nSemi-Supervised Learning\nOffline and Online Learning\nInstance-Based and Model-Based Learning\n\nSupervised and Unsupervised Learning\nSupervised learning is like spoon-feeding our ML model in its initial stages, allowing it to learn and improve over time. Here, I’m referring to the training process.\n\nIn supervised learning, there are input columns and output columns, also called target columns. For example, in spam detection—a classification problem—the input is the email, and the target is whether the email is spam or not. That’s it!\nThis process resembles a student-teacher scenario where the teacher is a human, and the student is the model. The dataset serves as the knowledge used to train the student (model)\nSee content credentials\n\nhumand and model\nIn Unsupervised learning, we are not aware of the data labels. Instead, we separate the input data by analyzing similarities and grouping them into different clusters.\nOnce the clusters are formed, we can assign custom labels to each one. This technique is widely used to identify product relationships in online shopping and to recommend new products based on a user’s purchase history.\n\nUnsupervised Learning Clusters\nHere three clusters with three different image categories are formed\n\nSemi-Supervised Learning\nSemi-Supervised Learning is a combination of supervised and unsupervised learning, where some data is labeled and some data is unlabeled. A good example of this is Google Photos, which automatically separates new photos into their respective groups based on whether they contain a particular person in each image.\n\nThere are several techniques under semi-supervised learning; here, we will focus on the following:\nSelf Learning\nConsistency Regularization\nGenerative Models\nGraph-Based Learning\n\nLet's discuss them one by one:\n\nSelf Learning\nSelf Learning trains a model with labeled data and generates pseudo-labels for the unlabeled data. \n\nSelf learning\nThen, the model is trained on the entire dataset, which includes both the generated pseudo-labels and the labeled data.\n\nConsistency Regularization\nThis technique uses data augmentation to generate similar data, and then the model is enforced to predict the same outcome for both the augmented and original data. It helps create a model that can find similarities in both labeled and unlabeled data, predicting the same output for unlabeled data as it would for labeled data.\n\nData augmentation includes techniques like image flipping, blurring, rotation, etc. After augmenting the data, the model is trained to predict the same class for both the augmented and original images.\n\nGenerative Models\nGenerative models create synthetic data points and learn the underlying structure of the training data. These models can generate new datasets using encoders. \n\nExamples include Generative Adversarial Networks (GANs) and Variational Autoencoders (VAEs).\n\nGraph-Based Learning\nThis technique creates nodes for both labeled and unlabeled data. Similar nodes are then connected to each other.\nFor example, when dealing with images, each image starts as a separate node. If similar images are found based on their features, they are grouped or connected by an edge.\nRefer below graph to understand graph-based learning\nGraph based learning\nHere label 1 is came from labeled image of group 1, simillarly for label 2 and label 3. Finally we came up with three labels that means labeled data has three unique labels.\nBatch (Offline) and Online Learning\nBatch Learning\nBatch learning, also known as offline learning, is a technique where the developer needs to stop the deployed ML model, retrain it on new data, and then redeploy it. \nThis technique is useful when frequent updates are not required. For example, product recommendation systems can often be retrained weekly or even over several days.\nBatch learning can be costly when training the model on the entire dataset every time. However, using incremental learning models can help avoid unnecessary retraining, making the process more efficient.\nHere are some examples on incremental learning models, as of now we are focusing on general concepts so don't go much deeper into these types.\nStochastic Gradient Descent (SGD)\nNaive Bayes (Online Version)\nIncremental Support Vector Machines (ISVM)\nOnline Random Forests\nIncremental Decision Trees\nYou can refer this article to learn more about incremental learning\nHere’s your text with grammatical corrections and improved clarity:\nOnline Learning\nOnline learning is used when an ML model needs to be updated continuously with new inputs, such as in stock price prediction. In this scenario, the model must remain aware of the most recent data.\n\nBatch and online learning\nOnly models capable of incremental learning are used in online learning. Each new instance is fed back into the model to update its internal weights based on the latest input.\n\nInstance-Based and Model-Based Learning\nInstance-Based Learning\nInstance-based learning involves comparing a new input with stored data. If similarity is found, the model returns the label of the corresponding input. This approach uses pattern matching techniques. \nHere model works as a search engin not exactly same but it will find simmilarity from stored data.\nSome common models used in instance-based learning include:\nK-Nearest Neighbors (K-NN) with labels\nLocally Weighted Regression, etc.\nModel-Based Learning\nIn model-based learning, we train the model using a training dataset. During the training process, the model creates its own complex patterns (mathematical equations) to make predictions for future inputs. \nA simple example is the equation y=mx+cy = mx + c, which represents a trained model. Some common models used in model-based learning include:\nLinear Regression\nLogistic Regression\nDecision Tree, etc.\n\nSummary\nMachine Learning techniques can be broadly categorized into supervised, unsupervised, and semi-supervised learning. \nSupervised learning uses labeled data, like spam detection, while unsupervised learning works with unlabeled data, grouping similar points into clusters. \nSemi-supervised learning combines both, employing techniques like self-learning and consistency regularization to leverage partially labeled datasets.\nOther approaches include batch learning, where models are retrained periodically, and online learning, which updates models continuously with new data, suitable for dynamic tasks like stock price prediction. \nInstance-based learning relies on pattern matching (e.g., K-Nearest Neighbors), while model-based learning creates mathematical models (e.g., Linear Regression) to make predictions.\nThis is all about Machine Learning techniques. If you learned something, let me know in the comments. Your suggestions will help me improve my blogs.\nThanks for reading!\n\n"}, {'content': '\nBlogging\n\nMastering the End-to-End Machine Learning Lifecycle: From Data to Deployment\n\nNovember 24, 2024\n This article takes you through the complete lifecycle of a Machine Learning project. From ETL to deployment, I’ll share every detail of how I brought this project to life.\nData Engineering\nData Engineering serves as the starting point in the Machine Learning project lifecycle, bringing all distributed data together in one place.\nFor this project, I utilized an ETL pipeline—a core concept in Data Engineering. It enabled me to extract raw data, transform it into a meaningful format, and load it into a suitable location for further processing.\nLet\'s deep dive into ETL pipeline\nETL stands for Extract, Transform, and Load. These pipelines can be executed periodically (e.g., daily or hourly) to fetch real-time data, enhancing the predictive power of our Machine Learning model for real-world applications.\nThis process can be automated using tools like Apache Airflow, Kubeflow Pipelines, AWS Step Functions, and more, streamlining the workflow for consistent and efficient data updates.\n\nETL Pipeline:\nETL Pipeline\nExtract Operation\nThe extract operation is responsible for fetching data from various sources such as websites, databases, and APIs, as illustrated in the flowchart above.\n\nTransform Operation\nThe transform operation focuses on data cleaning, feature selection, and manipulation. In my project, I used this step to extract only the required features, ensuring the data is ready for the next phase.\n\nLoad Operation\nThe load operation transfers the transformed data to its destination, where we can choose the appropriate storage format. Typically, the processed data is stored in a database for further analysis and model training.\n\nFor my project, I used PySpark to build the ETL pipeline, as it enables efficient processing of large datasets.\nWhy not Pandas?\nWhile Pandas is excellent for small to medium-sized datasets, it stores DataFrames in RAM, which can lead to out-of-memory exceptions when handling large datasets.\nIn contrast, PySpark creates a session and processes data in chunks, storing it in ROM, making it ideal for handling large-scale data.\nFor the actual implementation of the pipeline using PySpark, please refer to the accompanying jupyter notebook.\nEDA (Data Analysis)\nData understanding is a critical stage before building any Machine Learning model. It allows us to analyze the data, plan the data processing steps, and gain insights into its structure and quality.\nIn this project, I examined the balance of data in my training and testing datasets and found it to be well-distributed across all four categories.\n\nBalanceness Checking on train and test data\nThroughout the entire process, I focused on two columns:\nTweets\nSentiments\nThe Tweets column contains the raw Twitter text data, while the Sentiments column serves as the target variable for prediction.\nNatural Language Processing\nI applied several key NLP techniques to preprocess the data and prepare it for Machine Learning model building. Below are the main steps I executed:\n\nText Processing\nRegex: I applied regular expressions to clean the text by removing URLs, hashtags, HTML tags, and keeping only alphanumeric characters. This helped eliminate unnecessary noise from the data.\nNLTK: Using the Natural Language Toolkit (NLTK), I performed word tokenization, stemming and lemmatization.\nWord tokenization is just splitting sentence into words, so we can processing each word from sentence individually.\nStemming helps us to truncate prefix or suffix of text to reduce count on unique words from corpus (paragraph).\nLemmatization is the proecss of converting any word into it\'s base word, for eg. Played will convert to play.\n\nWordCloud \nWord cloud concept help you to understand importance of words from given data. I splitted my data into four sections.\ndata for negative sentiments\ndata for positive sentiments\ndata for neutral sentiments\ndata for irrelevant sentiments\n\nHere is the representation of most frequent words for each category.\nCategorywise Word Cloud Presentation\nHere you can clearly see that there is no much difference in negative and positive sentiments data.\nthis is representation of bad data, here we can filterout our data for further processing, it might reduce data but you can do data augmentation techniques here to increase your data.\naugmenting of data in NLP with TF-IDF will not bad idea because TF-IDF and any other porcessing technique that I used is not able to detect sentence grammer or it doesn,t require sophisticated text. \nYou can think our input text will work as bag of word for model. you can understand by refering below vectorization technique.\nVectorization with TF-IDF \nI used Term Frequency-Inverse Document Frequency (TF-IDF) for text vectorization. This method transforms text into a numerical format, considering the importance of each word across documents, which prevents frequent words from dominating the model.\n\nTF-IDF\nTF-IDF stands for Term Frequency-Inverse Document Frequency. \nTerm Frequency (TF): This measures how frequently a term (t) appears in a specific document (d). It\'s calculated by dividing the number of occurrences of the term in the document by the total number of terms in that document.\nInverse Document Frequency (IDF): This measures how important a term is across the entire collection of documents. It\'s calculated by taking the logarithm of the ratio of the total number of documents (N) to the number of documents containing the term (df(t)). A higher IDF value indicates a rarer term, making it more significant.\nHere is the resulant data we got from TF-IDF\nAfter all preprocessing of text it\'s time to save our data for mlflow experiments. To see actual implementation of end to end process of NLP you can refer jupyter notebook.\nmlflow experiments\nMLflow experiments allow you to conduct multiple experiments with your trained model, helping you track and compare results over time. For running MLflow experiments, I prefer using Databricks as it offers an integrated experiment section, making the process much more streamlined and efficient.\nIn Databricks, you can easily connect your experiment by passing the experiment ID into the MLflow code. This integration simplifies the entire workflow, enabling better experiment tracking and easier comparison of model performance.\nFor my experiments, I worked with several models, including Logistic Regression, Multinomial Naive Bayes, and Decision Tree Classifier. By applying different combinations of parameters, I was able to experiment with and compare the performance of each model. Here\'s a preview of the Logistic Regression model’s F1 score, which highlights the model\'s ability to balance precision and recall:\nLogistic Regression F1 Score: 0.58\nThis approach allowed me to track the effectiveness of each model and make adjustments as needed for improving performance.\n\nF1 score with different parameters (Logistic Regression)\nI recommend running the notebook below in your own account to see the results. You\'ll definitely start appreciating the power of MLflow.\nFor more information, please refer to the accompanying Databricks notebook.\n\nHyperparameter tuning\nHyperparameter tuning is crucial for identifying the best parameter combination for your model. This technique involves an iterative process where, for every parameter combination, the model is trained and tested on a dataset. It is often described as a "trial and error" method.\nHowever, this approach can be computationally expensive, especially when working with complex or heavy machine learning models. For large-scale problems, hyperparameter tuning can be made more efficient through sampling or batch methods. In these methods, you don\'t use the entire dataset for training the model; instead, you choose random or stratified data points from the dataset to train the model. Although this may slightly reduce accuracy, it is more feasible when working with large datasets.\nFor my project, I used Grid Search CV to find the best hyperparameter combination for the model. Below are some common techniques for hyperparameter tuning, especially for large datasets:\nGrid Search This technique exhaustively searches through a specified set of hyperparameter values, trying all possible combinations. While effective, it can be computationally expensive for larger datasets due to the exhaustive nature of the search.\nRandom Search Randomly samples from the hyperparameter space, offering a faster alternative to grid search. This method explores a wider range of hyperparameters with fewer evaluations, making it more efficient for larger datasets.\nBayesian Optimization This method uses probabilistic models to predict the performance of different hyperparameters. It selects the next set of hyperparameters to evaluate based on previous results, making it more efficient and suitable for large datasets.\nGenetic Algorithms Inspired by natural selection, these algorithms iteratively evolve a population of hyperparameter sets to improve model performance. This method works well with complex search spaces.\nHyperband Hyperband combines random search with early stopping to dynamically allocate resources across multiple configurations, identifying promising hyperparameters quickly without excessive computational costs.\nBayesian Optimization with Gaussian Processes This technique models the hyperparameter search space using Gaussian processes, focusing on regions that are likely to yield better results, which is particularly useful for large datasets where computational resources are limited.\n\nTo optimize hyperparameter tuning for larger datasets, these techniques can be combined with parallel computing and distributed processing frameworks such as Dask, Spark, or multi-GPU setups. This enables more efficient hyperparameter search and reduces the overall computational overhead.\nI choosed Logistc Regression model and trained my Model with best parameters.\nSource Distribution for Model Packaging\nPackaging your machine learning model is a best practice, especially if you don’t plan to update it frequently. Imagine thousands of lines of code that can now be utilized with just a single line—this is the power of model packaging.\n\n1. Project Folder Setup\nTo ensure better organization, I created a main folder called sentiment_prediction and moved all machine learning pipeline files and dependencies into this folder. This helped in maintaining a clean structure and simplified the management of the entire project.\nBefore moving forward I recommend you to visit this pdf it will practically show you step by step process for building python package.\nPDF : step by step guid for python package building\n\n2. Manifest.in\nThe Manifest.in file plays a crucial role in controlling which files and folders should be included or excluded during the packaging process. It helps to specify the structure of the package for distribution.\nKey commands used in the Manifest.in file include:\ninclude <file/folder>: Include specific files or folders.\nexclude <file/folder>: Exclude specific files or folders.\nrecursive-include <path>: Include all files from a directory recursively.\nrecursive-exclude <path>: Exclude all files from a directory recursively.\n3. Setup.py\nThe setup.py file contains the project\'s metadata and is essential for creating the package. It defines key information about the project, such as:\nProject name, version, description, and author details.\nDependencies required for the package (install_requires), making it easy to install all necessary libraries.\n\n4. Building the Package\nTo build the package, I used the following command:\n\npython setup.py sdist bdist_wheel\nThis command generates two folders:\n\nbuild/: Contains the entire project package as defined in the Manifest.in.\ndist/: Contains the distributable files: .whl (wheel file) .gz (compressed source archive)\n\n5. Global Access via GitHub\nNow you can access your package gloablly, by refering your repository. I provided my package below go and check out.\n\nRepository: GitHub Repo Link\nYou can install the package directly from GitHub using the following command:\n\npip install git+https://github.com/vijaytakbhate2002/sentiment_prediction_python_package.git \nTo ensure it worked globally, I tested it again:\n\nfrom sentiment_prediction import predict\nprint(predict.predictor("Great progress shared today!")) \noutput:[\'Negative\']\nFlask Application \nBuilding application will help us to give our NLP model experience to people, so I built one flask application.\nWeb Application UI\n\nHere is demonstration of project: project demo\nI left a blank section for user suggestion and feedback, these feedbacks are getting stored in database for future model analysis or any business work.\nDatabase configuration\nFor storing collected user data we need to configure a database. It will help us to improve model as per user need.\nI used Google Cloud MySQL instance for integrating my application with database, GCP is paid but you can use free credit of GCP for first 3 months, for doing almost all Cloud Work.\nYou need to create your GCP account, then create one instance under SQL and by configuring your local system with instance you are good to go.\nDocker containerization\nIf you are not awared about docker and it\'s basic concepts you can refer my previous article which explains you all about docker.\nThis guide will help you build a solid foundation in Docker, enabling you to confidently use it for your projects.\nDocker guide: Comprehensive Docker guide for deploying Flas app\nDeploy\nDeployment of web app will help us to engange people and provide them real actual experience of our services.\nAfter deployment you need to collect user data and store it for future analysis, this data contain user feedback and suggestions.\nAfter deployment it\'s not end of the process we need to collect user feedback and again follow same steps fine tune Model as per user need.\n\nSummary of Blog\nThis blog covers the lifecycle of a Machine Learning project, from ETL to deployment. It details building an ETL pipeline using PySpark for efficient data handling, EDA, and NLP preprocessing techniques like tokenization, TF-IDF, and WordCloud visualization. \nIt highlights ML experiments with MLflow on Databricks and hyperparameter tuning using Grid Search.\nThe model was packaged into a Python package and deployed as a Flask application with a database backend (Google Cloud MySQL) and Dockerized for scalability. \nThe app collects user feedback for continuous improvement. It emphasizes end-to-end integration, including cloud and containerization, to deliver a robust ML solution.\nHappy Learning!\n\n'}, {'content': '\nLanguages I Speak\nEnglish, Marathi, Hindi\n\n'}, {'content': '\n\nSoft skills:\nCritical Thinking, Intellectual Rigor, Problem Solving, Understanding Business Needs'}]
2024-11-25 21:50:09 - root - INFO - Document store already contains 14 documents. Skipping write.
2024-11-25 21:50:09 - haystack.modeling.utils - INFO - Using devices: CPU - Number of GPUs: 0
2024-11-25 21:50:09 - haystack.modeling.utils - INFO - Using devices: CPU - Number of GPUs: 0
2024-11-25 21:50:10 - haystack.modeling.model.language_model - INFO -  * LOADING MODEL: 'deepset/minilm-uncased-squad2' (Bert)
2024-11-25 21:50:10 - haystack.modeling.model.language_model - INFO - Auto-detected model language: english
2024-11-25 21:50:10 - haystack.modeling.model.language_model - INFO - Loaded 'deepset/minilm-uncased-squad2' (Bert model) from model hub.
2024-11-25 21:50:11 - haystack.modeling.utils - INFO - Using devices: CPU - Number of GPUs: 0
2024-11-25 21:51:38 - root - INFO - Loading existing FAISS document store...
2024-11-25 21:51:38 - haystack.modeling.utils - INFO - Using devices: CPU - Number of GPUs: 0
2024-11-25 21:51:40 - haystack.modeling.model.language_model - WARNING - Using a model of type 'mpnet' which might be incompatible with DPR encoders. Only Bert-based encoders are supported. They need input_ids, token_type_ids, attention_mask as input tensors.
2024-11-25 21:51:42 - haystack.modeling.model.language_model - INFO - Auto-detected model language: english
2024-11-25 21:51:44 - haystack.modeling.model.language_model - WARNING - Using a model of type 'mpnet' which might be incompatible with DPR encoders. Only Bert-based encoders are supported. They need input_ids, token_type_ids, attention_mask as input tensors.
2024-11-25 21:51:46 - haystack.modeling.model.language_model - INFO - Auto-detected model language: english
2024-11-25 21:51:46 - root - INFO - Reading text document
2024-11-25 21:51:46 - root - INFO - loaded document = [{'content': '\nPersonal Information\n\nName: Vijay Takbhate\nEmail: vijaytakbhate20@gmail.com Phone: 8767363681\nGitHub: https://github.com/vijaytakbhate2002\nLinkedIn: https://www.linkedin.com/in/vijay-takbhate-b9231a236/ Kaggle: https://www.kaggle.com/vijay20213\n\n'}, {'content': '\n\nExperience - Fox Solutions Pvt. Ltd.\nRole: Automation Engineer Duration: Jun 2024 - Oct 2024 Location: Maharashtra\nKey Contributions:\n-Completed 2 months of internship plus 4 months of full-time work.\n-Worked with PLC and SCADA systems, focusing on automating processes and optimizing operational efficiency.\n-Collaborated with cross-functional teams to implement automation solutions for industrial applications.\n\n'}, {'content': '\n\nExperience - Cei Design Consultancy Pvt. Ltd.\nRole: Python Developer Intern Duration: Aug 2024 - Sept 2024 Location: Remote, Maharashtra\nKey Contributions:\n-Specialized in data processing using Python and Excel.\n-Utilized OpenCV for image processing tasks.\n\n'}, {'content': '\n\nExperience - Ujucode\nRole: Subject Matter Expert Intern Duration: Aug 2023 - Oct 2023 Location: Remote, Maharashtra\nKey Contributions:\n-Contributed as a Python developer for a ChatBot project.\n-Handled backend development tasks and researched Python modules.\n\n'}, {'content': '\n\nProject - Twitter Post Sentiment Prediction\nDetails:\n-Engineered an ETL pipeline using PySpark and SQL.\n-Conducted sentiment analysis using NLP (TF-IDF) and optimized hyperparameters.\n-Monitored model performance through MLFlow on Databricks.\n-Leveraged Google Cloud Storage and MySQL for data management.\n-Deployed the model using Docker and hosted it on Render.\n\n'}, {'content': '\n\nProject - Text-Text Chat-Bot\nDetails:\n-Designed an advanced Chat-Bot using the NVIDIA API and prompt engineering.\n-Features include paraphrasing, grammar correction, AI detection, plagiarism checking, and content summarization.\n-Targeted at content creators, researchers, and businesses.\n-Technologies Used: HTML, CSS, Python Flask, Cloud Database, and Render.\n\n'}, {'content': '\n\nProject - Hand Gesture Recognition\nDetails:\n-Used Google-s MediaPipe framework for detecting hand landmarks and gestures.\n-Created and labeled a custom dataset of hand gestures for training.\n-Developed a Streamlit application to improve accessibility and flexibility.\n\n'}, {'content': '\n\nTechnical Skills\nLanguages: MySQL, Python, HTML, CSS\nTechnologies: Streamlit, Flask, VS Code, GitHub, MLflow, Docker, PySpark, Databricks, Google Cloud Platform\n\n'}, {'content': '\n\nCertification\nMLOps Bootcamp: Mastering AI Operations for Success (Jun 2024)\n-Learned about the MLOps lifecycle and modular programming.\n-Acquired skills in Git, Python, Flask, and MLflow.\n\n'}, {'content': '\n\nEducation Details:\nBachelor of Technology in Electronics and Telecommunication (May 2024) Institution: SVERI-s College of Engineering, Pandharpur, Maharashtra Score: 81.71\nDiploma in Electronics and Telecommunication (May 2021) Institution: SVERI-s College of Engineering, Pandharpur, Maharashtra Score: 91.73\n\n'}, {'content': "\n\nBlogging\n\n\nSupervised, Unsupervised, and Beyond: ML Techniques Simplified\nNovember 25, 2024\nThere are several techniques for ML training. Among these, I will cover the following:\n\nSupervised and Unsupervised Learning\nSemi-Supervised Learning\nOffline and Online Learning\nInstance-Based and Model-Based Learning\n\nSupervised and Unsupervised Learning\nSupervised learning is like spoon-feeding our ML model in its initial stages, allowing it to learn and improve over time. Here, I’m referring to the training process.\n\nIn supervised learning, there are input columns and output columns, also called target columns. For example, in spam detection—a classification problem—the input is the email, and the target is whether the email is spam or not. That’s it!\nThis process resembles a student-teacher scenario where the teacher is a human, and the student is the model. The dataset serves as the knowledge used to train the student (model)\nSee content credentials\n\nhumand and model\nIn Unsupervised learning, we are not aware of the data labels. Instead, we separate the input data by analyzing similarities and grouping them into different clusters.\nOnce the clusters are formed, we can assign custom labels to each one. This technique is widely used to identify product relationships in online shopping and to recommend new products based on a user’s purchase history.\n\nUnsupervised Learning Clusters\nHere three clusters with three different image categories are formed\n\nSemi-Supervised Learning\nSemi-Supervised Learning is a combination of supervised and unsupervised learning, where some data is labeled and some data is unlabeled. A good example of this is Google Photos, which automatically separates new photos into their respective groups based on whether they contain a particular person in each image.\n\nThere are several techniques under semi-supervised learning; here, we will focus on the following:\nSelf Learning\nConsistency Regularization\nGenerative Models\nGraph-Based Learning\n\nLet's discuss them one by one:\n\nSelf Learning\nSelf Learning trains a model with labeled data and generates pseudo-labels for the unlabeled data. \n\nSelf learning\nThen, the model is trained on the entire dataset, which includes both the generated pseudo-labels and the labeled data.\n\nConsistency Regularization\nThis technique uses data augmentation to generate similar data, and then the model is enforced to predict the same outcome for both the augmented and original data. It helps create a model that can find similarities in both labeled and unlabeled data, predicting the same output for unlabeled data as it would for labeled data.\n\nData augmentation includes techniques like image flipping, blurring, rotation, etc. After augmenting the data, the model is trained to predict the same class for both the augmented and original images.\n\nGenerative Models\nGenerative models create synthetic data points and learn the underlying structure of the training data. These models can generate new datasets using encoders. \n\nExamples include Generative Adversarial Networks (GANs) and Variational Autoencoders (VAEs).\n\nGraph-Based Learning\nThis technique creates nodes for both labeled and unlabeled data. Similar nodes are then connected to each other.\nFor example, when dealing with images, each image starts as a separate node. If similar images are found based on their features, they are grouped or connected by an edge.\nRefer below graph to understand graph-based learning\nGraph based learning\nHere label 1 is came from labeled image of group 1, simillarly for label 2 and label 3. Finally we came up with three labels that means labeled data has three unique labels.\nBatch (Offline) and Online Learning\nBatch Learning\nBatch learning, also known as offline learning, is a technique where the developer needs to stop the deployed ML model, retrain it on new data, and then redeploy it. \nThis technique is useful when frequent updates are not required. For example, product recommendation systems can often be retrained weekly or even over several days.\nBatch learning can be costly when training the model on the entire dataset every time. However, using incremental learning models can help avoid unnecessary retraining, making the process more efficient.\nHere are some examples on incremental learning models, as of now we are focusing on general concepts so don't go much deeper into these types.\nStochastic Gradient Descent (SGD)\nNaive Bayes (Online Version)\nIncremental Support Vector Machines (ISVM)\nOnline Random Forests\nIncremental Decision Trees\nYou can refer this article to learn more about incremental learning\nHere’s your text with grammatical corrections and improved clarity:\nOnline Learning\nOnline learning is used when an ML model needs to be updated continuously with new inputs, such as in stock price prediction. In this scenario, the model must remain aware of the most recent data.\n\nBatch and online learning\nOnly models capable of incremental learning are used in online learning. Each new instance is fed back into the model to update its internal weights based on the latest input.\n\nInstance-Based and Model-Based Learning\nInstance-Based Learning\nInstance-based learning involves comparing a new input with stored data. If similarity is found, the model returns the label of the corresponding input. This approach uses pattern matching techniques. \nHere model works as a search engin not exactly same but it will find simmilarity from stored data.\nSome common models used in instance-based learning include:\nK-Nearest Neighbors (K-NN) with labels\nLocally Weighted Regression, etc.\nModel-Based Learning\nIn model-based learning, we train the model using a training dataset. During the training process, the model creates its own complex patterns (mathematical equations) to make predictions for future inputs. \nA simple example is the equation y=mx+cy = mx + c, which represents a trained model. Some common models used in model-based learning include:\nLinear Regression\nLogistic Regression\nDecision Tree, etc.\n\nSummary\nMachine Learning techniques can be broadly categorized into supervised, unsupervised, and semi-supervised learning. \nSupervised learning uses labeled data, like spam detection, while unsupervised learning works with unlabeled data, grouping similar points into clusters. \nSemi-supervised learning combines both, employing techniques like self-learning and consistency regularization to leverage partially labeled datasets.\nOther approaches include batch learning, where models are retrained periodically, and online learning, which updates models continuously with new data, suitable for dynamic tasks like stock price prediction. \nInstance-based learning relies on pattern matching (e.g., K-Nearest Neighbors), while model-based learning creates mathematical models (e.g., Linear Regression) to make predictions.\nThis is all about Machine Learning techniques. If you learned something, let me know in the comments. Your suggestions will help me improve my blogs.\nThanks for reading!\n\n"}, {'content': '\nBlogging\n\nMastering the End-to-End Machine Learning Lifecycle: From Data to Deployment\n\nNovember 24, 2024\n This article takes you through the complete lifecycle of a Machine Learning project. From ETL to deployment, I’ll share every detail of how I brought this project to life.\nData Engineering\nData Engineering serves as the starting point in the Machine Learning project lifecycle, bringing all distributed data together in one place.\nFor this project, I utilized an ETL pipeline—a core concept in Data Engineering. It enabled me to extract raw data, transform it into a meaningful format, and load it into a suitable location for further processing.\nLet\'s deep dive into ETL pipeline\nETL stands for Extract, Transform, and Load. These pipelines can be executed periodically (e.g., daily or hourly) to fetch real-time data, enhancing the predictive power of our Machine Learning model for real-world applications.\nThis process can be automated using tools like Apache Airflow, Kubeflow Pipelines, AWS Step Functions, and more, streamlining the workflow for consistent and efficient data updates.\n\nETL Pipeline:\nETL Pipeline\nExtract Operation\nThe extract operation is responsible for fetching data from various sources such as websites, databases, and APIs, as illustrated in the flowchart above.\n\nTransform Operation\nThe transform operation focuses on data cleaning, feature selection, and manipulation. In my project, I used this step to extract only the required features, ensuring the data is ready for the next phase.\n\nLoad Operation\nThe load operation transfers the transformed data to its destination, where we can choose the appropriate storage format. Typically, the processed data is stored in a database for further analysis and model training.\n\nFor my project, I used PySpark to build the ETL pipeline, as it enables efficient processing of large datasets.\nWhy not Pandas?\nWhile Pandas is excellent for small to medium-sized datasets, it stores DataFrames in RAM, which can lead to out-of-memory exceptions when handling large datasets.\nIn contrast, PySpark creates a session and processes data in chunks, storing it in ROM, making it ideal for handling large-scale data.\nFor the actual implementation of the pipeline using PySpark, please refer to the accompanying jupyter notebook.\nEDA (Data Analysis)\nData understanding is a critical stage before building any Machine Learning model. It allows us to analyze the data, plan the data processing steps, and gain insights into its structure and quality.\nIn this project, I examined the balance of data in my training and testing datasets and found it to be well-distributed across all four categories.\n\nBalanceness Checking on train and test data\nThroughout the entire process, I focused on two columns:\nTweets\nSentiments\nThe Tweets column contains the raw Twitter text data, while the Sentiments column serves as the target variable for prediction.\nNatural Language Processing\nI applied several key NLP techniques to preprocess the data and prepare it for Machine Learning model building. Below are the main steps I executed:\n\nText Processing\nRegex: I applied regular expressions to clean the text by removing URLs, hashtags, HTML tags, and keeping only alphanumeric characters. This helped eliminate unnecessary noise from the data.\nNLTK: Using the Natural Language Toolkit (NLTK), I performed word tokenization, stemming and lemmatization.\nWord tokenization is just splitting sentence into words, so we can processing each word from sentence individually.\nStemming helps us to truncate prefix or suffix of text to reduce count on unique words from corpus (paragraph).\nLemmatization is the proecss of converting any word into it\'s base word, for eg. Played will convert to play.\n\nWordCloud \nWord cloud concept help you to understand importance of words from given data. I splitted my data into four sections.\ndata for negative sentiments\ndata for positive sentiments\ndata for neutral sentiments\ndata for irrelevant sentiments\n\nHere is the representation of most frequent words for each category.\nCategorywise Word Cloud Presentation\nHere you can clearly see that there is no much difference in negative and positive sentiments data.\nthis is representation of bad data, here we can filterout our data for further processing, it might reduce data but you can do data augmentation techniques here to increase your data.\naugmenting of data in NLP with TF-IDF will not bad idea because TF-IDF and any other porcessing technique that I used is not able to detect sentence grammer or it doesn,t require sophisticated text. \nYou can think our input text will work as bag of word for model. you can understand by refering below vectorization technique.\nVectorization with TF-IDF \nI used Term Frequency-Inverse Document Frequency (TF-IDF) for text vectorization. This method transforms text into a numerical format, considering the importance of each word across documents, which prevents frequent words from dominating the model.\n\nTF-IDF\nTF-IDF stands for Term Frequency-Inverse Document Frequency. \nTerm Frequency (TF): This measures how frequently a term (t) appears in a specific document (d). It\'s calculated by dividing the number of occurrences of the term in the document by the total number of terms in that document.\nInverse Document Frequency (IDF): This measures how important a term is across the entire collection of documents. It\'s calculated by taking the logarithm of the ratio of the total number of documents (N) to the number of documents containing the term (df(t)). A higher IDF value indicates a rarer term, making it more significant.\nHere is the resulant data we got from TF-IDF\nAfter all preprocessing of text it\'s time to save our data for mlflow experiments. To see actual implementation of end to end process of NLP you can refer jupyter notebook.\nmlflow experiments\nMLflow experiments allow you to conduct multiple experiments with your trained model, helping you track and compare results over time. For running MLflow experiments, I prefer using Databricks as it offers an integrated experiment section, making the process much more streamlined and efficient.\nIn Databricks, you can easily connect your experiment by passing the experiment ID into the MLflow code. This integration simplifies the entire workflow, enabling better experiment tracking and easier comparison of model performance.\nFor my experiments, I worked with several models, including Logistic Regression, Multinomial Naive Bayes, and Decision Tree Classifier. By applying different combinations of parameters, I was able to experiment with and compare the performance of each model. Here\'s a preview of the Logistic Regression model’s F1 score, which highlights the model\'s ability to balance precision and recall:\nLogistic Regression F1 Score: 0.58\nThis approach allowed me to track the effectiveness of each model and make adjustments as needed for improving performance.\n\nF1 score with different parameters (Logistic Regression)\nI recommend running the notebook below in your own account to see the results. You\'ll definitely start appreciating the power of MLflow.\nFor more information, please refer to the accompanying Databricks notebook.\n\nHyperparameter tuning\nHyperparameter tuning is crucial for identifying the best parameter combination for your model. This technique involves an iterative process where, for every parameter combination, the model is trained and tested on a dataset. It is often described as a "trial and error" method.\nHowever, this approach can be computationally expensive, especially when working with complex or heavy machine learning models. For large-scale problems, hyperparameter tuning can be made more efficient through sampling or batch methods. In these methods, you don\'t use the entire dataset for training the model; instead, you choose random or stratified data points from the dataset to train the model. Although this may slightly reduce accuracy, it is more feasible when working with large datasets.\nFor my project, I used Grid Search CV to find the best hyperparameter combination for the model. Below are some common techniques for hyperparameter tuning, especially for large datasets:\nGrid Search This technique exhaustively searches through a specified set of hyperparameter values, trying all possible combinations. While effective, it can be computationally expensive for larger datasets due to the exhaustive nature of the search.\nRandom Search Randomly samples from the hyperparameter space, offering a faster alternative to grid search. This method explores a wider range of hyperparameters with fewer evaluations, making it more efficient for larger datasets.\nBayesian Optimization This method uses probabilistic models to predict the performance of different hyperparameters. It selects the next set of hyperparameters to evaluate based on previous results, making it more efficient and suitable for large datasets.\nGenetic Algorithms Inspired by natural selection, these algorithms iteratively evolve a population of hyperparameter sets to improve model performance. This method works well with complex search spaces.\nHyperband Hyperband combines random search with early stopping to dynamically allocate resources across multiple configurations, identifying promising hyperparameters quickly without excessive computational costs.\nBayesian Optimization with Gaussian Processes This technique models the hyperparameter search space using Gaussian processes, focusing on regions that are likely to yield better results, which is particularly useful for large datasets where computational resources are limited.\n\nTo optimize hyperparameter tuning for larger datasets, these techniques can be combined with parallel computing and distributed processing frameworks such as Dask, Spark, or multi-GPU setups. This enables more efficient hyperparameter search and reduces the overall computational overhead.\nI choosed Logistc Regression model and trained my Model with best parameters.\nSource Distribution for Model Packaging\nPackaging your machine learning model is a best practice, especially if you don’t plan to update it frequently. Imagine thousands of lines of code that can now be utilized with just a single line—this is the power of model packaging.\n\n1. Project Folder Setup\nTo ensure better organization, I created a main folder called sentiment_prediction and moved all machine learning pipeline files and dependencies into this folder. This helped in maintaining a clean structure and simplified the management of the entire project.\nBefore moving forward I recommend you to visit this pdf it will practically show you step by step process for building python package.\nPDF : step by step guid for python package building\n\n2. Manifest.in\nThe Manifest.in file plays a crucial role in controlling which files and folders should be included or excluded during the packaging process. It helps to specify the structure of the package for distribution.\nKey commands used in the Manifest.in file include:\ninclude <file/folder>: Include specific files or folders.\nexclude <file/folder>: Exclude specific files or folders.\nrecursive-include <path>: Include all files from a directory recursively.\nrecursive-exclude <path>: Exclude all files from a directory recursively.\n3. Setup.py\nThe setup.py file contains the project\'s metadata and is essential for creating the package. It defines key information about the project, such as:\nProject name, version, description, and author details.\nDependencies required for the package (install_requires), making it easy to install all necessary libraries.\n\n4. Building the Package\nTo build the package, I used the following command:\n\npython setup.py sdist bdist_wheel\nThis command generates two folders:\n\nbuild/: Contains the entire project package as defined in the Manifest.in.\ndist/: Contains the distributable files: .whl (wheel file) .gz (compressed source archive)\n\n5. Global Access via GitHub\nNow you can access your package gloablly, by refering your repository. I provided my package below go and check out.\n\nRepository: GitHub Repo Link\nYou can install the package directly from GitHub using the following command:\n\npip install git+https://github.com/vijaytakbhate2002/sentiment_prediction_python_package.git \nTo ensure it worked globally, I tested it again:\n\nfrom sentiment_prediction import predict\nprint(predict.predictor("Great progress shared today!")) \noutput:[\'Negative\']\nFlask Application \nBuilding application will help us to give our NLP model experience to people, so I built one flask application.\nWeb Application UI\n\nHere is demonstration of project: project demo\nI left a blank section for user suggestion and feedback, these feedbacks are getting stored in database for future model analysis or any business work.\nDatabase configuration\nFor storing collected user data we need to configure a database. It will help us to improve model as per user need.\nI used Google Cloud MySQL instance for integrating my application with database, GCP is paid but you can use free credit of GCP for first 3 months, for doing almost all Cloud Work.\nYou need to create your GCP account, then create one instance under SQL and by configuring your local system with instance you are good to go.\nDocker containerization\nIf you are not awared about docker and it\'s basic concepts you can refer my previous article which explains you all about docker.\nThis guide will help you build a solid foundation in Docker, enabling you to confidently use it for your projects.\nDocker guide: Comprehensive Docker guide for deploying Flas app\nDeploy\nDeployment of web app will help us to engange people and provide them real actual experience of our services.\nAfter deployment you need to collect user data and store it for future analysis, this data contain user feedback and suggestions.\nAfter deployment it\'s not end of the process we need to collect user feedback and again follow same steps fine tune Model as per user need.\n\nSummary of Blog\nThis blog covers the lifecycle of a Machine Learning project, from ETL to deployment. It details building an ETL pipeline using PySpark for efficient data handling, EDA, and NLP preprocessing techniques like tokenization, TF-IDF, and WordCloud visualization. \nIt highlights ML experiments with MLflow on Databricks and hyperparameter tuning using Grid Search.\nThe model was packaged into a Python package and deployed as a Flask application with a database backend (Google Cloud MySQL) and Dockerized for scalability. \nThe app collects user feedback for continuous improvement. It emphasizes end-to-end integration, including cloud and containerization, to deliver a robust ML solution.\nHappy Learning!\n\n'}, {'content': '\nLanguages I Speak\nEnglish, Marathi, Hindi\n\n'}, {'content': '\n\nSoft skills:\nCritical Thinking, Intellectual Rigor, Problem Solving, Understanding Business Needs'}]
2024-11-25 21:51:46 - root - INFO - Document store already contains 14 documents. Skipping write.
2024-11-25 21:51:46 - haystack.modeling.utils - INFO - Using devices: CPU - Number of GPUs: 0
2024-11-25 21:51:46 - haystack.modeling.utils - INFO - Using devices: CPU - Number of GPUs: 0
2024-11-25 21:51:46 - haystack.modeling.model.language_model - INFO -  * LOADING MODEL: 'deepset/roberta-base-squad2' (Roberta)
2024-11-25 21:51:47 - haystack.modeling.model.language_model - INFO - Auto-detected model language: english
2024-11-25 21:51:47 - haystack.modeling.model.language_model - INFO - Loaded 'deepset/roberta-base-squad2' (Roberta model) from model hub.
2024-11-25 21:51:48 - haystack.modeling.utils - INFO - Using devices: CPU - Number of GPUs: 0
2024-11-25 21:52:26 - root - INFO - Reading text document
2024-11-25 21:52:26 - root - INFO - loaded document = [{'content': '\nPersonal Information\n\nName: Vijay Takbhate\nEmail: vijaytakbhate20@gmail.com Phone: 8767363681\nGitHub: https://github.com/vijaytakbhate2002\nLinkedIn: https://www.linkedin.com/in/vijay-takbhate-b9231a236/ Kaggle: https://www.kaggle.com/vijay20213\n\n'}, {'content': '\n\nExperience - Fox Solutions Pvt. Ltd.\nRole: Automation Engineer Duration: Jun 2024 - Oct 2024 Location: Maharashtra\nKey Contributions:\n-Completed 2 months of internship plus 4 months of full-time work.\n-Worked with PLC and SCADA systems, focusing on automating processes and optimizing operational efficiency.\n-Collaborated with cross-functional teams to implement automation solutions for industrial applications.\n\n'}, {'content': '\n\nExperience - Cei Design Consultancy Pvt. Ltd.\nRole: Python Developer Intern Duration: Aug 2024 - Sept 2024 Location: Remote, Maharashtra\nKey Contributions:\n-Specialized in data processing using Python and Excel.\n-Utilized OpenCV for image processing tasks.\n\n'}, {'content': '\n\nExperience - Ujucode\nRole: Subject Matter Expert Intern Duration: Aug 2023 - Oct 2023 Location: Remote, Maharashtra\nKey Contributions:\n-Contributed as a Python developer for a ChatBot project.\n-Handled backend development tasks and researched Python modules.\n\n'}, {'content': '\n\nProject - Twitter Post Sentiment Prediction\nDetails:\n-Engineered an ETL pipeline using PySpark and SQL.\n-Conducted sentiment analysis using NLP (TF-IDF) and optimized hyperparameters.\n-Monitored model performance through MLFlow on Databricks.\n-Leveraged Google Cloud Storage and MySQL for data management.\n-Deployed the model using Docker and hosted it on Render.\n\n'}, {'content': '\n\nProject - Text-Text Chat-Bot\nDetails:\n-Designed an advanced Chat-Bot using the NVIDIA API and prompt engineering.\n-Features include paraphrasing, grammar correction, AI detection, plagiarism checking, and content summarization.\n-Targeted at content creators, researchers, and businesses.\n-Technologies Used: HTML, CSS, Python Flask, Cloud Database, and Render.\n\n'}, {'content': '\n\nProject - Hand Gesture Recognition\nDetails:\n-Used Google-s MediaPipe framework for detecting hand landmarks and gestures.\n-Created and labeled a custom dataset of hand gestures for training.\n-Developed a Streamlit application to improve accessibility and flexibility.\n\n'}, {'content': '\n\nTechnical Skills\nLanguages: MySQL, Python, HTML, CSS\nTechnologies: Streamlit, Flask, VS Code, GitHub, MLflow, Docker, PySpark, Databricks, Google Cloud Platform\n\n'}, {'content': '\n\nCertification\nMLOps Bootcamp: Mastering AI Operations for Success (Jun 2024)\n-Learned about the MLOps lifecycle and modular programming.\n-Acquired skills in Git, Python, Flask, and MLflow.\n\n'}, {'content': '\n\nEducation Details:\nBachelor of Technology in Electronics and Telecommunication (May 2024) Institution: SVERI-s College of Engineering, Pandharpur, Maharashtra Score: 81.71\nDiploma in Electronics and Telecommunication (May 2021) Institution: SVERI-s College of Engineering, Pandharpur, Maharashtra Score: 91.73\n\n'}, {'content': "\n\nBlogging\n\n\nSupervised, Unsupervised, and Beyond: ML Techniques Simplified\nNovember 25, 2024\nThere are several techniques for ML training. Among these, I will cover the following:\n\nSupervised and Unsupervised Learning\nSemi-Supervised Learning\nOffline and Online Learning\nInstance-Based and Model-Based Learning\n\nSupervised and Unsupervised Learning\nSupervised learning is like spoon-feeding our ML model in its initial stages, allowing it to learn and improve over time. Here, I’m referring to the training process.\n\nIn supervised learning, there are input columns and output columns, also called target columns. For example, in spam detection—a classification problem—the input is the email, and the target is whether the email is spam or not. That’s it!\nThis process resembles a student-teacher scenario where the teacher is a human, and the student is the model. The dataset serves as the knowledge used to train the student (model)\nSee content credentials\n\nhumand and model\nIn Unsupervised learning, we are not aware of the data labels. Instead, we separate the input data by analyzing similarities and grouping them into different clusters.\nOnce the clusters are formed, we can assign custom labels to each one. This technique is widely used to identify product relationships in online shopping and to recommend new products based on a user’s purchase history.\n\nUnsupervised Learning Clusters\nHere three clusters with three different image categories are formed\n\nSemi-Supervised Learning\nSemi-Supervised Learning is a combination of supervised and unsupervised learning, where some data is labeled and some data is unlabeled. A good example of this is Google Photos, which automatically separates new photos into their respective groups based on whether they contain a particular person in each image.\n\nThere are several techniques under semi-supervised learning; here, we will focus on the following:\nSelf Learning\nConsistency Regularization\nGenerative Models\nGraph-Based Learning\n\nLet's discuss them one by one:\n\nSelf Learning\nSelf Learning trains a model with labeled data and generates pseudo-labels for the unlabeled data. \n\nSelf learning\nThen, the model is trained on the entire dataset, which includes both the generated pseudo-labels and the labeled data.\n\nConsistency Regularization\nThis technique uses data augmentation to generate similar data, and then the model is enforced to predict the same outcome for both the augmented and original data. It helps create a model that can find similarities in both labeled and unlabeled data, predicting the same output for unlabeled data as it would for labeled data.\n\nData augmentation includes techniques like image flipping, blurring, rotation, etc. After augmenting the data, the model is trained to predict the same class for both the augmented and original images.\n\nGenerative Models\nGenerative models create synthetic data points and learn the underlying structure of the training data. These models can generate new datasets using encoders. \n\nExamples include Generative Adversarial Networks (GANs) and Variational Autoencoders (VAEs).\n\nGraph-Based Learning\nThis technique creates nodes for both labeled and unlabeled data. Similar nodes are then connected to each other.\nFor example, when dealing with images, each image starts as a separate node. If similar images are found based on their features, they are grouped or connected by an edge.\nRefer below graph to understand graph-based learning\nGraph based learning\nHere label 1 is came from labeled image of group 1, simillarly for label 2 and label 3. Finally we came up with three labels that means labeled data has three unique labels.\nBatch (Offline) and Online Learning\nBatch Learning\nBatch learning, also known as offline learning, is a technique where the developer needs to stop the deployed ML model, retrain it on new data, and then redeploy it. \nThis technique is useful when frequent updates are not required. For example, product recommendation systems can often be retrained weekly or even over several days.\nBatch learning can be costly when training the model on the entire dataset every time. However, using incremental learning models can help avoid unnecessary retraining, making the process more efficient.\nHere are some examples on incremental learning models, as of now we are focusing on general concepts so don't go much deeper into these types.\nStochastic Gradient Descent (SGD)\nNaive Bayes (Online Version)\nIncremental Support Vector Machines (ISVM)\nOnline Random Forests\nIncremental Decision Trees\nYou can refer this article to learn more about incremental learning\nHere’s your text with grammatical corrections and improved clarity:\nOnline Learning\nOnline learning is used when an ML model needs to be updated continuously with new inputs, such as in stock price prediction. In this scenario, the model must remain aware of the most recent data.\n\nBatch and online learning\nOnly models capable of incremental learning are used in online learning. Each new instance is fed back into the model to update its internal weights based on the latest input.\n\nInstance-Based and Model-Based Learning\nInstance-Based Learning\nInstance-based learning involves comparing a new input with stored data. If similarity is found, the model returns the label of the corresponding input. This approach uses pattern matching techniques. \nHere model works as a search engin not exactly same but it will find simmilarity from stored data.\nSome common models used in instance-based learning include:\nK-Nearest Neighbors (K-NN) with labels\nLocally Weighted Regression, etc.\nModel-Based Learning\nIn model-based learning, we train the model using a training dataset. During the training process, the model creates its own complex patterns (mathematical equations) to make predictions for future inputs. \nA simple example is the equation y=mx+cy = mx + c, which represents a trained model. Some common models used in model-based learning include:\nLinear Regression\nLogistic Regression\nDecision Tree, etc.\n\nSummary\nMachine Learning techniques can be broadly categorized into supervised, unsupervised, and semi-supervised learning. \nSupervised learning uses labeled data, like spam detection, while unsupervised learning works with unlabeled data, grouping similar points into clusters. \nSemi-supervised learning combines both, employing techniques like self-learning and consistency regularization to leverage partially labeled datasets.\nOther approaches include batch learning, where models are retrained periodically, and online learning, which updates models continuously with new data, suitable for dynamic tasks like stock price prediction. \nInstance-based learning relies on pattern matching (e.g., K-Nearest Neighbors), while model-based learning creates mathematical models (e.g., Linear Regression) to make predictions.\nThis is all about Machine Learning techniques. If you learned something, let me know in the comments. Your suggestions will help me improve my blogs.\nThanks for reading!\n\n"}, {'content': '\nBlogging\n\nMastering the End-to-End Machine Learning Lifecycle: From Data to Deployment\n\nNovember 24, 2024\n This article takes you through the complete lifecycle of a Machine Learning project. From ETL to deployment, I’ll share every detail of how I brought this project to life.\nData Engineering\nData Engineering serves as the starting point in the Machine Learning project lifecycle, bringing all distributed data together in one place.\nFor this project, I utilized an ETL pipeline—a core concept in Data Engineering. It enabled me to extract raw data, transform it into a meaningful format, and load it into a suitable location for further processing.\nLet\'s deep dive into ETL pipeline\nETL stands for Extract, Transform, and Load. These pipelines can be executed periodically (e.g., daily or hourly) to fetch real-time data, enhancing the predictive power of our Machine Learning model for real-world applications.\nThis process can be automated using tools like Apache Airflow, Kubeflow Pipelines, AWS Step Functions, and more, streamlining the workflow for consistent and efficient data updates.\n\nETL Pipeline:\nETL Pipeline\nExtract Operation\nThe extract operation is responsible for fetching data from various sources such as websites, databases, and APIs, as illustrated in the flowchart above.\n\nTransform Operation\nThe transform operation focuses on data cleaning, feature selection, and manipulation. In my project, I used this step to extract only the required features, ensuring the data is ready for the next phase.\n\nLoad Operation\nThe load operation transfers the transformed data to its destination, where we can choose the appropriate storage format. Typically, the processed data is stored in a database for further analysis and model training.\n\nFor my project, I used PySpark to build the ETL pipeline, as it enables efficient processing of large datasets.\nWhy not Pandas?\nWhile Pandas is excellent for small to medium-sized datasets, it stores DataFrames in RAM, which can lead to out-of-memory exceptions when handling large datasets.\nIn contrast, PySpark creates a session and processes data in chunks, storing it in ROM, making it ideal for handling large-scale data.\nFor the actual implementation of the pipeline using PySpark, please refer to the accompanying jupyter notebook.\nEDA (Data Analysis)\nData understanding is a critical stage before building any Machine Learning model. It allows us to analyze the data, plan the data processing steps, and gain insights into its structure and quality.\nIn this project, I examined the balance of data in my training and testing datasets and found it to be well-distributed across all four categories.\n\nBalanceness Checking on train and test data\nThroughout the entire process, I focused on two columns:\nTweets\nSentiments\nThe Tweets column contains the raw Twitter text data, while the Sentiments column serves as the target variable for prediction.\nNatural Language Processing\nI applied several key NLP techniques to preprocess the data and prepare it for Machine Learning model building. Below are the main steps I executed:\n\nText Processing\nRegex: I applied regular expressions to clean the text by removing URLs, hashtags, HTML tags, and keeping only alphanumeric characters. This helped eliminate unnecessary noise from the data.\nNLTK: Using the Natural Language Toolkit (NLTK), I performed word tokenization, stemming and lemmatization.\nWord tokenization is just splitting sentence into words, so we can processing each word from sentence individually.\nStemming helps us to truncate prefix or suffix of text to reduce count on unique words from corpus (paragraph).\nLemmatization is the proecss of converting any word into it\'s base word, for eg. Played will convert to play.\n\nWordCloud \nWord cloud concept help you to understand importance of words from given data. I splitted my data into four sections.\ndata for negative sentiments\ndata for positive sentiments\ndata for neutral sentiments\ndata for irrelevant sentiments\n\nHere is the representation of most frequent words for each category.\nCategorywise Word Cloud Presentation\nHere you can clearly see that there is no much difference in negative and positive sentiments data.\nthis is representation of bad data, here we can filterout our data for further processing, it might reduce data but you can do data augmentation techniques here to increase your data.\naugmenting of data in NLP with TF-IDF will not bad idea because TF-IDF and any other porcessing technique that I used is not able to detect sentence grammer or it doesn,t require sophisticated text. \nYou can think our input text will work as bag of word for model. you can understand by refering below vectorization technique.\nVectorization with TF-IDF \nI used Term Frequency-Inverse Document Frequency (TF-IDF) for text vectorization. This method transforms text into a numerical format, considering the importance of each word across documents, which prevents frequent words from dominating the model.\n\nTF-IDF\nTF-IDF stands for Term Frequency-Inverse Document Frequency. \nTerm Frequency (TF): This measures how frequently a term (t) appears in a specific document (d). It\'s calculated by dividing the number of occurrences of the term in the document by the total number of terms in that document.\nInverse Document Frequency (IDF): This measures how important a term is across the entire collection of documents. It\'s calculated by taking the logarithm of the ratio of the total number of documents (N) to the number of documents containing the term (df(t)). A higher IDF value indicates a rarer term, making it more significant.\nHere is the resulant data we got from TF-IDF\nAfter all preprocessing of text it\'s time to save our data for mlflow experiments. To see actual implementation of end to end process of NLP you can refer jupyter notebook.\nmlflow experiments\nMLflow experiments allow you to conduct multiple experiments with your trained model, helping you track and compare results over time. For running MLflow experiments, I prefer using Databricks as it offers an integrated experiment section, making the process much more streamlined and efficient.\nIn Databricks, you can easily connect your experiment by passing the experiment ID into the MLflow code. This integration simplifies the entire workflow, enabling better experiment tracking and easier comparison of model performance.\nFor my experiments, I worked with several models, including Logistic Regression, Multinomial Naive Bayes, and Decision Tree Classifier. By applying different combinations of parameters, I was able to experiment with and compare the performance of each model. Here\'s a preview of the Logistic Regression model’s F1 score, which highlights the model\'s ability to balance precision and recall:\nLogistic Regression F1 Score: 0.58\nThis approach allowed me to track the effectiveness of each model and make adjustments as needed for improving performance.\n\nF1 score with different parameters (Logistic Regression)\nI recommend running the notebook below in your own account to see the results. You\'ll definitely start appreciating the power of MLflow.\nFor more information, please refer to the accompanying Databricks notebook.\n\nHyperparameter tuning\nHyperparameter tuning is crucial for identifying the best parameter combination for your model. This technique involves an iterative process where, for every parameter combination, the model is trained and tested on a dataset. It is often described as a "trial and error" method.\nHowever, this approach can be computationally expensive, especially when working with complex or heavy machine learning models. For large-scale problems, hyperparameter tuning can be made more efficient through sampling or batch methods. In these methods, you don\'t use the entire dataset for training the model; instead, you choose random or stratified data points from the dataset to train the model. Although this may slightly reduce accuracy, it is more feasible when working with large datasets.\nFor my project, I used Grid Search CV to find the best hyperparameter combination for the model. Below are some common techniques for hyperparameter tuning, especially for large datasets:\nGrid Search This technique exhaustively searches through a specified set of hyperparameter values, trying all possible combinations. While effective, it can be computationally expensive for larger datasets due to the exhaustive nature of the search.\nRandom Search Randomly samples from the hyperparameter space, offering a faster alternative to grid search. This method explores a wider range of hyperparameters with fewer evaluations, making it more efficient for larger datasets.\nBayesian Optimization This method uses probabilistic models to predict the performance of different hyperparameters. It selects the next set of hyperparameters to evaluate based on previous results, making it more efficient and suitable for large datasets.\nGenetic Algorithms Inspired by natural selection, these algorithms iteratively evolve a population of hyperparameter sets to improve model performance. This method works well with complex search spaces.\nHyperband Hyperband combines random search with early stopping to dynamically allocate resources across multiple configurations, identifying promising hyperparameters quickly without excessive computational costs.\nBayesian Optimization with Gaussian Processes This technique models the hyperparameter search space using Gaussian processes, focusing on regions that are likely to yield better results, which is particularly useful for large datasets where computational resources are limited.\n\nTo optimize hyperparameter tuning for larger datasets, these techniques can be combined with parallel computing and distributed processing frameworks such as Dask, Spark, or multi-GPU setups. This enables more efficient hyperparameter search and reduces the overall computational overhead.\nI choosed Logistc Regression model and trained my Model with best parameters.\nSource Distribution for Model Packaging\nPackaging your machine learning model is a best practice, especially if you don’t plan to update it frequently. Imagine thousands of lines of code that can now be utilized with just a single line—this is the power of model packaging.\n\n1. Project Folder Setup\nTo ensure better organization, I created a main folder called sentiment_prediction and moved all machine learning pipeline files and dependencies into this folder. This helped in maintaining a clean structure and simplified the management of the entire project.\nBefore moving forward I recommend you to visit this pdf it will practically show you step by step process for building python package.\nPDF : step by step guid for python package building\n\n2. Manifest.in\nThe Manifest.in file plays a crucial role in controlling which files and folders should be included or excluded during the packaging process. It helps to specify the structure of the package for distribution.\nKey commands used in the Manifest.in file include:\ninclude <file/folder>: Include specific files or folders.\nexclude <file/folder>: Exclude specific files or folders.\nrecursive-include <path>: Include all files from a directory recursively.\nrecursive-exclude <path>: Exclude all files from a directory recursively.\n3. Setup.py\nThe setup.py file contains the project\'s metadata and is essential for creating the package. It defines key information about the project, such as:\nProject name, version, description, and author details.\nDependencies required for the package (install_requires), making it easy to install all necessary libraries.\n\n4. Building the Package\nTo build the package, I used the following command:\n\npython setup.py sdist bdist_wheel\nThis command generates two folders:\n\nbuild/: Contains the entire project package as defined in the Manifest.in.\ndist/: Contains the distributable files: .whl (wheel file) .gz (compressed source archive)\n\n5. Global Access via GitHub\nNow you can access your package gloablly, by refering your repository. I provided my package below go and check out.\n\nRepository: GitHub Repo Link\nYou can install the package directly from GitHub using the following command:\n\npip install git+https://github.com/vijaytakbhate2002/sentiment_prediction_python_package.git \nTo ensure it worked globally, I tested it again:\n\nfrom sentiment_prediction import predict\nprint(predict.predictor("Great progress shared today!")) \noutput:[\'Negative\']\nFlask Application \nBuilding application will help us to give our NLP model experience to people, so I built one flask application.\nWeb Application UI\n\nHere is demonstration of project: project demo\nI left a blank section for user suggestion and feedback, these feedbacks are getting stored in database for future model analysis or any business work.\nDatabase configuration\nFor storing collected user data we need to configure a database. It will help us to improve model as per user need.\nI used Google Cloud MySQL instance for integrating my application with database, GCP is paid but you can use free credit of GCP for first 3 months, for doing almost all Cloud Work.\nYou need to create your GCP account, then create one instance under SQL and by configuring your local system with instance you are good to go.\nDocker containerization\nIf you are not awared about docker and it\'s basic concepts you can refer my previous article which explains you all about docker.\nThis guide will help you build a solid foundation in Docker, enabling you to confidently use it for your projects.\nDocker guide: Comprehensive Docker guide for deploying Flas app\nDeploy\nDeployment of web app will help us to engange people and provide them real actual experience of our services.\nAfter deployment you need to collect user data and store it for future analysis, this data contain user feedback and suggestions.\nAfter deployment it\'s not end of the process we need to collect user feedback and again follow same steps fine tune Model as per user need.\n\nSummary of Blog\nThis blog covers the lifecycle of a Machine Learning project, from ETL to deployment. It details building an ETL pipeline using PySpark for efficient data handling, EDA, and NLP preprocessing techniques like tokenization, TF-IDF, and WordCloud visualization. \nIt highlights ML experiments with MLflow on Databricks and hyperparameter tuning using Grid Search.\nThe model was packaged into a Python package and deployed as a Flask application with a database backend (Google Cloud MySQL) and Dockerized for scalability. \nThe app collects user feedback for continuous improvement. It emphasizes end-to-end integration, including cloud and containerization, to deliver a robust ML solution.\nHappy Learning!\n\n'}, {'content': '\nLanguages I Speak\nEnglish, Marathi, Hindi\n\n'}, {'content': '\n\nSoft skills:\nCritical Thinking, Intellectual Rigor, Problem Solving, Understanding Business Needs'}]
2024-11-25 21:52:26 - root - INFO - Document store already contains 14 documents. Skipping write.
2024-11-25 21:52:26 - haystack.modeling.utils - INFO - Using devices: CPU - Number of GPUs: 0
2024-11-25 21:52:26 - haystack.modeling.utils - INFO - Using devices: CPU - Number of GPUs: 0
2024-11-25 21:52:26 - haystack.modeling.model.language_model - INFO -  * LOADING MODEL: 'deepset/minilm-uncased-squad2' (Bert)
2024-11-25 21:52:27 - haystack.modeling.model.language_model - INFO - Auto-detected model language: english
2024-11-25 21:52:27 - haystack.modeling.model.language_model - INFO - Loaded 'deepset/minilm-uncased-squad2' (Bert model) from model hub.
2024-11-25 21:52:28 - haystack.modeling.utils - INFO - Using devices: CPU - Number of GPUs: 0
2024-11-25 21:53:52 - root - INFO - Loading existing FAISS document store...
2024-11-25 21:53:52 - haystack.modeling.utils - INFO - Using devices: CPU - Number of GPUs: 0
2024-11-25 21:53:53 - haystack.modeling.model.language_model - WARNING - Using a model of type 'mpnet' which might be incompatible with DPR encoders. Only Bert-based encoders are supported. They need input_ids, token_type_ids, attention_mask as input tensors.
2024-11-25 21:53:57 - haystack.modeling.model.language_model - INFO - Auto-detected model language: english
2024-11-25 21:53:58 - haystack.modeling.model.language_model - WARNING - Using a model of type 'mpnet' which might be incompatible with DPR encoders. Only Bert-based encoders are supported. They need input_ids, token_type_ids, attention_mask as input tensors.
2024-11-25 21:54:01 - haystack.modeling.model.language_model - INFO - Auto-detected model language: english
2024-11-25 21:54:01 - root - INFO - Reading text document
2024-11-25 21:54:01 - root - INFO - loaded document = [{'content': '\nPersonal Information\n\nName: Vijay Takbhate\nEmail: vijaytakbhate20@gmail.com Phone: 8767363681\nGitHub: https://github.com/vijaytakbhate2002\nLinkedIn: https://www.linkedin.com/in/vijay-takbhate-b9231a236/ Kaggle: https://www.kaggle.com/vijay20213\n\n'}, {'content': '\n\nExperience - Fox Solutions Pvt. Ltd.\nRole: Automation Engineer Duration: Jun 2024 - Oct 2024 Location: Maharashtra\nKey Contributions:\n-Completed 2 months of internship plus 4 months of full-time work.\n-Worked with PLC and SCADA systems, focusing on automating processes and optimizing operational efficiency.\n-Collaborated with cross-functional teams to implement automation solutions for industrial applications.\n\n'}, {'content': '\n\nExperience - Cei Design Consultancy Pvt. Ltd.\nRole: Python Developer Intern Duration: Aug 2024 - Sept 2024 Location: Remote, Maharashtra\nKey Contributions:\n-Specialized in data processing using Python and Excel.\n-Utilized OpenCV for image processing tasks.\n\n'}, {'content': '\n\nExperience - Ujucode\nRole: Subject Matter Expert Intern Duration: Aug 2023 - Oct 2023 Location: Remote, Maharashtra\nKey Contributions:\n-Contributed as a Python developer for a ChatBot project.\n-Handled backend development tasks and researched Python modules.\n\n'}, {'content': '\n\nProject - Twitter Post Sentiment Prediction\nDetails:\n-Engineered an ETL pipeline using PySpark and SQL.\n-Conducted sentiment analysis using NLP (TF-IDF) and optimized hyperparameters.\n-Monitored model performance through MLFlow on Databricks.\n-Leveraged Google Cloud Storage and MySQL for data management.\n-Deployed the model using Docker and hosted it on Render.\n\n'}, {'content': '\n\nProject - Text-Text Chat-Bot\nDetails:\n-Designed an advanced Chat-Bot using the NVIDIA API and prompt engineering.\n-Features include paraphrasing, grammar correction, AI detection, plagiarism checking, and content summarization.\n-Targeted at content creators, researchers, and businesses.\n-Technologies Used: HTML, CSS, Python Flask, Cloud Database, and Render.\n\n'}, {'content': '\n\nProject - Hand Gesture Recognition\nDetails:\n-Used Google-s MediaPipe framework for detecting hand landmarks and gestures.\n-Created and labeled a custom dataset of hand gestures for training.\n-Developed a Streamlit application to improve accessibility and flexibility.\n\n'}, {'content': '\n\nTechnical Skills\nLanguages: MySQL, Python, HTML, CSS\nTechnologies: Streamlit, Flask, VS Code, GitHub, MLflow, Docker, PySpark, Databricks, Google Cloud Platform\n\n'}, {'content': '\n\nCertification\nMLOps Bootcamp: Mastering AI Operations for Success (Jun 2024)\n-Learned about the MLOps lifecycle and modular programming.\n-Acquired skills in Git, Python, Flask, and MLflow.\n\n'}, {'content': '\n\nEducation Details:\nBachelor of Technology in Electronics and Telecommunication (May 2024) Institution: SVERI-s College of Engineering, Pandharpur, Maharashtra Score: 81.71\nDiploma in Electronics and Telecommunication (May 2021) Institution: SVERI-s College of Engineering, Pandharpur, Maharashtra Score: 91.73\n\n'}, {'content': "\n\nBlogging\n\n\nSupervised, Unsupervised, and Beyond: ML Techniques Simplified\nNovember 25, 2024\nThere are several techniques for ML training. Among these, I will cover the following:\n\nSupervised and Unsupervised Learning\nSemi-Supervised Learning\nOffline and Online Learning\nInstance-Based and Model-Based Learning\n\nSupervised and Unsupervised Learning\nSupervised learning is like spoon-feeding our ML model in its initial stages, allowing it to learn and improve over time. Here, I’m referring to the training process.\n\nIn supervised learning, there are input columns and output columns, also called target columns. For example, in spam detection—a classification problem—the input is the email, and the target is whether the email is spam or not. That’s it!\nThis process resembles a student-teacher scenario where the teacher is a human, and the student is the model. The dataset serves as the knowledge used to train the student (model)\nSee content credentials\n\nhumand and model\nIn Unsupervised learning, we are not aware of the data labels. Instead, we separate the input data by analyzing similarities and grouping them into different clusters.\nOnce the clusters are formed, we can assign custom labels to each one. This technique is widely used to identify product relationships in online shopping and to recommend new products based on a user’s purchase history.\n\nUnsupervised Learning Clusters\nHere three clusters with three different image categories are formed\n\nSemi-Supervised Learning\nSemi-Supervised Learning is a combination of supervised and unsupervised learning, where some data is labeled and some data is unlabeled. A good example of this is Google Photos, which automatically separates new photos into their respective groups based on whether they contain a particular person in each image.\n\nThere are several techniques under semi-supervised learning; here, we will focus on the following:\nSelf Learning\nConsistency Regularization\nGenerative Models\nGraph-Based Learning\n\nLet's discuss them one by one:\n\nSelf Learning\nSelf Learning trains a model with labeled data and generates pseudo-labels for the unlabeled data. \n\nSelf learning\nThen, the model is trained on the entire dataset, which includes both the generated pseudo-labels and the labeled data.\n\nConsistency Regularization\nThis technique uses data augmentation to generate similar data, and then the model is enforced to predict the same outcome for both the augmented and original data. It helps create a model that can find similarities in both labeled and unlabeled data, predicting the same output for unlabeled data as it would for labeled data.\n\nData augmentation includes techniques like image flipping, blurring, rotation, etc. After augmenting the data, the model is trained to predict the same class for both the augmented and original images.\n\nGenerative Models\nGenerative models create synthetic data points and learn the underlying structure of the training data. These models can generate new datasets using encoders. \n\nExamples include Generative Adversarial Networks (GANs) and Variational Autoencoders (VAEs).\n\nGraph-Based Learning\nThis technique creates nodes for both labeled and unlabeled data. Similar nodes are then connected to each other.\nFor example, when dealing with images, each image starts as a separate node. If similar images are found based on their features, they are grouped or connected by an edge.\nRefer below graph to understand graph-based learning\nGraph based learning\nHere label 1 is came from labeled image of group 1, simillarly for label 2 and label 3. Finally we came up with three labels that means labeled data has three unique labels.\nBatch (Offline) and Online Learning\nBatch Learning\nBatch learning, also known as offline learning, is a technique where the developer needs to stop the deployed ML model, retrain it on new data, and then redeploy it. \nThis technique is useful when frequent updates are not required. For example, product recommendation systems can often be retrained weekly or even over several days.\nBatch learning can be costly when training the model on the entire dataset every time. However, using incremental learning models can help avoid unnecessary retraining, making the process more efficient.\nHere are some examples on incremental learning models, as of now we are focusing on general concepts so don't go much deeper into these types.\nStochastic Gradient Descent (SGD)\nNaive Bayes (Online Version)\nIncremental Support Vector Machines (ISVM)\nOnline Random Forests\nIncremental Decision Trees\nYou can refer this article to learn more about incremental learning\nHere’s your text with grammatical corrections and improved clarity:\nOnline Learning\nOnline learning is used when an ML model needs to be updated continuously with new inputs, such as in stock price prediction. In this scenario, the model must remain aware of the most recent data.\n\nBatch and online learning\nOnly models capable of incremental learning are used in online learning. Each new instance is fed back into the model to update its internal weights based on the latest input.\n\nInstance-Based and Model-Based Learning\nInstance-Based Learning\nInstance-based learning involves comparing a new input with stored data. If similarity is found, the model returns the label of the corresponding input. This approach uses pattern matching techniques. \nHere model works as a search engin not exactly same but it will find simmilarity from stored data.\nSome common models used in instance-based learning include:\nK-Nearest Neighbors (K-NN) with labels\nLocally Weighted Regression, etc.\nModel-Based Learning\nIn model-based learning, we train the model using a training dataset. During the training process, the model creates its own complex patterns (mathematical equations) to make predictions for future inputs. \nA simple example is the equation y=mx+cy = mx + c, which represents a trained model. Some common models used in model-based learning include:\nLinear Regression\nLogistic Regression\nDecision Tree, etc.\n\nSummary\nMachine Learning techniques can be broadly categorized into supervised, unsupervised, and semi-supervised learning. \nSupervised learning uses labeled data, like spam detection, while unsupervised learning works with unlabeled data, grouping similar points into clusters. \nSemi-supervised learning combines both, employing techniques like self-learning and consistency regularization to leverage partially labeled datasets.\nOther approaches include batch learning, where models are retrained periodically, and online learning, which updates models continuously with new data, suitable for dynamic tasks like stock price prediction. \nInstance-based learning relies on pattern matching (e.g., K-Nearest Neighbors), while model-based learning creates mathematical models (e.g., Linear Regression) to make predictions.\nThis is all about Machine Learning techniques. If you learned something, let me know in the comments. Your suggestions will help me improve my blogs.\nThanks for reading!\n\n"}, {'content': '\nBlogging\n\nMastering the End-to-End Machine Learning Lifecycle: From Data to Deployment\n\nNovember 24, 2024\n This article takes you through the complete lifecycle of a Machine Learning project. From ETL to deployment, I’ll share every detail of how I brought this project to life.\nData Engineering\nData Engineering serves as the starting point in the Machine Learning project lifecycle, bringing all distributed data together in one place.\nFor this project, I utilized an ETL pipeline—a core concept in Data Engineering. It enabled me to extract raw data, transform it into a meaningful format, and load it into a suitable location for further processing.\nLet\'s deep dive into ETL pipeline\nETL stands for Extract, Transform, and Load. These pipelines can be executed periodically (e.g., daily or hourly) to fetch real-time data, enhancing the predictive power of our Machine Learning model for real-world applications.\nThis process can be automated using tools like Apache Airflow, Kubeflow Pipelines, AWS Step Functions, and more, streamlining the workflow for consistent and efficient data updates.\n\nETL Pipeline:\nETL Pipeline\nExtract Operation\nThe extract operation is responsible for fetching data from various sources such as websites, databases, and APIs, as illustrated in the flowchart above.\n\nTransform Operation\nThe transform operation focuses on data cleaning, feature selection, and manipulation. In my project, I used this step to extract only the required features, ensuring the data is ready for the next phase.\n\nLoad Operation\nThe load operation transfers the transformed data to its destination, where we can choose the appropriate storage format. Typically, the processed data is stored in a database for further analysis and model training.\n\nFor my project, I used PySpark to build the ETL pipeline, as it enables efficient processing of large datasets.\nWhy not Pandas?\nWhile Pandas is excellent for small to medium-sized datasets, it stores DataFrames in RAM, which can lead to out-of-memory exceptions when handling large datasets.\nIn contrast, PySpark creates a session and processes data in chunks, storing it in ROM, making it ideal for handling large-scale data.\nFor the actual implementation of the pipeline using PySpark, please refer to the accompanying jupyter notebook.\nEDA (Data Analysis)\nData understanding is a critical stage before building any Machine Learning model. It allows us to analyze the data, plan the data processing steps, and gain insights into its structure and quality.\nIn this project, I examined the balance of data in my training and testing datasets and found it to be well-distributed across all four categories.\n\nBalanceness Checking on train and test data\nThroughout the entire process, I focused on two columns:\nTweets\nSentiments\nThe Tweets column contains the raw Twitter text data, while the Sentiments column serves as the target variable for prediction.\nNatural Language Processing\nI applied several key NLP techniques to preprocess the data and prepare it for Machine Learning model building. Below are the main steps I executed:\n\nText Processing\nRegex: I applied regular expressions to clean the text by removing URLs, hashtags, HTML tags, and keeping only alphanumeric characters. This helped eliminate unnecessary noise from the data.\nNLTK: Using the Natural Language Toolkit (NLTK), I performed word tokenization, stemming and lemmatization.\nWord tokenization is just splitting sentence into words, so we can processing each word from sentence individually.\nStemming helps us to truncate prefix or suffix of text to reduce count on unique words from corpus (paragraph).\nLemmatization is the proecss of converting any word into it\'s base word, for eg. Played will convert to play.\n\nWordCloud \nWord cloud concept help you to understand importance of words from given data. I splitted my data into four sections.\ndata for negative sentiments\ndata for positive sentiments\ndata for neutral sentiments\ndata for irrelevant sentiments\n\nHere is the representation of most frequent words for each category.\nCategorywise Word Cloud Presentation\nHere you can clearly see that there is no much difference in negative and positive sentiments data.\nthis is representation of bad data, here we can filterout our data for further processing, it might reduce data but you can do data augmentation techniques here to increase your data.\naugmenting of data in NLP with TF-IDF will not bad idea because TF-IDF and any other porcessing technique that I used is not able to detect sentence grammer or it doesn,t require sophisticated text. \nYou can think our input text will work as bag of word for model. you can understand by refering below vectorization technique.\nVectorization with TF-IDF \nI used Term Frequency-Inverse Document Frequency (TF-IDF) for text vectorization. This method transforms text into a numerical format, considering the importance of each word across documents, which prevents frequent words from dominating the model.\n\nTF-IDF\nTF-IDF stands for Term Frequency-Inverse Document Frequency. \nTerm Frequency (TF): This measures how frequently a term (t) appears in a specific document (d). It\'s calculated by dividing the number of occurrences of the term in the document by the total number of terms in that document.\nInverse Document Frequency (IDF): This measures how important a term is across the entire collection of documents. It\'s calculated by taking the logarithm of the ratio of the total number of documents (N) to the number of documents containing the term (df(t)). A higher IDF value indicates a rarer term, making it more significant.\nHere is the resulant data we got from TF-IDF\nAfter all preprocessing of text it\'s time to save our data for mlflow experiments. To see actual implementation of end to end process of NLP you can refer jupyter notebook.\nmlflow experiments\nMLflow experiments allow you to conduct multiple experiments with your trained model, helping you track and compare results over time. For running MLflow experiments, I prefer using Databricks as it offers an integrated experiment section, making the process much more streamlined and efficient.\nIn Databricks, you can easily connect your experiment by passing the experiment ID into the MLflow code. This integration simplifies the entire workflow, enabling better experiment tracking and easier comparison of model performance.\nFor my experiments, I worked with several models, including Logistic Regression, Multinomial Naive Bayes, and Decision Tree Classifier. By applying different combinations of parameters, I was able to experiment with and compare the performance of each model. Here\'s a preview of the Logistic Regression model’s F1 score, which highlights the model\'s ability to balance precision and recall:\nLogistic Regression F1 Score: 0.58\nThis approach allowed me to track the effectiveness of each model and make adjustments as needed for improving performance.\n\nF1 score with different parameters (Logistic Regression)\nI recommend running the notebook below in your own account to see the results. You\'ll definitely start appreciating the power of MLflow.\nFor more information, please refer to the accompanying Databricks notebook.\n\nHyperparameter tuning\nHyperparameter tuning is crucial for identifying the best parameter combination for your model. This technique involves an iterative process where, for every parameter combination, the model is trained and tested on a dataset. It is often described as a "trial and error" method.\nHowever, this approach can be computationally expensive, especially when working with complex or heavy machine learning models. For large-scale problems, hyperparameter tuning can be made more efficient through sampling or batch methods. In these methods, you don\'t use the entire dataset for training the model; instead, you choose random or stratified data points from the dataset to train the model. Although this may slightly reduce accuracy, it is more feasible when working with large datasets.\nFor my project, I used Grid Search CV to find the best hyperparameter combination for the model. Below are some common techniques for hyperparameter tuning, especially for large datasets:\nGrid Search This technique exhaustively searches through a specified set of hyperparameter values, trying all possible combinations. While effective, it can be computationally expensive for larger datasets due to the exhaustive nature of the search.\nRandom Search Randomly samples from the hyperparameter space, offering a faster alternative to grid search. This method explores a wider range of hyperparameters with fewer evaluations, making it more efficient for larger datasets.\nBayesian Optimization This method uses probabilistic models to predict the performance of different hyperparameters. It selects the next set of hyperparameters to evaluate based on previous results, making it more efficient and suitable for large datasets.\nGenetic Algorithms Inspired by natural selection, these algorithms iteratively evolve a population of hyperparameter sets to improve model performance. This method works well with complex search spaces.\nHyperband Hyperband combines random search with early stopping to dynamically allocate resources across multiple configurations, identifying promising hyperparameters quickly without excessive computational costs.\nBayesian Optimization with Gaussian Processes This technique models the hyperparameter search space using Gaussian processes, focusing on regions that are likely to yield better results, which is particularly useful for large datasets where computational resources are limited.\n\nTo optimize hyperparameter tuning for larger datasets, these techniques can be combined with parallel computing and distributed processing frameworks such as Dask, Spark, or multi-GPU setups. This enables more efficient hyperparameter search and reduces the overall computational overhead.\nI choosed Logistc Regression model and trained my Model with best parameters.\nSource Distribution for Model Packaging\nPackaging your machine learning model is a best practice, especially if you don’t plan to update it frequently. Imagine thousands of lines of code that can now be utilized with just a single line—this is the power of model packaging.\n\n1. Project Folder Setup\nTo ensure better organization, I created a main folder called sentiment_prediction and moved all machine learning pipeline files and dependencies into this folder. This helped in maintaining a clean structure and simplified the management of the entire project.\nBefore moving forward I recommend you to visit this pdf it will practically show you step by step process for building python package.\nPDF : step by step guid for python package building\n\n2. Manifest.in\nThe Manifest.in file plays a crucial role in controlling which files and folders should be included or excluded during the packaging process. It helps to specify the structure of the package for distribution.\nKey commands used in the Manifest.in file include:\ninclude <file/folder>: Include specific files or folders.\nexclude <file/folder>: Exclude specific files or folders.\nrecursive-include <path>: Include all files from a directory recursively.\nrecursive-exclude <path>: Exclude all files from a directory recursively.\n3. Setup.py\nThe setup.py file contains the project\'s metadata and is essential for creating the package. It defines key information about the project, such as:\nProject name, version, description, and author details.\nDependencies required for the package (install_requires), making it easy to install all necessary libraries.\n\n4. Building the Package\nTo build the package, I used the following command:\n\npython setup.py sdist bdist_wheel\nThis command generates two folders:\n\nbuild/: Contains the entire project package as defined in the Manifest.in.\ndist/: Contains the distributable files: .whl (wheel file) .gz (compressed source archive)\n\n5. Global Access via GitHub\nNow you can access your package gloablly, by refering your repository. I provided my package below go and check out.\n\nRepository: GitHub Repo Link\nYou can install the package directly from GitHub using the following command:\n\npip install git+https://github.com/vijaytakbhate2002/sentiment_prediction_python_package.git \nTo ensure it worked globally, I tested it again:\n\nfrom sentiment_prediction import predict\nprint(predict.predictor("Great progress shared today!")) \noutput:[\'Negative\']\nFlask Application \nBuilding application will help us to give our NLP model experience to people, so I built one flask application.\nWeb Application UI\n\nHere is demonstration of project: project demo\nI left a blank section for user suggestion and feedback, these feedbacks are getting stored in database for future model analysis or any business work.\nDatabase configuration\nFor storing collected user data we need to configure a database. It will help us to improve model as per user need.\nI used Google Cloud MySQL instance for integrating my application with database, GCP is paid but you can use free credit of GCP for first 3 months, for doing almost all Cloud Work.\nYou need to create your GCP account, then create one instance under SQL and by configuring your local system with instance you are good to go.\nDocker containerization\nIf you are not awared about docker and it\'s basic concepts you can refer my previous article which explains you all about docker.\nThis guide will help you build a solid foundation in Docker, enabling you to confidently use it for your projects.\nDocker guide: Comprehensive Docker guide for deploying Flas app\nDeploy\nDeployment of web app will help us to engange people and provide them real actual experience of our services.\nAfter deployment you need to collect user data and store it for future analysis, this data contain user feedback and suggestions.\nAfter deployment it\'s not end of the process we need to collect user feedback and again follow same steps fine tune Model as per user need.\n\nSummary of Blog\nThis blog covers the lifecycle of a Machine Learning project, from ETL to deployment. It details building an ETL pipeline using PySpark for efficient data handling, EDA, and NLP preprocessing techniques like tokenization, TF-IDF, and WordCloud visualization. \nIt highlights ML experiments with MLflow on Databricks and hyperparameter tuning using Grid Search.\nThe model was packaged into a Python package and deployed as a Flask application with a database backend (Google Cloud MySQL) and Dockerized for scalability. \nThe app collects user feedback for continuous improvement. It emphasizes end-to-end integration, including cloud and containerization, to deliver a robust ML solution.\nHappy Learning!\n\n'}, {'content': '\nLanguages I Speak\nEnglish, Marathi, Hindi\n\n'}, {'content': '\n\nSoft skills:\nCritical Thinking, Intellectual Rigor, Problem Solving, Understanding Business Needs'}]
2024-11-25 21:54:01 - root - INFO - Document store already contains 14 documents. Skipping write.
2024-11-25 21:54:01 - haystack.modeling.utils - INFO - Using devices: CPU - Number of GPUs: 0
2024-11-25 21:54:01 - haystack.modeling.utils - INFO - Using devices: CPU - Number of GPUs: 0
2024-11-25 21:54:02 - haystack.modeling.model.language_model - INFO -  * LOADING MODEL: 'deepset/minilm-uncased-squad2' (Bert)
2024-11-25 21:54:03 - haystack.modeling.model.language_model - INFO - Auto-detected model language: english
2024-11-25 21:54:03 - haystack.modeling.model.language_model - INFO - Loaded 'deepset/minilm-uncased-squad2' (Bert model) from model hub.
2024-11-25 21:54:04 - haystack.modeling.utils - INFO - Using devices: CPU - Number of GPUs: 0
2024-11-25 21:54:04 - root - INFO - Loading existing FAISS document store...
2024-11-25 21:54:04 - haystack.modeling.utils - INFO - Using devices: CPU - Number of GPUs: 0
2024-11-25 21:54:06 - haystack.modeling.model.language_model - WARNING - Using a model of type 'mpnet' which might be incompatible with DPR encoders. Only Bert-based encoders are supported. They need input_ids, token_type_ids, attention_mask as input tensors.
2024-11-25 21:54:22 - haystack.modeling.model.language_model - INFO - Auto-detected model language: english
2024-11-25 21:54:23 - haystack.modeling.model.language_model - WARNING - Using a model of type 'mpnet' which might be incompatible with DPR encoders. Only Bert-based encoders are supported. They need input_ids, token_type_ids, attention_mask as input tensors.
2024-11-25 21:54:30 - haystack.modeling.model.language_model - INFO - Auto-detected model language: english
2024-11-25 21:54:30 - root - INFO - Reading text document
2024-11-25 21:54:30 - root - INFO - loaded document = [{'content': '\nPersonal Information\n\nName: Vijay Takbhate\nEmail: vijaytakbhate20@gmail.com Phone: 8767363681\nGitHub: https://github.com/vijaytakbhate2002\nLinkedIn: https://www.linkedin.com/in/vijay-takbhate-b9231a236/ Kaggle: https://www.kaggle.com/vijay20213\n\n'}, {'content': '\n\nExperience - Fox Solutions Pvt. Ltd.\nRole: Automation Engineer Duration: Jun 2024 - Oct 2024 Location: Maharashtra\nKey Contributions:\n-Completed 2 months of internship plus 4 months of full-time work.\n-Worked with PLC and SCADA systems, focusing on automating processes and optimizing operational efficiency.\n-Collaborated with cross-functional teams to implement automation solutions for industrial applications.\n\n'}, {'content': '\n\nExperience - Cei Design Consultancy Pvt. Ltd.\nRole: Python Developer Intern Duration: Aug 2024 - Sept 2024 Location: Remote, Maharashtra\nKey Contributions:\n-Specialized in data processing using Python and Excel.\n-Utilized OpenCV for image processing tasks.\n\n'}, {'content': '\n\nExperience - Ujucode\nRole: Subject Matter Expert Intern Duration: Aug 2023 - Oct 2023 Location: Remote, Maharashtra\nKey Contributions:\n-Contributed as a Python developer for a ChatBot project.\n-Handled backend development tasks and researched Python modules.\n\n'}, {'content': '\n\nProject - Twitter Post Sentiment Prediction\nDetails:\n-Engineered an ETL pipeline using PySpark and SQL.\n-Conducted sentiment analysis using NLP (TF-IDF) and optimized hyperparameters.\n-Monitored model performance through MLFlow on Databricks.\n-Leveraged Google Cloud Storage and MySQL for data management.\n-Deployed the model using Docker and hosted it on Render.\n\n'}, {'content': '\n\nProject - Text-Text Chat-Bot\nDetails:\n-Designed an advanced Chat-Bot using the NVIDIA API and prompt engineering.\n-Features include paraphrasing, grammar correction, AI detection, plagiarism checking, and content summarization.\n-Targeted at content creators, researchers, and businesses.\n-Technologies Used: HTML, CSS, Python Flask, Cloud Database, and Render.\n\n'}, {'content': '\n\nProject - Hand Gesture Recognition\nDetails:\n-Used Google-s MediaPipe framework for detecting hand landmarks and gestures.\n-Created and labeled a custom dataset of hand gestures for training.\n-Developed a Streamlit application to improve accessibility and flexibility.\n\n'}, {'content': '\n\nTechnical Skills\nLanguages: MySQL, Python, HTML, CSS\nTechnologies: Streamlit, Flask, VS Code, GitHub, MLflow, Docker, PySpark, Databricks, Google Cloud Platform\n\n'}, {'content': '\n\nCertification\nMLOps Bootcamp: Mastering AI Operations for Success (Jun 2024)\n-Learned about the MLOps lifecycle and modular programming.\n-Acquired skills in Git, Python, Flask, and MLflow.\n\n'}, {'content': '\n\nEducation Details:\nBachelor of Technology in Electronics and Telecommunication (May 2024) Institution: SVERI-s College of Engineering, Pandharpur, Maharashtra Score: 81.71\nDiploma in Electronics and Telecommunication (May 2021) Institution: SVERI-s College of Engineering, Pandharpur, Maharashtra Score: 91.73\n\n'}, {'content': "\n\nBlogging\n\n\nSupervised, Unsupervised, and Beyond: ML Techniques Simplified\nNovember 25, 2024\nThere are several techniques for ML training. Among these, I will cover the following:\n\nSupervised and Unsupervised Learning\nSemi-Supervised Learning\nOffline and Online Learning\nInstance-Based and Model-Based Learning\n\nSupervised and Unsupervised Learning\nSupervised learning is like spoon-feeding our ML model in its initial stages, allowing it to learn and improve over time. Here, I’m referring to the training process.\n\nIn supervised learning, there are input columns and output columns, also called target columns. For example, in spam detection—a classification problem—the input is the email, and the target is whether the email is spam or not. That’s it!\nThis process resembles a student-teacher scenario where the teacher is a human, and the student is the model. The dataset serves as the knowledge used to train the student (model)\nSee content credentials\n\nhumand and model\nIn Unsupervised learning, we are not aware of the data labels. Instead, we separate the input data by analyzing similarities and grouping them into different clusters.\nOnce the clusters are formed, we can assign custom labels to each one. This technique is widely used to identify product relationships in online shopping and to recommend new products based on a user’s purchase history.\n\nUnsupervised Learning Clusters\nHere three clusters with three different image categories are formed\n\nSemi-Supervised Learning\nSemi-Supervised Learning is a combination of supervised and unsupervised learning, where some data is labeled and some data is unlabeled. A good example of this is Google Photos, which automatically separates new photos into their respective groups based on whether they contain a particular person in each image.\n\nThere are several techniques under semi-supervised learning; here, we will focus on the following:\nSelf Learning\nConsistency Regularization\nGenerative Models\nGraph-Based Learning\n\nLet's discuss them one by one:\n\nSelf Learning\nSelf Learning trains a model with labeled data and generates pseudo-labels for the unlabeled data. \n\nSelf learning\nThen, the model is trained on the entire dataset, which includes both the generated pseudo-labels and the labeled data.\n\nConsistency Regularization\nThis technique uses data augmentation to generate similar data, and then the model is enforced to predict the same outcome for both the augmented and original data. It helps create a model that can find similarities in both labeled and unlabeled data, predicting the same output for unlabeled data as it would for labeled data.\n\nData augmentation includes techniques like image flipping, blurring, rotation, etc. After augmenting the data, the model is trained to predict the same class for both the augmented and original images.\n\nGenerative Models\nGenerative models create synthetic data points and learn the underlying structure of the training data. These models can generate new datasets using encoders. \n\nExamples include Generative Adversarial Networks (GANs) and Variational Autoencoders (VAEs).\n\nGraph-Based Learning\nThis technique creates nodes for both labeled and unlabeled data. Similar nodes are then connected to each other.\nFor example, when dealing with images, each image starts as a separate node. If similar images are found based on their features, they are grouped or connected by an edge.\nRefer below graph to understand graph-based learning\nGraph based learning\nHere label 1 is came from labeled image of group 1, simillarly for label 2 and label 3. Finally we came up with three labels that means labeled data has three unique labels.\nBatch (Offline) and Online Learning\nBatch Learning\nBatch learning, also known as offline learning, is a technique where the developer needs to stop the deployed ML model, retrain it on new data, and then redeploy it. \nThis technique is useful when frequent updates are not required. For example, product recommendation systems can often be retrained weekly or even over several days.\nBatch learning can be costly when training the model on the entire dataset every time. However, using incremental learning models can help avoid unnecessary retraining, making the process more efficient.\nHere are some examples on incremental learning models, as of now we are focusing on general concepts so don't go much deeper into these types.\nStochastic Gradient Descent (SGD)\nNaive Bayes (Online Version)\nIncremental Support Vector Machines (ISVM)\nOnline Random Forests\nIncremental Decision Trees\nYou can refer this article to learn more about incremental learning\nHere’s your text with grammatical corrections and improved clarity:\nOnline Learning\nOnline learning is used when an ML model needs to be updated continuously with new inputs, such as in stock price prediction. In this scenario, the model must remain aware of the most recent data.\n\nBatch and online learning\nOnly models capable of incremental learning are used in online learning. Each new instance is fed back into the model to update its internal weights based on the latest input.\n\nInstance-Based and Model-Based Learning\nInstance-Based Learning\nInstance-based learning involves comparing a new input with stored data. If similarity is found, the model returns the label of the corresponding input. This approach uses pattern matching techniques. \nHere model works as a search engin not exactly same but it will find simmilarity from stored data.\nSome common models used in instance-based learning include:\nK-Nearest Neighbors (K-NN) with labels\nLocally Weighted Regression, etc.\nModel-Based Learning\nIn model-based learning, we train the model using a training dataset. During the training process, the model creates its own complex patterns (mathematical equations) to make predictions for future inputs. \nA simple example is the equation y=mx+cy = mx + c, which represents a trained model. Some common models used in model-based learning include:\nLinear Regression\nLogistic Regression\nDecision Tree, etc.\n\nSummary\nMachine Learning techniques can be broadly categorized into supervised, unsupervised, and semi-supervised learning. \nSupervised learning uses labeled data, like spam detection, while unsupervised learning works with unlabeled data, grouping similar points into clusters. \nSemi-supervised learning combines both, employing techniques like self-learning and consistency regularization to leverage partially labeled datasets.\nOther approaches include batch learning, where models are retrained periodically, and online learning, which updates models continuously with new data, suitable for dynamic tasks like stock price prediction. \nInstance-based learning relies on pattern matching (e.g., K-Nearest Neighbors), while model-based learning creates mathematical models (e.g., Linear Regression) to make predictions.\nThis is all about Machine Learning techniques. If you learned something, let me know in the comments. Your suggestions will help me improve my blogs.\nThanks for reading!\n\n"}, {'content': '\nBlogging\n\nMastering the End-to-End Machine Learning Lifecycle: From Data to Deployment\n\nNovember 24, 2024\n This article takes you through the complete lifecycle of a Machine Learning project. From ETL to deployment, I’ll share every detail of how I brought this project to life.\nData Engineering\nData Engineering serves as the starting point in the Machine Learning project lifecycle, bringing all distributed data together in one place.\nFor this project, I utilized an ETL pipeline—a core concept in Data Engineering. It enabled me to extract raw data, transform it into a meaningful format, and load it into a suitable location for further processing.\nLet\'s deep dive into ETL pipeline\nETL stands for Extract, Transform, and Load. These pipelines can be executed periodically (e.g., daily or hourly) to fetch real-time data, enhancing the predictive power of our Machine Learning model for real-world applications.\nThis process can be automated using tools like Apache Airflow, Kubeflow Pipelines, AWS Step Functions, and more, streamlining the workflow for consistent and efficient data updates.\n\nETL Pipeline:\nETL Pipeline\nExtract Operation\nThe extract operation is responsible for fetching data from various sources such as websites, databases, and APIs, as illustrated in the flowchart above.\n\nTransform Operation\nThe transform operation focuses on data cleaning, feature selection, and manipulation. In my project, I used this step to extract only the required features, ensuring the data is ready for the next phase.\n\nLoad Operation\nThe load operation transfers the transformed data to its destination, where we can choose the appropriate storage format. Typically, the processed data is stored in a database for further analysis and model training.\n\nFor my project, I used PySpark to build the ETL pipeline, as it enables efficient processing of large datasets.\nWhy not Pandas?\nWhile Pandas is excellent for small to medium-sized datasets, it stores DataFrames in RAM, which can lead to out-of-memory exceptions when handling large datasets.\nIn contrast, PySpark creates a session and processes data in chunks, storing it in ROM, making it ideal for handling large-scale data.\nFor the actual implementation of the pipeline using PySpark, please refer to the accompanying jupyter notebook.\nEDA (Data Analysis)\nData understanding is a critical stage before building any Machine Learning model. It allows us to analyze the data, plan the data processing steps, and gain insights into its structure and quality.\nIn this project, I examined the balance of data in my training and testing datasets and found it to be well-distributed across all four categories.\n\nBalanceness Checking on train and test data\nThroughout the entire process, I focused on two columns:\nTweets\nSentiments\nThe Tweets column contains the raw Twitter text data, while the Sentiments column serves as the target variable for prediction.\nNatural Language Processing\nI applied several key NLP techniques to preprocess the data and prepare it for Machine Learning model building. Below are the main steps I executed:\n\nText Processing\nRegex: I applied regular expressions to clean the text by removing URLs, hashtags, HTML tags, and keeping only alphanumeric characters. This helped eliminate unnecessary noise from the data.\nNLTK: Using the Natural Language Toolkit (NLTK), I performed word tokenization, stemming and lemmatization.\nWord tokenization is just splitting sentence into words, so we can processing each word from sentence individually.\nStemming helps us to truncate prefix or suffix of text to reduce count on unique words from corpus (paragraph).\nLemmatization is the proecss of converting any word into it\'s base word, for eg. Played will convert to play.\n\nWordCloud \nWord cloud concept help you to understand importance of words from given data. I splitted my data into four sections.\ndata for negative sentiments\ndata for positive sentiments\ndata for neutral sentiments\ndata for irrelevant sentiments\n\nHere is the representation of most frequent words for each category.\nCategorywise Word Cloud Presentation\nHere you can clearly see that there is no much difference in negative and positive sentiments data.\nthis is representation of bad data, here we can filterout our data for further processing, it might reduce data but you can do data augmentation techniques here to increase your data.\naugmenting of data in NLP with TF-IDF will not bad idea because TF-IDF and any other porcessing technique that I used is not able to detect sentence grammer or it doesn,t require sophisticated text. \nYou can think our input text will work as bag of word for model. you can understand by refering below vectorization technique.\nVectorization with TF-IDF \nI used Term Frequency-Inverse Document Frequency (TF-IDF) for text vectorization. This method transforms text into a numerical format, considering the importance of each word across documents, which prevents frequent words from dominating the model.\n\nTF-IDF\nTF-IDF stands for Term Frequency-Inverse Document Frequency. \nTerm Frequency (TF): This measures how frequently a term (t) appears in a specific document (d). It\'s calculated by dividing the number of occurrences of the term in the document by the total number of terms in that document.\nInverse Document Frequency (IDF): This measures how important a term is across the entire collection of documents. It\'s calculated by taking the logarithm of the ratio of the total number of documents (N) to the number of documents containing the term (df(t)). A higher IDF value indicates a rarer term, making it more significant.\nHere is the resulant data we got from TF-IDF\nAfter all preprocessing of text it\'s time to save our data for mlflow experiments. To see actual implementation of end to end process of NLP you can refer jupyter notebook.\nmlflow experiments\nMLflow experiments allow you to conduct multiple experiments with your trained model, helping you track and compare results over time. For running MLflow experiments, I prefer using Databricks as it offers an integrated experiment section, making the process much more streamlined and efficient.\nIn Databricks, you can easily connect your experiment by passing the experiment ID into the MLflow code. This integration simplifies the entire workflow, enabling better experiment tracking and easier comparison of model performance.\nFor my experiments, I worked with several models, including Logistic Regression, Multinomial Naive Bayes, and Decision Tree Classifier. By applying different combinations of parameters, I was able to experiment with and compare the performance of each model. Here\'s a preview of the Logistic Regression model’s F1 score, which highlights the model\'s ability to balance precision and recall:\nLogistic Regression F1 Score: 0.58\nThis approach allowed me to track the effectiveness of each model and make adjustments as needed for improving performance.\n\nF1 score with different parameters (Logistic Regression)\nI recommend running the notebook below in your own account to see the results. You\'ll definitely start appreciating the power of MLflow.\nFor more information, please refer to the accompanying Databricks notebook.\n\nHyperparameter tuning\nHyperparameter tuning is crucial for identifying the best parameter combination for your model. This technique involves an iterative process where, for every parameter combination, the model is trained and tested on a dataset. It is often described as a "trial and error" method.\nHowever, this approach can be computationally expensive, especially when working with complex or heavy machine learning models. For large-scale problems, hyperparameter tuning can be made more efficient through sampling or batch methods. In these methods, you don\'t use the entire dataset for training the model; instead, you choose random or stratified data points from the dataset to train the model. Although this may slightly reduce accuracy, it is more feasible when working with large datasets.\nFor my project, I used Grid Search CV to find the best hyperparameter combination for the model. Below are some common techniques for hyperparameter tuning, especially for large datasets:\nGrid Search This technique exhaustively searches through a specified set of hyperparameter values, trying all possible combinations. While effective, it can be computationally expensive for larger datasets due to the exhaustive nature of the search.\nRandom Search Randomly samples from the hyperparameter space, offering a faster alternative to grid search. This method explores a wider range of hyperparameters with fewer evaluations, making it more efficient for larger datasets.\nBayesian Optimization This method uses probabilistic models to predict the performance of different hyperparameters. It selects the next set of hyperparameters to evaluate based on previous results, making it more efficient and suitable for large datasets.\nGenetic Algorithms Inspired by natural selection, these algorithms iteratively evolve a population of hyperparameter sets to improve model performance. This method works well with complex search spaces.\nHyperband Hyperband combines random search with early stopping to dynamically allocate resources across multiple configurations, identifying promising hyperparameters quickly without excessive computational costs.\nBayesian Optimization with Gaussian Processes This technique models the hyperparameter search space using Gaussian processes, focusing on regions that are likely to yield better results, which is particularly useful for large datasets where computational resources are limited.\n\nTo optimize hyperparameter tuning for larger datasets, these techniques can be combined with parallel computing and distributed processing frameworks such as Dask, Spark, or multi-GPU setups. This enables more efficient hyperparameter search and reduces the overall computational overhead.\nI choosed Logistc Regression model and trained my Model with best parameters.\nSource Distribution for Model Packaging\nPackaging your machine learning model is a best practice, especially if you don’t plan to update it frequently. Imagine thousands of lines of code that can now be utilized with just a single line—this is the power of model packaging.\n\n1. Project Folder Setup\nTo ensure better organization, I created a main folder called sentiment_prediction and moved all machine learning pipeline files and dependencies into this folder. This helped in maintaining a clean structure and simplified the management of the entire project.\nBefore moving forward I recommend you to visit this pdf it will practically show you step by step process for building python package.\nPDF : step by step guid for python package building\n\n2. Manifest.in\nThe Manifest.in file plays a crucial role in controlling which files and folders should be included or excluded during the packaging process. It helps to specify the structure of the package for distribution.\nKey commands used in the Manifest.in file include:\ninclude <file/folder>: Include specific files or folders.\nexclude <file/folder>: Exclude specific files or folders.\nrecursive-include <path>: Include all files from a directory recursively.\nrecursive-exclude <path>: Exclude all files from a directory recursively.\n3. Setup.py\nThe setup.py file contains the project\'s metadata and is essential for creating the package. It defines key information about the project, such as:\nProject name, version, description, and author details.\nDependencies required for the package (install_requires), making it easy to install all necessary libraries.\n\n4. Building the Package\nTo build the package, I used the following command:\n\npython setup.py sdist bdist_wheel\nThis command generates two folders:\n\nbuild/: Contains the entire project package as defined in the Manifest.in.\ndist/: Contains the distributable files: .whl (wheel file) .gz (compressed source archive)\n\n5. Global Access via GitHub\nNow you can access your package gloablly, by refering your repository. I provided my package below go and check out.\n\nRepository: GitHub Repo Link\nYou can install the package directly from GitHub using the following command:\n\npip install git+https://github.com/vijaytakbhate2002/sentiment_prediction_python_package.git \nTo ensure it worked globally, I tested it again:\n\nfrom sentiment_prediction import predict\nprint(predict.predictor("Great progress shared today!")) \noutput:[\'Negative\']\nFlask Application \nBuilding application will help us to give our NLP model experience to people, so I built one flask application.\nWeb Application UI\n\nHere is demonstration of project: project demo\nI left a blank section for user suggestion and feedback, these feedbacks are getting stored in database for future model analysis or any business work.\nDatabase configuration\nFor storing collected user data we need to configure a database. It will help us to improve model as per user need.\nI used Google Cloud MySQL instance for integrating my application with database, GCP is paid but you can use free credit of GCP for first 3 months, for doing almost all Cloud Work.\nYou need to create your GCP account, then create one instance under SQL and by configuring your local system with instance you are good to go.\nDocker containerization\nIf you are not awared about docker and it\'s basic concepts you can refer my previous article which explains you all about docker.\nThis guide will help you build a solid foundation in Docker, enabling you to confidently use it for your projects.\nDocker guide: Comprehensive Docker guide for deploying Flas app\nDeploy\nDeployment of web app will help us to engange people and provide them real actual experience of our services.\nAfter deployment you need to collect user data and store it for future analysis, this data contain user feedback and suggestions.\nAfter deployment it\'s not end of the process we need to collect user feedback and again follow same steps fine tune Model as per user need.\n\nSummary of Blog\nThis blog covers the lifecycle of a Machine Learning project, from ETL to deployment. It details building an ETL pipeline using PySpark for efficient data handling, EDA, and NLP preprocessing techniques like tokenization, TF-IDF, and WordCloud visualization. \nIt highlights ML experiments with MLflow on Databricks and hyperparameter tuning using Grid Search.\nThe model was packaged into a Python package and deployed as a Flask application with a database backend (Google Cloud MySQL) and Dockerized for scalability. \nThe app collects user feedback for continuous improvement. It emphasizes end-to-end integration, including cloud and containerization, to deliver a robust ML solution.\nHappy Learning!\n\n'}, {'content': '\nLanguages I Speak\nEnglish, Marathi, Hindi\n\n'}, {'content': '\n\nSoft skills:\nCritical Thinking, Intellectual Rigor, Problem Solving, Understanding Business Needs'}]
2024-11-25 21:54:30 - root - INFO - Document store already contains 14 documents. Skipping write.
2024-11-25 21:54:30 - haystack.modeling.utils - INFO - Using devices: CPU - Number of GPUs: 0
2024-11-25 21:54:30 - haystack.modeling.utils - INFO - Using devices: CPU - Number of GPUs: 0
2024-11-25 21:54:31 - haystack.modeling.model.language_model - INFO -  * LOADING MODEL: 'deepset/minilm-uncased-squad2' (Bert)
2024-11-25 21:54:31 - haystack.modeling.model.language_model - INFO - Auto-detected model language: english
2024-11-25 21:54:31 - haystack.modeling.model.language_model - INFO - Loaded 'deepset/minilm-uncased-squad2' (Bert model) from model hub.
2024-11-25 21:54:32 - haystack.modeling.utils - INFO - Using devices: CPU - Number of GPUs: 0
2024-11-25 21:54:32 - haystack.modeling.data_handler.processor - ERROR - There were 1 errors during preprocessing at positions: {0}
2024-11-27 14:19:02 - root - INFO - Creating a new FAISS document store...
2024-11-27 14:19:03 - haystack.modeling.utils - INFO - Using devices: CPU - Number of GPUs: 0
2024-11-27 14:19:05 - haystack.modeling.model.language_model - WARNING - Using a model of type 'mpnet' which might be incompatible with DPR encoders. Only Bert-based encoders are supported. They need input_ids, token_type_ids, attention_mask as input tensors.
2024-11-27 14:19:08 - haystack.modeling.model.language_model - INFO - Auto-detected model language: english
2024-11-27 14:19:09 - haystack.modeling.model.language_model - WARNING - Using a model of type 'mpnet' which might be incompatible with DPR encoders. Only Bert-based encoders are supported. They need input_ids, token_type_ids, attention_mask as input tensors.
2024-11-27 14:19:11 - haystack.modeling.model.language_model - INFO - Auto-detected model language: english
2024-11-27 14:19:11 - root - INFO - Reading text document
2024-11-27 14:19:11 - root - INFO - loaded document = [{'content': '\nPersonal Information\n\nName: Vijay Takbhate\nEmail: vijaytakbhate20@gmail.com Phone: 8767363681\nGitHub: https://github.com/vijaytakbhate2002\nLinkedIn: https://www.linkedin.com/in/vijay-takbhate-b9231a236/ Kaggle: https://www.kaggle.com/vijay20213\n\n'}, {'content': '\n\nExperience - Fox Solutions Pvt. Ltd.\nRole: Automation Engineer Duration: Jun 2024 - Oct 2024 Location: Maharashtra\nKey Contributions:\n-Completed 2 months of internship plus 4 months of full-time work.\n-Worked with PLC and SCADA systems, focusing on automating processes and optimizing operational efficiency.\n-Collaborated with cross-functional teams to implement automation solutions for industrial applications.\n\n'}, {'content': '\n\nExperience - Cei Design Consultancy Pvt. Ltd.\nRole: Python Developer Intern Duration: Aug 2024 - Sept 2024 Location: Remote, Maharashtra\nKey Contributions:\n-Specialized in data processing using Python and Excel.\n-Utilized OpenCV for image processing tasks.\n\n'}, {'content': '\n\nExperience - Ujucode\nRole: Subject Matter Expert Intern Duration: Aug 2023 - Oct 2023 Location: Remote, Maharashtra\nKey Contributions:\n-Contributed as a Python developer for a ChatBot project.\n-Handled backend development tasks and researched Python modules.\n\n'}, {'content': '\n\nProject - Twitter Post Sentiment Prediction\nDetails:\n-Engineered an ETL pipeline using PySpark and SQL.\n-Conducted sentiment analysis using NLP (TF-IDF) and optimized hyperparameters.\n-Monitored model performance through MLFlow on Databricks.\n-Leveraged Google Cloud Storage and MySQL for data management.\n-Deployed the model using Docker and hosted it on Render.\n\n'}, {'content': '\n\nProject - Text-Text Chat-Bot\nDetails:\n-Designed an advanced Chat-Bot using the NVIDIA API and prompt engineering.\n-Features include paraphrasing, grammar correction, AI detection, plagiarism checking, and content summarization.\n-Targeted at content creators, researchers, and businesses.\n-Technologies Used: HTML, CSS, Python Flask, Cloud Database, and Render.\n\n'}, {'content': '\n\nProject - Hand Gesture Recognition\nDetails:\n-Used Google-s MediaPipe framework for detecting hand landmarks and gestures.\n-Created and labeled a custom dataset of hand gestures for training.\n-Developed a Streamlit application to improve accessibility and flexibility.\n\n'}, {'content': '\n\nTechnical Skills\nLanguages: MySQL, Python, HTML, CSS\nTechnologies: Streamlit, Flask, VS Code, GitHub, MLflow, Docker, PySpark, Databricks, Google Cloud Platform\n\n'}, {'content': '\n\nCertification\nMLOps Bootcamp: Mastering AI Operations for Success (Jun 2024)\n-Learned about the MLOps lifecycle and modular programming.\n-Acquired skills in Git, Python, Flask, and MLflow.\n\n'}, {'content': '\n\nEducation Details:\nBachelor of Technology in Electronics and Telecommunication (May 2024) Institution: SVERI-s College of Engineering, Pandharpur, Maharashtra Score: 81.71\nDiploma in Electronics and Telecommunication (May 2021) Institution: SVERI-s College of Engineering, Pandharpur, Maharashtra Score: 91.73\n\n'}, {'content': "\n\nBlogging\n\n\nSupervised, Unsupervised, and Beyond: ML Techniques Simplified\nNovember 25, 2024\nThere are several techniques for ML training. Among these, I will cover the following:\n\nSupervised and Unsupervised Learning\nSemi-Supervised Learning\nOffline and Online Learning\nInstance-Based and Model-Based Learning\n\nSupervised and Unsupervised Learning\nSupervised learning is like spoon-feeding our ML model in its initial stages, allowing it to learn and improve over time. Here, I’m referring to the training process.\n\nIn supervised learning, there are input columns and output columns, also called target columns. For example, in spam detection—a classification problem—the input is the email, and the target is whether the email is spam or not. That’s it!\nThis process resembles a student-teacher scenario where the teacher is a human, and the student is the model. The dataset serves as the knowledge used to train the student (model)\nSee content credentials\n\nhumand and model\nIn Unsupervised learning, we are not aware of the data labels. Instead, we separate the input data by analyzing similarities and grouping them into different clusters.\nOnce the clusters are formed, we can assign custom labels to each one. This technique is widely used to identify product relationships in online shopping and to recommend new products based on a user’s purchase history.\n\nUnsupervised Learning Clusters\nHere three clusters with three different image categories are formed\n\nSemi-Supervised Learning\nSemi-Supervised Learning is a combination of supervised and unsupervised learning, where some data is labeled and some data is unlabeled. A good example of this is Google Photos, which automatically separates new photos into their respective groups based on whether they contain a particular person in each image.\n\nThere are several techniques under semi-supervised learning; here, we will focus on the following:\nSelf Learning\nConsistency Regularization\nGenerative Models\nGraph-Based Learning\n\nLet's discuss them one by one:\n\nSelf Learning\nSelf Learning trains a model with labeled data and generates pseudo-labels for the unlabeled data. \n\nSelf learning\nThen, the model is trained on the entire dataset, which includes both the generated pseudo-labels and the labeled data.\n\nConsistency Regularization\nThis technique uses data augmentation to generate similar data, and then the model is enforced to predict the same outcome for both the augmented and original data. It helps create a model that can find similarities in both labeled and unlabeled data, predicting the same output for unlabeled data as it would for labeled data.\n\nData augmentation includes techniques like image flipping, blurring, rotation, etc. After augmenting the data, the model is trained to predict the same class for both the augmented and original images.\n\nGenerative Models\nGenerative models create synthetic data points and learn the underlying structure of the training data. These models can generate new datasets using encoders. \n\nExamples include Generative Adversarial Networks (GANs) and Variational Autoencoders (VAEs).\n\nGraph-Based Learning\nThis technique creates nodes for both labeled and unlabeled data. Similar nodes are then connected to each other.\nFor example, when dealing with images, each image starts as a separate node. If similar images are found based on their features, they are grouped or connected by an edge.\nRefer below graph to understand graph-based learning\nGraph based learning\nHere label 1 is came from labeled image of group 1, simillarly for label 2 and label 3. Finally we came up with three labels that means labeled data has three unique labels.\nBatch (Offline) and Online Learning\nBatch Learning\nBatch learning, also known as offline learning, is a technique where the developer needs to stop the deployed ML model, retrain it on new data, and then redeploy it. \nThis technique is useful when frequent updates are not required. For example, product recommendation systems can often be retrained weekly or even over several days.\nBatch learning can be costly when training the model on the entire dataset every time. However, using incremental learning models can help avoid unnecessary retraining, making the process more efficient.\nHere are some examples on incremental learning models, as of now we are focusing on general concepts so don't go much deeper into these types.\nStochastic Gradient Descent (SGD)\nNaive Bayes (Online Version)\nIncremental Support Vector Machines (ISVM)\nOnline Random Forests\nIncremental Decision Trees\nYou can refer this article to learn more about incremental learning\nHere’s your text with grammatical corrections and improved clarity:\nOnline Learning\nOnline learning is used when an ML model needs to be updated continuously with new inputs, such as in stock price prediction. In this scenario, the model must remain aware of the most recent data.\n\nBatch and online learning\nOnly models capable of incremental learning are used in online learning. Each new instance is fed back into the model to update its internal weights based on the latest input.\n\nInstance-Based and Model-Based Learning\nInstance-Based Learning\nInstance-based learning involves comparing a new input with stored data. If similarity is found, the model returns the label of the corresponding input. This approach uses pattern matching techniques. \nHere model works as a search engin not exactly same but it will find simmilarity from stored data.\nSome common models used in instance-based learning include:\nK-Nearest Neighbors (K-NN) with labels\nLocally Weighted Regression, etc.\nModel-Based Learning\nIn model-based learning, we train the model using a training dataset. During the training process, the model creates its own complex patterns (mathematical equations) to make predictions for future inputs. \nA simple example is the equation y=mx+cy = mx + c, which represents a trained model. Some common models used in model-based learning include:\nLinear Regression\nLogistic Regression\nDecision Tree, etc.\n\nSummary\nMachine Learning techniques can be broadly categorized into supervised, unsupervised, and semi-supervised learning. \nSupervised learning uses labeled data, like spam detection, while unsupervised learning works with unlabeled data, grouping similar points into clusters. \nSemi-supervised learning combines both, employing techniques like self-learning and consistency regularization to leverage partially labeled datasets.\nOther approaches include batch learning, where models are retrained periodically, and online learning, which updates models continuously with new data, suitable for dynamic tasks like stock price prediction. \nInstance-based learning relies on pattern matching (e.g., K-Nearest Neighbors), while model-based learning creates mathematical models (e.g., Linear Regression) to make predictions.\nThis is all about Machine Learning techniques. If you learned something, let me know in the comments. Your suggestions will help me improve my blogs.\nThanks for reading!\n\n"}, {'content': '\nBlogging\n\nMastering the End-to-End Machine Learning Lifecycle: From Data to Deployment\n\nNovember 24, 2024\n This article takes you through the complete lifecycle of a Machine Learning project. From ETL to deployment, I’ll share every detail of how I brought this project to life.\nData Engineering\nData Engineering serves as the starting point in the Machine Learning project lifecycle, bringing all distributed data together in one place.\nFor this project, I utilized an ETL pipeline—a core concept in Data Engineering. It enabled me to extract raw data, transform it into a meaningful format, and load it into a suitable location for further processing.\nLet\'s deep dive into ETL pipeline\nETL stands for Extract, Transform, and Load. These pipelines can be executed periodically (e.g., daily or hourly) to fetch real-time data, enhancing the predictive power of our Machine Learning model for real-world applications.\nThis process can be automated using tools like Apache Airflow, Kubeflow Pipelines, AWS Step Functions, and more, streamlining the workflow for consistent and efficient data updates.\n\nETL Pipeline:\nETL Pipeline\nExtract Operation\nThe extract operation is responsible for fetching data from various sources such as websites, databases, and APIs, as illustrated in the flowchart above.\n\nTransform Operation\nThe transform operation focuses on data cleaning, feature selection, and manipulation. In my project, I used this step to extract only the required features, ensuring the data is ready for the next phase.\n\nLoad Operation\nThe load operation transfers the transformed data to its destination, where we can choose the appropriate storage format. Typically, the processed data is stored in a database for further analysis and model training.\n\nFor my project, I used PySpark to build the ETL pipeline, as it enables efficient processing of large datasets.\nWhy not Pandas?\nWhile Pandas is excellent for small to medium-sized datasets, it stores DataFrames in RAM, which can lead to out-of-memory exceptions when handling large datasets.\nIn contrast, PySpark creates a session and processes data in chunks, storing it in ROM, making it ideal for handling large-scale data.\nFor the actual implementation of the pipeline using PySpark, please refer to the accompanying jupyter notebook.\nEDA (Data Analysis)\nData understanding is a critical stage before building any Machine Learning model. It allows us to analyze the data, plan the data processing steps, and gain insights into its structure and quality.\nIn this project, I examined the balance of data in my training and testing datasets and found it to be well-distributed across all four categories.\n\nBalanceness Checking on train and test data\nThroughout the entire process, I focused on two columns:\nTweets\nSentiments\nThe Tweets column contains the raw Twitter text data, while the Sentiments column serves as the target variable for prediction.\nNatural Language Processing\nI applied several key NLP techniques to preprocess the data and prepare it for Machine Learning model building. Below are the main steps I executed:\n\nText Processing\nRegex: I applied regular expressions to clean the text by removing URLs, hashtags, HTML tags, and keeping only alphanumeric characters. This helped eliminate unnecessary noise from the data.\nNLTK: Using the Natural Language Toolkit (NLTK), I performed word tokenization, stemming and lemmatization.\nWord tokenization is just splitting sentence into words, so we can processing each word from sentence individually.\nStemming helps us to truncate prefix or suffix of text to reduce count on unique words from corpus (paragraph).\nLemmatization is the proecss of converting any word into it\'s base word, for eg. Played will convert to play.\n\nWordCloud \nWord cloud concept help you to understand importance of words from given data. I splitted my data into four sections.\ndata for negative sentiments\ndata for positive sentiments\ndata for neutral sentiments\ndata for irrelevant sentiments\n\nHere is the representation of most frequent words for each category.\nCategorywise Word Cloud Presentation\nHere you can clearly see that there is no much difference in negative and positive sentiments data.\nthis is representation of bad data, here we can filterout our data for further processing, it might reduce data but you can do data augmentation techniques here to increase your data.\naugmenting of data in NLP with TF-IDF will not bad idea because TF-IDF and any other porcessing technique that I used is not able to detect sentence grammer or it doesn,t require sophisticated text. \nYou can think our input text will work as bag of word for model. you can understand by refering below vectorization technique.\nVectorization with TF-IDF \nI used Term Frequency-Inverse Document Frequency (TF-IDF) for text vectorization. This method transforms text into a numerical format, considering the importance of each word across documents, which prevents frequent words from dominating the model.\n\nTF-IDF\nTF-IDF stands for Term Frequency-Inverse Document Frequency. \nTerm Frequency (TF): This measures how frequently a term (t) appears in a specific document (d). It\'s calculated by dividing the number of occurrences of the term in the document by the total number of terms in that document.\nInverse Document Frequency (IDF): This measures how important a term is across the entire collection of documents. It\'s calculated by taking the logarithm of the ratio of the total number of documents (N) to the number of documents containing the term (df(t)). A higher IDF value indicates a rarer term, making it more significant.\nHere is the resulant data we got from TF-IDF\nAfter all preprocessing of text it\'s time to save our data for mlflow experiments. To see actual implementation of end to end process of NLP you can refer jupyter notebook.\nmlflow experiments\nMLflow experiments allow you to conduct multiple experiments with your trained model, helping you track and compare results over time. For running MLflow experiments, I prefer using Databricks as it offers an integrated experiment section, making the process much more streamlined and efficient.\nIn Databricks, you can easily connect your experiment by passing the experiment ID into the MLflow code. This integration simplifies the entire workflow, enabling better experiment tracking and easier comparison of model performance.\nFor my experiments, I worked with several models, including Logistic Regression, Multinomial Naive Bayes, and Decision Tree Classifier. By applying different combinations of parameters, I was able to experiment with and compare the performance of each model. Here\'s a preview of the Logistic Regression model’s F1 score, which highlights the model\'s ability to balance precision and recall:\nLogistic Regression F1 Score: 0.58\nThis approach allowed me to track the effectiveness of each model and make adjustments as needed for improving performance.\n\nF1 score with different parameters (Logistic Regression)\nI recommend running the notebook below in your own account to see the results. You\'ll definitely start appreciating the power of MLflow.\nFor more information, please refer to the accompanying Databricks notebook.\n\nHyperparameter tuning\nHyperparameter tuning is crucial for identifying the best parameter combination for your model. This technique involves an iterative process where, for every parameter combination, the model is trained and tested on a dataset. It is often described as a "trial and error" method.\nHowever, this approach can be computationally expensive, especially when working with complex or heavy machine learning models. For large-scale problems, hyperparameter tuning can be made more efficient through sampling or batch methods. In these methods, you don\'t use the entire dataset for training the model; instead, you choose random or stratified data points from the dataset to train the model. Although this may slightly reduce accuracy, it is more feasible when working with large datasets.\nFor my project, I used Grid Search CV to find the best hyperparameter combination for the model. Below are some common techniques for hyperparameter tuning, especially for large datasets:\nGrid Search This technique exhaustively searches through a specified set of hyperparameter values, trying all possible combinations. While effective, it can be computationally expensive for larger datasets due to the exhaustive nature of the search.\nRandom Search Randomly samples from the hyperparameter space, offering a faster alternative to grid search. This method explores a wider range of hyperparameters with fewer evaluations, making it more efficient for larger datasets.\nBayesian Optimization This method uses probabilistic models to predict the performance of different hyperparameters. It selects the next set of hyperparameters to evaluate based on previous results, making it more efficient and suitable for large datasets.\nGenetic Algorithms Inspired by natural selection, these algorithms iteratively evolve a population of hyperparameter sets to improve model performance. This method works well with complex search spaces.\nHyperband Hyperband combines random search with early stopping to dynamically allocate resources across multiple configurations, identifying promising hyperparameters quickly without excessive computational costs.\nBayesian Optimization with Gaussian Processes This technique models the hyperparameter search space using Gaussian processes, focusing on regions that are likely to yield better results, which is particularly useful for large datasets where computational resources are limited.\n\nTo optimize hyperparameter tuning for larger datasets, these techniques can be combined with parallel computing and distributed processing frameworks such as Dask, Spark, or multi-GPU setups. This enables more efficient hyperparameter search and reduces the overall computational overhead.\nI choosed Logistc Regression model and trained my Model with best parameters.\nSource Distribution for Model Packaging\nPackaging your machine learning model is a best practice, especially if you don’t plan to update it frequently. Imagine thousands of lines of code that can now be utilized with just a single line—this is the power of model packaging.\n\n1. Project Folder Setup\nTo ensure better organization, I created a main folder called sentiment_prediction and moved all machine learning pipeline files and dependencies into this folder. This helped in maintaining a clean structure and simplified the management of the entire project.\nBefore moving forward I recommend you to visit this pdf it will practically show you step by step process for building python package.\nPDF : step by step guid for python package building\n\n2. Manifest.in\nThe Manifest.in file plays a crucial role in controlling which files and folders should be included or excluded during the packaging process. It helps to specify the structure of the package for distribution.\nKey commands used in the Manifest.in file include:\ninclude <file/folder>: Include specific files or folders.\nexclude <file/folder>: Exclude specific files or folders.\nrecursive-include <path>: Include all files from a directory recursively.\nrecursive-exclude <path>: Exclude all files from a directory recursively.\n3. Setup.py\nThe setup.py file contains the project\'s metadata and is essential for creating the package. It defines key information about the project, such as:\nProject name, version, description, and author details.\nDependencies required for the package (install_requires), making it easy to install all necessary libraries.\n\n4. Building the Package\nTo build the package, I used the following command:\n\npython setup.py sdist bdist_wheel\nThis command generates two folders:\n\nbuild/: Contains the entire project package as defined in the Manifest.in.\ndist/: Contains the distributable files: .whl (wheel file) .gz (compressed source archive)\n\n5. Global Access via GitHub\nNow you can access your package gloablly, by refering your repository. I provided my package below go and check out.\n\nRepository: GitHub Repo Link\nYou can install the package directly from GitHub using the following command:\n\npip install git+https://github.com/vijaytakbhate2002/sentiment_prediction_python_package.git \nTo ensure it worked globally, I tested it again:\n\nfrom sentiment_prediction import predict\nprint(predict.predictor("Great progress shared today!")) \noutput:[\'Negative\']\nFlask Application \nBuilding application will help us to give our NLP model experience to people, so I built one flask application.\nWeb Application UI\n\nHere is demonstration of project: project demo\nI left a blank section for user suggestion and feedback, these feedbacks are getting stored in database for future model analysis or any business work.\nDatabase configuration\nFor storing collected user data we need to configure a database. It will help us to improve model as per user need.\nI used Google Cloud MySQL instance for integrating my application with database, GCP is paid but you can use free credit of GCP for first 3 months, for doing almost all Cloud Work.\nYou need to create your GCP account, then create one instance under SQL and by configuring your local system with instance you are good to go.\nDocker containerization\nIf you are not awared about docker and it\'s basic concepts you can refer my previous article which explains you all about docker.\nThis guide will help you build a solid foundation in Docker, enabling you to confidently use it for your projects.\nDocker guide: Comprehensive Docker guide for deploying Flas app\nDeploy\nDeployment of web app will help us to engange people and provide them real actual experience of our services.\nAfter deployment you need to collect user data and store it for future analysis, this data contain user feedback and suggestions.\nAfter deployment it\'s not end of the process we need to collect user feedback and again follow same steps fine tune Model as per user need.\n\nSummary of Blog\nThis blog covers the lifecycle of a Machine Learning project, from ETL to deployment. It details building an ETL pipeline using PySpark for efficient data handling, EDA, and NLP preprocessing techniques like tokenization, TF-IDF, and WordCloud visualization. \nIt highlights ML experiments with MLflow on Databricks and hyperparameter tuning using Grid Search.\nThe model was packaged into a Python package and deployed as a Flask application with a database backend (Google Cloud MySQL) and Dockerized for scalability. \nThe app collects user feedback for continuous improvement. It emphasizes end-to-end integration, including cloud and containerization, to deliver a robust ML solution.\nHappy Learning!\n\n'}, {'content': '\nLanguages I Speak\nEnglish, Marathi, Hindi\n\n'}, {'content': '\n\nSoft skills:\nCritical Thinking, Intellectual Rigor, Problem Solving, Understanding Business Needs'}]
2024-11-27 14:19:11 - root - INFO - Writing documents to the document store...
2024-11-27 14:19:16 - haystack.document_stores.faiss - INFO - Updating embeddings for 14 docs...
2024-11-27 14:19:22 - root - INFO - Documents and embeddings updated.
2024-11-27 14:21:05 - root - INFO - Loading existing FAISS document store...
2024-11-27 14:21:05 - haystack.modeling.utils - INFO - Using devices: CPU - Number of GPUs: 0
2024-11-27 14:21:07 - haystack.modeling.model.language_model - WARNING - Using a model of type 'mpnet' which might be incompatible with DPR encoders. Only Bert-based encoders are supported. They need input_ids, token_type_ids, attention_mask as input tensors.
2024-11-27 14:21:08 - haystack.modeling.model.language_model - INFO - Auto-detected model language: english
2024-11-27 14:21:09 - haystack.modeling.model.language_model - WARNING - Using a model of type 'mpnet' which might be incompatible with DPR encoders. Only Bert-based encoders are supported. They need input_ids, token_type_ids, attention_mask as input tensors.
2024-11-27 14:21:11 - haystack.modeling.model.language_model - INFO - Auto-detected model language: english
2024-11-27 14:21:11 - root - INFO - Reading text document
2024-11-27 14:21:11 - root - INFO - loaded document = [{'content': '\nPersonal Information\n\nName: Vijay Takbhate\nEmail: vijaytakbhate20@gmail.com Phone: 8767363681\nGitHub: https://github.com/vijaytakbhate2002\nLinkedIn: https://www.linkedin.com/in/vijay-takbhate-b9231a236/ Kaggle: https://www.kaggle.com/vijay20213\n\n'}, {'content': '\n\nExperience - Fox Solutions Pvt. Ltd.\nRole: Automation Engineer Duration: Jun 2024 - Oct 2024 Location: Maharashtra\nKey Contributions:\n-Completed 2 months of internship plus 4 months of full-time work.\n-Worked with PLC and SCADA systems, focusing on automating processes and optimizing operational efficiency.\n-Collaborated with cross-functional teams to implement automation solutions for industrial applications.\n\n'}, {'content': '\n\nExperience - Cei Design Consultancy Pvt. Ltd.\nRole: Python Developer Intern Duration: Aug 2024 - Sept 2024 Location: Remote, Maharashtra\nKey Contributions:\n-Specialized in data processing using Python and Excel.\n-Utilized OpenCV for image processing tasks.\n\n'}, {'content': '\n\nExperience - Ujucode\nRole: Subject Matter Expert Intern Duration: Aug 2023 - Oct 2023 Location: Remote, Maharashtra\nKey Contributions:\n-Contributed as a Python developer for a ChatBot project.\n-Handled backend development tasks and researched Python modules.\n\n'}, {'content': '\n\nProject - Twitter Post Sentiment Prediction\nDetails:\n-Engineered an ETL pipeline using PySpark and SQL.\n-Conducted sentiment analysis using NLP (TF-IDF) and optimized hyperparameters.\n-Monitored model performance through MLFlow on Databricks.\n-Leveraged Google Cloud Storage and MySQL for data management.\n-Deployed the model using Docker and hosted it on Render.\n\n'}, {'content': '\n\nProject - Text-Text Chat-Bot\nDetails:\n-Designed an advanced Chat-Bot using the NVIDIA API and prompt engineering.\n-Features include paraphrasing, grammar correction, AI detection, plagiarism checking, and content summarization.\n-Targeted at content creators, researchers, and businesses.\n-Technologies Used: HTML, CSS, Python Flask, Cloud Database, and Render.\n\n'}, {'content': '\n\nProject - Hand Gesture Recognition\nDetails:\n-Used Google-s MediaPipe framework for detecting hand landmarks and gestures.\n-Created and labeled a custom dataset of hand gestures for training.\n-Developed a Streamlit application to improve accessibility and flexibility.\n\n'}, {'content': '\n\nTechnical Skills\nLanguages: MySQL, Python, HTML, CSS\nTechnologies: Streamlit, Flask, VS Code, GitHub, MLflow, Docker, PySpark, Databricks, Google Cloud Platform\n\n'}, {'content': '\n\nCertification\nMLOps Bootcamp: Mastering AI Operations for Success (Jun 2024)\n-Learned about the MLOps lifecycle and modular programming.\n-Acquired skills in Git, Python, Flask, and MLflow.\n\n'}, {'content': '\n\nEducation Details:\nBachelor of Technology in Electronics and Telecommunication (May 2024) Institution: SVERI-s College of Engineering, Pandharpur, Maharashtra Score: 81.71\nDiploma in Electronics and Telecommunication (May 2021) Institution: SVERI-s College of Engineering, Pandharpur, Maharashtra Score: 91.73\n\n'}, {'content': "\n\nBlogging\n\n\nSupervised, Unsupervised, and Beyond: ML Techniques Simplified\nNovember 25, 2024\nThere are several techniques for ML training. Among these, I will cover the following:\n\nSupervised and Unsupervised Learning\nSemi-Supervised Learning\nOffline and Online Learning\nInstance-Based and Model-Based Learning\n\nSupervised and Unsupervised Learning\nSupervised learning is like spoon-feeding our ML model in its initial stages, allowing it to learn and improve over time. Here, I’m referring to the training process.\n\nIn supervised learning, there are input columns and output columns, also called target columns. For example, in spam detection—a classification problem—the input is the email, and the target is whether the email is spam or not. That’s it!\nThis process resembles a student-teacher scenario where the teacher is a human, and the student is the model. The dataset serves as the knowledge used to train the student (model)\nSee content credentials\n\nhumand and model\nIn Unsupervised learning, we are not aware of the data labels. Instead, we separate the input data by analyzing similarities and grouping them into different clusters.\nOnce the clusters are formed, we can assign custom labels to each one. This technique is widely used to identify product relationships in online shopping and to recommend new products based on a user’s purchase history.\n\nUnsupervised Learning Clusters\nHere three clusters with three different image categories are formed\n\nSemi-Supervised Learning\nSemi-Supervised Learning is a combination of supervised and unsupervised learning, where some data is labeled and some data is unlabeled. A good example of this is Google Photos, which automatically separates new photos into their respective groups based on whether they contain a particular person in each image.\n\nThere are several techniques under semi-supervised learning; here, we will focus on the following:\nSelf Learning\nConsistency Regularization\nGenerative Models\nGraph-Based Learning\n\nLet's discuss them one by one:\n\nSelf Learning\nSelf Learning trains a model with labeled data and generates pseudo-labels for the unlabeled data. \n\nSelf learning\nThen, the model is trained on the entire dataset, which includes both the generated pseudo-labels and the labeled data.\n\nConsistency Regularization\nThis technique uses data augmentation to generate similar data, and then the model is enforced to predict the same outcome for both the augmented and original data. It helps create a model that can find similarities in both labeled and unlabeled data, predicting the same output for unlabeled data as it would for labeled data.\n\nData augmentation includes techniques like image flipping, blurring, rotation, etc. After augmenting the data, the model is trained to predict the same class for both the augmented and original images.\n\nGenerative Models\nGenerative models create synthetic data points and learn the underlying structure of the training data. These models can generate new datasets using encoders. \n\nExamples include Generative Adversarial Networks (GANs) and Variational Autoencoders (VAEs).\n\nGraph-Based Learning\nThis technique creates nodes for both labeled and unlabeled data. Similar nodes are then connected to each other.\nFor example, when dealing with images, each image starts as a separate node. If similar images are found based on their features, they are grouped or connected by an edge.\nRefer below graph to understand graph-based learning\nGraph based learning\nHere label 1 is came from labeled image of group 1, simillarly for label 2 and label 3. Finally we came up with three labels that means labeled data has three unique labels.\nBatch (Offline) and Online Learning\nBatch Learning\nBatch learning, also known as offline learning, is a technique where the developer needs to stop the deployed ML model, retrain it on new data, and then redeploy it. \nThis technique is useful when frequent updates are not required. For example, product recommendation systems can often be retrained weekly or even over several days.\nBatch learning can be costly when training the model on the entire dataset every time. However, using incremental learning models can help avoid unnecessary retraining, making the process more efficient.\nHere are some examples on incremental learning models, as of now we are focusing on general concepts so don't go much deeper into these types.\nStochastic Gradient Descent (SGD)\nNaive Bayes (Online Version)\nIncremental Support Vector Machines (ISVM)\nOnline Random Forests\nIncremental Decision Trees\nYou can refer this article to learn more about incremental learning\nHere’s your text with grammatical corrections and improved clarity:\nOnline Learning\nOnline learning is used when an ML model needs to be updated continuously with new inputs, such as in stock price prediction. In this scenario, the model must remain aware of the most recent data.\n\nBatch and online learning\nOnly models capable of incremental learning are used in online learning. Each new instance is fed back into the model to update its internal weights based on the latest input.\n\nInstance-Based and Model-Based Learning\nInstance-Based Learning\nInstance-based learning involves comparing a new input with stored data. If similarity is found, the model returns the label of the corresponding input. This approach uses pattern matching techniques. \nHere model works as a search engin not exactly same but it will find simmilarity from stored data.\nSome common models used in instance-based learning include:\nK-Nearest Neighbors (K-NN) with labels\nLocally Weighted Regression, etc.\nModel-Based Learning\nIn model-based learning, we train the model using a training dataset. During the training process, the model creates its own complex patterns (mathematical equations) to make predictions for future inputs. \nA simple example is the equation y=mx+cy = mx + c, which represents a trained model. Some common models used in model-based learning include:\nLinear Regression\nLogistic Regression\nDecision Tree, etc.\n\nSummary\nMachine Learning techniques can be broadly categorized into supervised, unsupervised, and semi-supervised learning. \nSupervised learning uses labeled data, like spam detection, while unsupervised learning works with unlabeled data, grouping similar points into clusters. \nSemi-supervised learning combines both, employing techniques like self-learning and consistency regularization to leverage partially labeled datasets.\nOther approaches include batch learning, where models are retrained periodically, and online learning, which updates models continuously with new data, suitable for dynamic tasks like stock price prediction. \nInstance-based learning relies on pattern matching (e.g., K-Nearest Neighbors), while model-based learning creates mathematical models (e.g., Linear Regression) to make predictions.\nThis is all about Machine Learning techniques. If you learned something, let me know in the comments. Your suggestions will help me improve my blogs.\nThanks for reading!\n\n"}, {'content': '\nBlogging\n\nMastering the End-to-End Machine Learning Lifecycle: From Data to Deployment\n\nNovember 24, 2024\n This article takes you through the complete lifecycle of a Machine Learning project. From ETL to deployment, I’ll share every detail of how I brought this project to life.\nData Engineering\nData Engineering serves as the starting point in the Machine Learning project lifecycle, bringing all distributed data together in one place.\nFor this project, I utilized an ETL pipeline—a core concept in Data Engineering. It enabled me to extract raw data, transform it into a meaningful format, and load it into a suitable location for further processing.\nLet\'s deep dive into ETL pipeline\nETL stands for Extract, Transform, and Load. These pipelines can be executed periodically (e.g., daily or hourly) to fetch real-time data, enhancing the predictive power of our Machine Learning model for real-world applications.\nThis process can be automated using tools like Apache Airflow, Kubeflow Pipelines, AWS Step Functions, and more, streamlining the workflow for consistent and efficient data updates.\n\nETL Pipeline:\nETL Pipeline\nExtract Operation\nThe extract operation is responsible for fetching data from various sources such as websites, databases, and APIs, as illustrated in the flowchart above.\n\nTransform Operation\nThe transform operation focuses on data cleaning, feature selection, and manipulation. In my project, I used this step to extract only the required features, ensuring the data is ready for the next phase.\n\nLoad Operation\nThe load operation transfers the transformed data to its destination, where we can choose the appropriate storage format. Typically, the processed data is stored in a database for further analysis and model training.\n\nFor my project, I used PySpark to build the ETL pipeline, as it enables efficient processing of large datasets.\nWhy not Pandas?\nWhile Pandas is excellent for small to medium-sized datasets, it stores DataFrames in RAM, which can lead to out-of-memory exceptions when handling large datasets.\nIn contrast, PySpark creates a session and processes data in chunks, storing it in ROM, making it ideal for handling large-scale data.\nFor the actual implementation of the pipeline using PySpark, please refer to the accompanying jupyter notebook.\nEDA (Data Analysis)\nData understanding is a critical stage before building any Machine Learning model. It allows us to analyze the data, plan the data processing steps, and gain insights into its structure and quality.\nIn this project, I examined the balance of data in my training and testing datasets and found it to be well-distributed across all four categories.\n\nBalanceness Checking on train and test data\nThroughout the entire process, I focused on two columns:\nTweets\nSentiments\nThe Tweets column contains the raw Twitter text data, while the Sentiments column serves as the target variable for prediction.\nNatural Language Processing\nI applied several key NLP techniques to preprocess the data and prepare it for Machine Learning model building. Below are the main steps I executed:\n\nText Processing\nRegex: I applied regular expressions to clean the text by removing URLs, hashtags, HTML tags, and keeping only alphanumeric characters. This helped eliminate unnecessary noise from the data.\nNLTK: Using the Natural Language Toolkit (NLTK), I performed word tokenization, stemming and lemmatization.\nWord tokenization is just splitting sentence into words, so we can processing each word from sentence individually.\nStemming helps us to truncate prefix or suffix of text to reduce count on unique words from corpus (paragraph).\nLemmatization is the proecss of converting any word into it\'s base word, for eg. Played will convert to play.\n\nWordCloud \nWord cloud concept help you to understand importance of words from given data. I splitted my data into four sections.\ndata for negative sentiments\ndata for positive sentiments\ndata for neutral sentiments\ndata for irrelevant sentiments\n\nHere is the representation of most frequent words for each category.\nCategorywise Word Cloud Presentation\nHere you can clearly see that there is no much difference in negative and positive sentiments data.\nthis is representation of bad data, here we can filterout our data for further processing, it might reduce data but you can do data augmentation techniques here to increase your data.\naugmenting of data in NLP with TF-IDF will not bad idea because TF-IDF and any other porcessing technique that I used is not able to detect sentence grammer or it doesn,t require sophisticated text. \nYou can think our input text will work as bag of word for model. you can understand by refering below vectorization technique.\nVectorization with TF-IDF \nI used Term Frequency-Inverse Document Frequency (TF-IDF) for text vectorization. This method transforms text into a numerical format, considering the importance of each word across documents, which prevents frequent words from dominating the model.\n\nTF-IDF\nTF-IDF stands for Term Frequency-Inverse Document Frequency. \nTerm Frequency (TF): This measures how frequently a term (t) appears in a specific document (d). It\'s calculated by dividing the number of occurrences of the term in the document by the total number of terms in that document.\nInverse Document Frequency (IDF): This measures how important a term is across the entire collection of documents. It\'s calculated by taking the logarithm of the ratio of the total number of documents (N) to the number of documents containing the term (df(t)). A higher IDF value indicates a rarer term, making it more significant.\nHere is the resulant data we got from TF-IDF\nAfter all preprocessing of text it\'s time to save our data for mlflow experiments. To see actual implementation of end to end process of NLP you can refer jupyter notebook.\nmlflow experiments\nMLflow experiments allow you to conduct multiple experiments with your trained model, helping you track and compare results over time. For running MLflow experiments, I prefer using Databricks as it offers an integrated experiment section, making the process much more streamlined and efficient.\nIn Databricks, you can easily connect your experiment by passing the experiment ID into the MLflow code. This integration simplifies the entire workflow, enabling better experiment tracking and easier comparison of model performance.\nFor my experiments, I worked with several models, including Logistic Regression, Multinomial Naive Bayes, and Decision Tree Classifier. By applying different combinations of parameters, I was able to experiment with and compare the performance of each model. Here\'s a preview of the Logistic Regression model’s F1 score, which highlights the model\'s ability to balance precision and recall:\nLogistic Regression F1 Score: 0.58\nThis approach allowed me to track the effectiveness of each model and make adjustments as needed for improving performance.\n\nF1 score with different parameters (Logistic Regression)\nI recommend running the notebook below in your own account to see the results. You\'ll definitely start appreciating the power of MLflow.\nFor more information, please refer to the accompanying Databricks notebook.\n\nHyperparameter tuning\nHyperparameter tuning is crucial for identifying the best parameter combination for your model. This technique involves an iterative process where, for every parameter combination, the model is trained and tested on a dataset. It is often described as a "trial and error" method.\nHowever, this approach can be computationally expensive, especially when working with complex or heavy machine learning models. For large-scale problems, hyperparameter tuning can be made more efficient through sampling or batch methods. In these methods, you don\'t use the entire dataset for training the model; instead, you choose random or stratified data points from the dataset to train the model. Although this may slightly reduce accuracy, it is more feasible when working with large datasets.\nFor my project, I used Grid Search CV to find the best hyperparameter combination for the model. Below are some common techniques for hyperparameter tuning, especially for large datasets:\nGrid Search This technique exhaustively searches through a specified set of hyperparameter values, trying all possible combinations. While effective, it can be computationally expensive for larger datasets due to the exhaustive nature of the search.\nRandom Search Randomly samples from the hyperparameter space, offering a faster alternative to grid search. This method explores a wider range of hyperparameters with fewer evaluations, making it more efficient for larger datasets.\nBayesian Optimization This method uses probabilistic models to predict the performance of different hyperparameters. It selects the next set of hyperparameters to evaluate based on previous results, making it more efficient and suitable for large datasets.\nGenetic Algorithms Inspired by natural selection, these algorithms iteratively evolve a population of hyperparameter sets to improve model performance. This method works well with complex search spaces.\nHyperband Hyperband combines random search with early stopping to dynamically allocate resources across multiple configurations, identifying promising hyperparameters quickly without excessive computational costs.\nBayesian Optimization with Gaussian Processes This technique models the hyperparameter search space using Gaussian processes, focusing on regions that are likely to yield better results, which is particularly useful for large datasets where computational resources are limited.\n\nTo optimize hyperparameter tuning for larger datasets, these techniques can be combined with parallel computing and distributed processing frameworks such as Dask, Spark, or multi-GPU setups. This enables more efficient hyperparameter search and reduces the overall computational overhead.\nI choosed Logistc Regression model and trained my Model with best parameters.\nSource Distribution for Model Packaging\nPackaging your machine learning model is a best practice, especially if you don’t plan to update it frequently. Imagine thousands of lines of code that can now be utilized with just a single line—this is the power of model packaging.\n\n1. Project Folder Setup\nTo ensure better organization, I created a main folder called sentiment_prediction and moved all machine learning pipeline files and dependencies into this folder. This helped in maintaining a clean structure and simplified the management of the entire project.\nBefore moving forward I recommend you to visit this pdf it will practically show you step by step process for building python package.\nPDF : step by step guid for python package building\n\n2. Manifest.in\nThe Manifest.in file plays a crucial role in controlling which files and folders should be included or excluded during the packaging process. It helps to specify the structure of the package for distribution.\nKey commands used in the Manifest.in file include:\ninclude <file/folder>: Include specific files or folders.\nexclude <file/folder>: Exclude specific files or folders.\nrecursive-include <path>: Include all files from a directory recursively.\nrecursive-exclude <path>: Exclude all files from a directory recursively.\n3. Setup.py\nThe setup.py file contains the project\'s metadata and is essential for creating the package. It defines key information about the project, such as:\nProject name, version, description, and author details.\nDependencies required for the package (install_requires), making it easy to install all necessary libraries.\n\n4. Building the Package\nTo build the package, I used the following command:\n\npython setup.py sdist bdist_wheel\nThis command generates two folders:\n\nbuild/: Contains the entire project package as defined in the Manifest.in.\ndist/: Contains the distributable files: .whl (wheel file) .gz (compressed source archive)\n\n5. Global Access via GitHub\nNow you can access your package gloablly, by refering your repository. I provided my package below go and check out.\n\nRepository: GitHub Repo Link\nYou can install the package directly from GitHub using the following command:\n\npip install git+https://github.com/vijaytakbhate2002/sentiment_prediction_python_package.git \nTo ensure it worked globally, I tested it again:\n\nfrom sentiment_prediction import predict\nprint(predict.predictor("Great progress shared today!")) \noutput:[\'Negative\']\nFlask Application \nBuilding application will help us to give our NLP model experience to people, so I built one flask application.\nWeb Application UI\n\nHere is demonstration of project: project demo\nI left a blank section for user suggestion and feedback, these feedbacks are getting stored in database for future model analysis or any business work.\nDatabase configuration\nFor storing collected user data we need to configure a database. It will help us to improve model as per user need.\nI used Google Cloud MySQL instance for integrating my application with database, GCP is paid but you can use free credit of GCP for first 3 months, for doing almost all Cloud Work.\nYou need to create your GCP account, then create one instance under SQL and by configuring your local system with instance you are good to go.\nDocker containerization\nIf you are not awared about docker and it\'s basic concepts you can refer my previous article which explains you all about docker.\nThis guide will help you build a solid foundation in Docker, enabling you to confidently use it for your projects.\nDocker guide: Comprehensive Docker guide for deploying Flas app\nDeploy\nDeployment of web app will help us to engange people and provide them real actual experience of our services.\nAfter deployment you need to collect user data and store it for future analysis, this data contain user feedback and suggestions.\nAfter deployment it\'s not end of the process we need to collect user feedback and again follow same steps fine tune Model as per user need.\n\nSummary of Blog\nThis blog covers the lifecycle of a Machine Learning project, from ETL to deployment. It details building an ETL pipeline using PySpark for efficient data handling, EDA, and NLP preprocessing techniques like tokenization, TF-IDF, and WordCloud visualization. \nIt highlights ML experiments with MLflow on Databricks and hyperparameter tuning using Grid Search.\nThe model was packaged into a Python package and deployed as a Flask application with a database backend (Google Cloud MySQL) and Dockerized for scalability. \nThe app collects user feedback for continuous improvement. It emphasizes end-to-end integration, including cloud and containerization, to deliver a robust ML solution.\nHappy Learning!\n\n'}, {'content': '\nLanguages I Speak\nEnglish, Marathi, Hindi\n\n'}, {'content': '\n\nSoft skills:\nCritical Thinking, Intellectual Rigor, Problem Solving, Understanding Business Needs'}]
2024-11-27 14:21:11 - root - INFO - Document store already contains 14 documents. Skipping write.
2024-11-27 14:23:43 - root - INFO - Loading existing FAISS document store...
2024-11-27 14:23:43 - haystack.modeling.utils - INFO - Using devices: CPU - Number of GPUs: 0
2024-11-27 14:23:44 - haystack.modeling.model.language_model - WARNING - Using a model of type 'mpnet' which might be incompatible with DPR encoders. Only Bert-based encoders are supported. They need input_ids, token_type_ids, attention_mask as input tensors.
2024-11-27 14:23:46 - haystack.modeling.model.language_model - INFO - Auto-detected model language: english
2024-11-27 14:23:46 - haystack.modeling.model.language_model - WARNING - Using a model of type 'mpnet' which might be incompatible with DPR encoders. Only Bert-based encoders are supported. They need input_ids, token_type_ids, attention_mask as input tensors.
2024-11-27 14:23:48 - haystack.modeling.model.language_model - INFO - Auto-detected model language: english
2024-11-27 14:24:07 - haystack.modeling.utils - INFO - Using devices: CPU - Number of GPUs: 0
2024-11-27 14:24:07 - haystack.modeling.utils - INFO - Using devices: CPU - Number of GPUs: 0
2024-11-27 14:25:26 - root - INFO - Loading existing FAISS document store...
2024-11-27 14:25:26 - haystack.modeling.utils - INFO - Using devices: CPU - Number of GPUs: 0
2024-11-27 14:25:28 - haystack.modeling.model.language_model - WARNING - Using a model of type 'mpnet' which might be incompatible with DPR encoders. Only Bert-based encoders are supported. They need input_ids, token_type_ids, attention_mask as input tensors.
2024-11-27 14:25:30 - haystack.modeling.model.language_model - INFO - Auto-detected model language: english
2024-11-27 14:25:31 - haystack.modeling.model.language_model - WARNING - Using a model of type 'mpnet' which might be incompatible with DPR encoders. Only Bert-based encoders are supported. They need input_ids, token_type_ids, attention_mask as input tensors.
2024-11-27 14:25:32 - haystack.modeling.model.language_model - INFO - Auto-detected model language: english
2024-11-27 14:25:38 - haystack.modeling.utils - INFO - Using devices: CPU - Number of GPUs: 0
2024-11-27 14:25:38 - haystack.modeling.utils - INFO - Using devices: CPU - Number of GPUs: 0
2024-11-27 14:29:53 - root - INFO - Loading existing FAISS document store...
2024-11-27 14:29:53 - haystack.modeling.utils - INFO - Using devices: CPU - Number of GPUs: 0
2024-11-27 14:29:55 - haystack.modeling.model.language_model - WARNING - Using a model of type 'mpnet' which might be incompatible with DPR encoders. Only Bert-based encoders are supported. They need input_ids, token_type_ids, attention_mask as input tensors.
2024-11-27 14:29:57 - haystack.modeling.model.language_model - INFO - Auto-detected model language: english
2024-11-27 14:29:58 - haystack.modeling.model.language_model - WARNING - Using a model of type 'mpnet' which might be incompatible with DPR encoders. Only Bert-based encoders are supported. They need input_ids, token_type_ids, attention_mask as input tensors.
2024-11-27 14:30:00 - haystack.modeling.model.language_model - INFO - Auto-detected model language: english
2024-11-27 14:30:03 - haystack.modeling.utils - INFO - Using devices: CPU - Number of GPUs: 0
2024-11-27 14:30:03 - haystack.modeling.utils - INFO - Using devices: CPU - Number of GPUs: 0
2024-11-27 14:32:06 - root - INFO - Loading existing FAISS document store...
2024-11-27 14:32:07 - haystack.modeling.utils - INFO - Using devices: CPU - Number of GPUs: 0
2024-11-27 14:32:07 - haystack.modeling.model.language_model - WARNING - Using a model of type 'mpnet' which might be incompatible with DPR encoders. Only Bert-based encoders are supported. They need input_ids, token_type_ids, attention_mask as input tensors.
2024-11-27 14:32:09 - haystack.modeling.model.language_model - INFO - Auto-detected model language: english
2024-11-27 14:32:11 - haystack.modeling.model.language_model - WARNING - Using a model of type 'mpnet' which might be incompatible with DPR encoders. Only Bert-based encoders are supported. They need input_ids, token_type_ids, attention_mask as input tensors.
2024-11-27 14:32:12 - haystack.modeling.model.language_model - INFO - Auto-detected model language: english
2024-11-27 14:32:15 - haystack.modeling.utils - INFO - Using devices: CPU - Number of GPUs: 0
2024-11-27 14:32:15 - haystack.modeling.utils - INFO - Using devices: CPU - Number of GPUs: 0
2024-11-27 14:32:15 - haystack.modeling.model.language_model - INFO -  * LOADING MODEL: 'deepset/minilm-uncased-squad2' (Bert)
2024-11-27 14:32:16 - haystack.modeling.model.language_model - INFO - Auto-detected model language: english
2024-11-27 14:32:16 - haystack.modeling.model.language_model - INFO - Loaded 'deepset/minilm-uncased-squad2' (Bert model) from model hub.
2024-11-27 14:32:17 - haystack.modeling.utils - INFO - Using devices: CPU - Number of GPUs: 0
